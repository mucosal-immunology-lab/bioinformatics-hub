{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Mucosal Immunology Lab Bioinformatics Hub","text":""},{"location":"#overview","title":"Overview","text":"<p>Over the years, we have refined several workflows for processing of the various omic modalities we utilise within our group. As with any workflows in the bioinformatics field, these are constantly evolving as new tools and best practices emerge. As such, this hub is very much a work-in-progress, and will remain so as we continue to add to and update it.</p>"},{"location":"#contributors","title":"Contributors","text":"<p>These sort of tasks are never accomplished alone! Massive thanks to the people who have contributed to this.</p> <ul> <li> <p> Matthew Macowan</p> <p>Bioinformatician \u2013 Mucosal Immunology Lab</p> </li> <li> <p> C\u00e9line Pattaroni</p> <p>Group Leader \u2013 Computational Immunology Group</p> </li> <li> <p> Giulia Iacono</p> <p>Post Doc \u2013 Mucosal Immunology Lab</p> </li> <li> <p> Bailey Cardwell</p> <p>PhD student \u2013 Mucosal Immunology Lab</p> </li> </ul> <ul> <li>Alana Butler: Bioinformatician (former member)</li> </ul>"},{"location":"#group-research-overview","title":"Group Research Overview","text":"<p>The Mucosal Immunology Research Group, led by Professors Marsland, Harris and Westall, is focused on understanding the fundamental principles in health and disease of the gut, lung and nervous system. Projects span the space from discovery using preclinical models of host-microbe interactions and inflammation through to high performance computational approaches to identify clinical biomarkers and development of novel drug candidates.</p>"},{"location":"#group-heads","title":"Group Heads","text":"<ul> <li> <p> Ben Marsland</p> </li> <li> <p> Nicola Harris</p> </li> <li> <p> Glen Westall</p> </li> </ul>"},{"location":"CrossOmicTools/DifferentialTesting/bio_limma/","title":"Linear modelling with <code>bio_limma</code>","text":""},{"location":"CrossOmicTools/DifferentialTesting/bio_limma/#overview","title":"Overview \ud83d\udcd6","text":"<p>Here we discuss the application of a wrapper function around the popular <code>limma</code> linear modelling package for differential abundance and expression testing of biological datasets in R. Our <code>bio_limma()</code> function can neatly handle this task and enable rapid exploratory analysis of the effect of various associated sample metadata, including correcting for various potential confounders.  Please see the limma documentation for further details regarding the main package if you have any particular limma-related queries.</p> <p>Citation</p> <p>If you use this function and end up publishing something, please consider including a reference to our work! \ud83d\ude0e\ud83d\ude4f</p> <p>Macowan, M., Pattaroni, C., Iacono, G., &amp; Marsland, B. (2025). Linear modelling for differential abundance with bio_limma - a wrapper around limma. Zenodo. https://doi.org/10.5281/zenodo.15726180</p>"},{"location":"CrossOmicTools/DifferentialTesting/bio_limma/#inputs","title":"Inputs \ud83d\udd22","text":"<p>Currently the function is suited to handle typical R data container formats related to microbiome, LCMS, and RNAseq data.</p> Data format Usage and notes <code>phyloseq</code> Classic format for microbiome data, as per our DADA2 pipeline guide. The function will use the OTU table and sample data as inputs. <code>SummarizedExperiment</code> Format used for metabolomic and lipidomic analysis, as per our LCMS data processing guide. The function will use the assay matrix and <code>@metadata$metadata</code> data as inputs. <code>DGEList</code> Format that we typically use for storing bulk RNAseq datasets, as per our RNAseq data processing guide. The function will use the assay and metadata as inputs. <code>EList</code> This is the object type resulting from an original <code>DGEList</code> object that has undergone <code>voom</code> normalisation \u2013 this is typical and means that <code>bio_limma()</code> doesn't then perform <code>voom</code> normalisation if requested. The function will use the <code>E</code> matrix and <code>targets</code> sample data table as inputs. <p><code>SummarizedExperiment</code> metadata is expected in a certain location</p> <p>The <code>bio_limma()</code> function expects that the metadata for your <code>SummarizedExperiment</code> can be found in <code>data.frame</code> format at <code>SE_obj@metadata$metadata</code>, which is the case if you have followed our LCMS data processing guide. However, if you have created your object using a different approach, then it may be stored instead under the <code>colData</code> list element for example. If this is the case, then simply make a copy and place it in the correct location for use with <code>bio_limma()</code>.</p> <p>Also, your feature naming column within the <code>elementMetadata</code> is expected to be called <code>shortname</code>. This was a convention on our part when setting up the LCMS processing pipeline. You can either copy your column to a new <code>elementMetadata</code> column called <code>shortname</code>, or pass the row names to the function directly as an argument.</p>"},{"location":"CrossOmicTools/DifferentialTesting/bio_limma/#function-parameters","title":"Function parameters \u2699\ufe0f","text":"<p>Because this function progressively evolved over time to handle the various tasks we wanted for our exploratory data analysis, there are a lot of different options you can set. View the pop-down window below for information on what parameters you can set.</p> What parameters and options can I set? Argument Description <code>input_data</code> An appropriate input data format. A <code>phyloseq</code> object, a <code>SummarizedExperiment</code> object (containing metabolomics/lipidomics data and a feature naming column called <code>shortname</code>), or an RNAseq <code>DGEList</code> of <code>EList</code> object to use for differential abundance testing. <code>metadata_var</code> OPTIONAL: the name of a single column from the <code>sample_data</code> to use for DA testing (e.g. <code>metadata_var = 'group'</code>). NOT required if providing a formula \u2013 it will be changed to <code>NULL</code> if you also provide <code>model_formula_as_string</code>. (default: <code>NULL</code>) <code>metadata_condition</code> OPTIONAL: a conditional statement about a certain metadata value, e.g. keep a certain treatment group only. (default: <code>NULL</code>) <code>metadata_keep_columns</code> OPTIONAL: this is typically required when you have complex metadata, and also supply your own model matrix and contrasts matrix. Choose columns from your metadata to retain using a character vector with column names. (default: <code>NULL</code>) <code>model_formula_as_string</code> Just like it sounds \u2013 a string containing the model formula you want to use. Only works with <code>+</code> and not <code>*</code> at this stage. (default: <code>NULL</code>) <code>model_matrix</code> OPTIONAL: although the function can create its own model matrix for simple testing, this is typically a good option for customising your comparisons. Also requires that provide a contrasts matrix. (default: <code>NULL</code>) <code>contrast_matrix</code> OPTIONAL: the corresponding contrasts matrix to complement the custom model matrix. (default: <code>NULL</code>) <code>use_contrast_matrix</code> A boolean selector for whether to use the contrast matrix or a selected coefficient. Best to leave this alone unless you're finding an error \u2013 the function will set this itself. (default: <code>TRUE</code>) <code>coefficients</code> A selection of coefficients you want to be tested. This will depend on the order of variables in the formula, and you can select as many as you'd like, e.g. <code>coefficients = 2</code> or <code>coefficients = 3:4</code>. (default: <code>NULL</code>) <code>DGEList_slot</code> Only used for RNAseq analysis when using a <code>DGEList</code> object as your input data. It tells this function which assay slot to use for analysis. Supply a string giving the name of the assay if it is not <code>'counts'</code>. (default: <code>'counts'</code>) <code>factor_reorder_list</code> OPTIONAL: a named list containing reordered factor values, e.g. <code>list(Group = c('GroupHealthy', 'GroupDisease'))</code>. This is only needed if you don't supply your own contrasts matrix, because then the first factor level will be used as the reference group. (default: <code>NULL</code>) <code>continuous_modifier_list</code> OPTIONAL: a named list containing functions to alter continuous metadata variables, e.g. <code>list(Age = function(x) x / 365)</code> to change ages in days to ages in years. (default: <code>NULL</code>) <code>adjust_method</code> OPTIONAL: the method used to correct for multiple comparisons, listed here. (default: <code>'BH'</code>) <code>rownames</code> OPTIONAL: a custom vector of names to be used if you don't wish the names to be automatically derived from your input data object. (default: <code>NULL</code>) <code>tax_id_col</code> OPTIONAL: the <code>phyloseq</code> object <code>tax_table</code> column you wish to use for naming. This should match the level being tested (and should also match the deepest taxonomic level in the <code>phyloseq</code> object). If you want to test a higher level, then agglomerate the data using the <code>phyloseq::tax_glom()</code> function. If you do not provide this, then the function will automatically select the deepest level, i.e. the right-most column that isn't entirely composed on <code>NA</code> values. (default: <code>NULL</code>) <code>override_tax_id_check</code> OPTIONAL: if you have decided to ignore the <code>phyloseq::tax_glom()</code> step, you can technically bypass the requirements for the <code>tax_id_col</code> argument by setting this to <code>TRUE</code>. You should really just go back and agglomerate your dataset though! (default: <code>FALSE</code>) <code>cores</code> EXPERIMENTAL: I would just avoid this parameter for now - it was part of an experimental edit to allow use of multiple cores for faster generation of figures. If your code is running slowly, it is 100% just because of the individual feature plots step \u2013 setting <code>max_feature_plot_pages</code> to a lower value like <code>5</code> will solve this. You will still get your volcano plots and bar plots in either case. (default: <code>NULL</code>) <code>adj_pval_threshold</code> The minimum level deemed statistically significant. (default: <code>0.05</code>) <code>logFC_threshold</code> The minimum logFC threshold deemed significant \u2013 this can vary a lot depending on the scale of your metadata variable. (default: <code>1</code>) <code>legend_metadata_string</code> OPTIONAL: a custom name for colour or fill options. (default: <code>NULL</code>) <code>volc_plot_title</code> OPTIONAL: a custom title for the volcano plot (will be reused for the associated bar plots and individual feature plots). (default: <code>NULL</code>) <code>volc_plot_subtitle</code> OPTIONAL: a custom subtitle for the volcano plot (will be reused for the associated bar plots and individual feature plots). (default: <code>NULL</code>) <code>use_groups_as_subtitle</code> OPTIONAL: if set to <code>TRUE</code>, the function will use the group names (or the contrast matrix comparison names) as the volcano plot subtitle. (default: <code>FALSE</code>) <code>volc_plot_xlab</code> OPTIONAL: a custom <code>x</code> label for the volcano plot. (default: <code>NULL</code>) <code>volc_plot_ylab</code> OPTIONAL: a custom <code>y</code> label for the volcano plot. (default: <code>NULL</code>) <code>remove_low_variance_taxa</code> OPTIONAL: is set to <code>TRUE</code>, the <code>phyloseq</code> OTU table will be checked for feature-wise variance, and all features with zero variance will be removed prior to downstream analysis. You may find <code>limma</code> throws an error if most of the features have no variance, so this step is sometimes required for certain datasets. (default: <code>FALSE</code>) <code>plot_output_folder</code> OPTIONAL: a path to a folder where you would like output plots to be saved. If left blank, no plots will be saved. It will create a new folder if it does not exist at the final level \u2013 the parent folder must still exist. (default: <code>NULL</code>) <code>plot_file_prefix</code> OPTIONAL: a string to attach to the start of the individual file names for your plots. This input is only used if the <code>plot_output_folder</code> argument is also provided. (default: <code>NULL</code>) <code>redo_boxplot_stats</code> OPTIONAL: box plot statistics can be recalculated using <code>stat_compare_means()</code> from the <code>ggpubr</code> package. The default option will only show the <code>limma</code> statistics for the single comparison, so setting this to <code>TRUE</code> can provide more information, particularly if you have more than 2 levels for your categorical variable. (default: <code>FALSE</code>) <code>max_feature_plot_pages</code> OPTIONAL: if an integer value is given, it will limit the number of feature plots generated \u2013 there will still be 12 plots per page. If you have a large number of significant features, setting this argument to something like 5 will dramatically improve running speed \u2013 this is the primary function bottleneck in terms of processing speed. (default: <code>NULL</code>) <code>use_voom</code> OPTIONAL: this is used for RNAseq analysis, and will perform <code>voom</code> normalisation prior to running the <code>limma::fitLM()</code> function using the input data nd model matrix. Typically this will have been run prior to differential testing, and your input data will already be of type <code>EList</code>. (default: <code>FALSE</code>) <code>force_feature_table_variable</code> OPTIONAL: if you provide your own model matrix and contrast matrix, <code>bio_limma</code> will attempt to determine which metadata column is being referenced when it tries to generate the feature plots. It does so using the <code>stringdist</code> pacakge, and does a pretty good job. However, if it is getting it wrong (especially if you have decided to make a nice contrast name that isn't close to the metadata column name), you can set this argument to the correct variable to ensure you get the plots you want. (default: <code>NULL</code>) <code>feature_plot_dot_colours</code> OPTIONAL: you can colour the dots in your feature plots using a categorical variable identified by providing a string with the name of a metadata column. Keep in mind this column should be complete to avoid potential errors (i.e. no <code>NA</code> values). (default: <code>NULL</code>) <code>feature_plot_beeswarm_cex</code> OPTIONAL: use this value to determine the horizontal spread of the dots. See <code>ggbeeswarm</code> documentation for more details. (default: <code>NULL</code>) <code>theme_pubr</code> OPTIONAL: set this to <code>TRUE</code> to get cleaner, prettier plots using the <code>ggpubr</code> <code>theme_pubr</code> theme. (default: <code>FALSE</code>)"},{"location":"CrossOmicTools/DifferentialTesting/bio_limma/#function-output","title":"Function output \ud83c\udf81","text":"<p>The function will return a list with different outputs from the analysis.</p> List element Description <code>input_data</code> The original counts table used to run the analysis. <code>input_metadata</code> A <code>data.frame</code> with the original metadata you provided. <code>test_variables</code> A <code>data.frame</code> with the subset of metadata variables used for the analysis. <code>model_matrix</code> The model matrix generated (or provided) to the function. <code>contrast_matrix</code> OR <code>coefficients</code> Either the contrast matrix used, or the coefficients selected, depending on the analysis you chose to run. <code>limma_significant</code> A list of <code>data.frames</code> containing the significant differentially abundant features determined by the <code>limma()</code> function, with the adjusted p-value and logFC threshold selected, for each comparison/coefficient. <code>limma_all</code> A list of <code>data.frames</code> containing the significance levels and logFC of all features, regardless of their significance, for each comparison/coefficient. <code>volcano_plots</code> Volcano plots for each of the comparisons/coefficients selected. <code>bar_plots</code> Bar plots combining significant features for each of the comparisons/coefficients selected. The x-axis shows the logFC values calculated by <code>limma</code>, with features names on the y-axis, ordered by the effect magnitude. <code>feature_plots</code> Individual box or scatter plots for each feature, for each of the comparisons/coefficients selected. <code>venn_diagram</code> An empty shell that originally contained the Venn diagram that showed up when running the function... This needs to be updated by using another Venn diagram function \u2013 hopefully to come in the future! Which plot files are saved to disk? <p>If a plot output folder path is provided, for each comparison/coefficient you have selected, three output <code>.pdf</code> files will be generated (provided there is at least 1 significant feature). All will have the calculated prefix: <code>{plot_file_prefix}_{test_variable + group}_</code>.</p> File name Description <code>volcplot.pdf</code> A volcano plot showing all features, with significant features labelled. Decreased and increased features are shown in blue and red respectively. <code>barplot.pdf</code> A bar plot showing significant features, with the logFC on the x-axis and the feature name on the y-axis. The y-axis is ordered by logFC magnitude, with negative at the bottom and positive at the top. Negative logFC features are coloured blue, while positive ones are coloured red. The top features of each direction are selected for plotting, and the plot will resize if there are fewer values than this. <code>featureplots.pdf</code> Individual box or scatter plots for each feature. These plots are arranged with 12 features to a page (3 columns and 4 rows). Multiple pages will be combined into a single output <code>.pdf</code> files if there are more than 12 significant features."},{"location":"CrossOmicTools/DifferentialTesting/bio_limma/#example","title":"Example \u2728","text":""},{"location":"CrossOmicTools/DifferentialTesting/bio_limma/#microbiome-data-with-custom-model-matrix","title":"Microbiome data with custom model matrix \ud83e\udda0","text":"<p>In this example, we want to find out the effect on the microbiome of two treatment options vs. sham treatment. We decide to test for the effects of each compared to sham, and then compared the two treatment groups combined compared to sham.</p> Example of running bio_limma with custom comparisons<pre><code># Load required packages\npkgs &lt;- c('BiocGenerics', 'base', 'ggtree', 'ggplot2', 'IRanges', 'Matrix', 'S4Vectors', 'biomformat', 'plotly', \n          'dplyr', 'rstatix', 'ggpubr', 'stats', 'phyloseq', 'SummarizedExperiment', 'ggpmisc', 'ggrepel', 'ggsci', \n          'grDevices', 'here', 'limma', 'stringr', 'stringdist', 'ggbeeswarm', 'doParallel', 'openxlsx')\nfor (pkg in pkgs) {\n    suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n}\n\n# Load phyloseq data\nbact_data_logCSS &lt;- readRDS(here('output', '01_Preprocessing', 'bact_data_logCSS.rds'))\n\n# Prepare the model matrix using the \"Treatment\" metadata column\nmodel_matrix &lt;- model.matrix(~ 0 + Treatment, data = data.frame(bact_data_logCSS@sam_data))\n\n# Prepare the corresponding contrasts matrix\ncontrast_matrix &lt;- makeContrasts('DrugA_vs_Sham' = TreatmentDrugA - TreatmentSham,\n                                 'DrugB_vs_Sham' = TreatmentDrugB - TreatmentSham,\n                                 'EitherDrug_vs_Sham' = (TreatmentDrugA + TreatmentDrugB) / 2 - TreatmentSham,\n                                 levels = colnames(model_matrix))\n\n# Run bio_limma\nres &lt;- bio_limma(input_data = bact_data_logCSS,\n                 model_matrix = model_matrix,\n                 contrast_matrix = contrast_matrix,\n                 metadata_keep_columns = c('Group', 'SubjectID'),\n                 logFC_threshold = 0.5,\n                 adjust_method = 'BH',\n                 adj_pval_threshold = 0.05,\n                 redo_boxplot_stats = TRUE,\n                 force_feature_table_variable = 'Treatment',\n                 max_feature_plot_pages = 5,\n                 plot_output_folder = here('figures', 'LimmaDA', 'ASV'),\n                 plot_file_prefix = 'BH',\n                 theme_pubr = TRUE)\n\n# Save results to disk\nsaveRDS(res, here('figures', 'LimmaDA', 'ASV', 'limma_res.rds'))\n\n# Write results tables to an Excel workbook\nlimma_sig &lt;- foreach(i = seq_along(res$limma_significant)) %do% {\n    return(res$limma_significant[[i]] %&gt;% rownames_to_column(var = 'feature'))\n}; names(limma_sig) &lt;- names(res$limma_significant)\nwrite.xlsx(limma_sig, here('figures', 'LimmaDA', 'ASV', 'limma_DA_significant.xlsx'))\n</code></pre>"},{"location":"CrossOmicTools/DifferentialTesting/bio_limma/#rights","title":"Rights","text":"<ul> <li>Copyright \u00a9\ufe0f 2024 Mucosal Immunology Lab, Monash University, Melbourne, Australia.</li> <li>Licence: This software is provided under the MIT license.</li> <li>Authors: M. Macowan</li> </ul> <ul> <li>limma: Ritchie ME, Phipson B, Wu D, Hu Y, Law CW, Shi W, Smyth GK (2015). \"limma powers differential expression analyses for RNA-sequencing and microarray studies.\" Nucleic Acids Research, 43(7), e47. doi:10.1093/nar/gkv007.</li> <li>ggplot2: Wickham H (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN 978-3-319-24277-4, https://ggplot2.tidyverse.org.</li> <li>ggpubr: Kassambara A (2023). ggpubr: 'ggplot2' Based Publication Ready Plots. R package version 0.6.0, https://rpkgs.datanovia.com/ggpubr/.</li> <li>ggrepel: Slowikowski K (2024). ggrepel: Automatically Position Non-Overlapping Text Labels with 'ggplot2'. https://ggrepel.slowkow.com/, https://github.com/slowkow/ggrepel.</li> <li>ggtree: Yu, G., Smith, D.K., Zhu, H., Guan, Y. and Lam, T.T.-Y. (2017), ggtree: an r package for visualization and annotation of phylogenetic trees with their covariates and other associated data. Methods Ecol Evol, 8: 28-36. https://doi.org/10.1111/2041-210X.12628</li> <li>biomformat: McMurdie PJ, Paulson JN (2025). biomformat: An interface package for the BIOM file format. doi:10.18129/B9.bioc.biomformat, R package version 1.36.0, https://bioconductor.org/packages/biomformat.</li> <li>tidyverse: Wickham H, Averick M, Bryan J, Chang W, McGowan LD, Fran\u00e7ois R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, M\u00fcller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \u201cWelcome to the tidyverse.\u201d Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686.</li> <li>phyloseq: McMurdie and Holmes (2013) phyloseq: An R Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data. PLoS ONE. 8(4):e61217</li> <li>SummarizedExperiment: Morgan M, Obenchain V, Hester J, Pag\u00e8s H (2025). SummarizedExperiment: A container (S4 class) for matrix-like assays. doi:10.18129/B9.bioc.SummarizedExperiment, R package version 1.38.1, https://bioconductor.org/packages/SummarizedExperiment.</li> <li>edgeR: Chen Y, Chen L, Lun ATL, Baldoni P, Smyth GK (2025). \u201cedgeR v4: powerful differential analysis of sequencing data with expanded functionality and improved support for small counts and larger datasets.\u201d Nucleic Acids Research, 53(2), gkaf018. doi:10.1093/nar/gkaf018.</li> <li>here: M\u00fcller K (2025). here: A Simpler Way to Find Your Files. R package version 1.0.1, https://here.r-lib.org/.</li> <li>stringr: Wickham H (2023). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.5.1, https://github.com/tidyverse/stringr, https://stringr.tidyverse.org.</li> <li>stringdist: van der Loo M (2014). \u201cThe stringdist package for approximate string matching.\u201d The R Journal, 6, 111-122. https://CRAN.R-project.org/package=stringdist.</li> <li>ggbeeswarm: Clarke, E., Sherrill-Mix, S., &amp; Dawson, C. (2023). ggbeeswarm: Categorical Scatter (Violin Point) Plots. R package version 0.7.2. CRAN.</li> <li>doParallel: Corporation M, Weston S (2022). doParallel: Foreach Parallel Adaptor for the 'parallel' Package. R package version 1.0.17. https://CRAN.R-project.org/package=doParallel</li> </ul>"},{"location":"LCMS/lcms-analysis/","title":"LCMS analysis","text":"<p>Here we will provide a guide for the processing of raw LCMS output files from metabolomics and lipidomics runs, how to undertake curation and annotation, and approaches for their analysis downstream.</p>"},{"location":"LCMS/lcms-analysis/#overview","title":"Overview","text":"<p>The MS-DIAL software we recommend provides a pipeline for untargeted metabolomics. Its outputs then require thorough quality control measures and additional annotation steps. Additional annotation with HMDB and/or LIPID MAPS supplements the annotations provided through standards and MS-DIAL to ensure we can retain and utilise the maximum number of features.</p> <p>Citation</p> <p>If you use this workflow and end up publishing something, please consider including a reference to our work! \ud83d\ude0e\ud83d\ude4f</p> <p>Macowan, M., Pattaroni, C., Cardwell, B. A., Iacono, G., &amp; Marsland, B. (2025). Mucosal Immunology Research Group - Processing metabolome and lipidome data with MS-DIAL. Zenodo. https://doi.org/10.5281/zenodo.15701971</p> <p>Analysis workflow</p> <ol> <li>Process raw LCMS data with MS-DIAL</li> <li>Perform automated quality control with pmp</li> <li>Secondary feature annotation with HMDB/LIPID MAPS</li> <li>Manual curation of annotated spectra</li> <li>Downstream analysis</li> </ol>"},{"location":"LCMS/manual-peak-curation/","title":"Manual curation of peaks","text":"<p>Manual peak curation is an essential part of the preprocessing of LMCS datasets, particularly for ensuring that only high quality spectra are retained for downstream analysis and interpretation. Automated tools like <code>pmp</code> are a great start, and remove a lot of the noise (which is a big help in reducing the number of peaks we actually have to check), but they are not foolproof and many poor quality peaks will inevitably remain. Manual curation is therefore critical, and both refines the dataset and strengthens confidence in metabolite and lipid identification, which in turn supports more robust biological conclusions.</p> <p>Cutting through the clutter...</p> <p>You will tend to find that about 30-50% of peaks are poor quality and require removal \u2013 this is of course dataset dependent.</p> <ul> <li>Poor signal-to-noise ratio is a common issue, resulting in \"messy\" and noisy spectra.</li> <li>Peaks only present in a few samples should also be removed to avoid over-fitted data imputation \u2013 it is fine to keep peaks that show up in just a single biological sample group however, because perhaps it is only abundant in this setting.</li> </ul>"},{"location":"LCMS/manual-peak-curation/#prepare-xml-files-for-ms-dial","title":"Prepare XML files for MS-DIAL \ud83d\udcc4\ud83d\udee0\ufe0f","text":"<p>To speed up the process, we can add a <code>1</code> tag to each of the remaining annotated features in our <code>SummarizedExperiment</code> object. In MS-DIAL, a <code>1</code> tag simply refers to a confirmed metabolite, but for our purposes, we simply need a way to filter the tens of thousands of peaks down to just the ones we need to check.</p> <p>MS-DIAL actually creates a file already, and so we can just update the two <code>.xml</code> files for the positive and negative ionisation modes.</p> <p>Where are these files?</p> <p>These files are located in the main MS-DIAL project folder where you processed the raw LCMS data.</p> <p>If you followed the recommendation to choose a custom project name in MS-DIAL, then this will be a lot easier </p> <ul> <li>For example, if your named your project files:<ul> <li><code>metab_stool_pos.mdproject</code></li> <li><code>metab_stool_neg.mdproject</code></li> </ul> </li> <li>Then these files will be:<ul> <li><code>metab_stool_pos_tags.xml</code></li> <li><code>metab_stool_neg_tags.xml</code></li> </ul> </li> </ul>"},{"location":"LCMS/manual-peak-curation/#generate-a-curation-table","title":"Generate a curation table \ud83d\udccb","text":"<p>To begin, we will generate a curation table that will identify which features we will need to manually check. The <code>make_curation_table()</code> function will do this for us.</p> Parameters for <code>make_curation_table()</code> Parameter Description <code>metab_SE</code> A <code>SummarizedExperiment</code> object with secondary annotations, which has been filtered for just annotated features. Generate curation table<pre><code># Make the curation table\ncuration_table &lt;- make_curation_table(metab_SE = metab_stool_glog)\n\n# Save curation table\ncuration_dir &lt;- here('input', '01_LCMS', 'manual_curation')\nsaveRDS(curation_table, here(curation_dir, 'curation_table.rds'))\n</code></pre> <p>Save your curation table</p> <p>Ensure you save the curation table, because you will need to it read back in the manual curation results from the MS-DIAL XML tag files.</p>"},{"location":"LCMS/manual-peak-curation/#prepare-the-relevant-xml-files-for-ms-dial","title":"Prepare the relevant XML files for MS-DIAL \ud83d\uddc2\ufe0f\u2699\ufe0f","text":"<p>MS-DIAL uses the <code>tag</code> files for identifying which features have certain available peak spot characteristics.</p> What peak spot tags does MS-DIAL use? <p>There are 5 different \"Peak Spot Tags\" in MS-DIAL. We are just co-opting the <code>Confirmed</code> tag as a marker for manual curation. The current tags for peaks are:</p> <ol> <li>Confirmed</li> <li>Low quality spectrum</li> <li>Misannotation</li> <li>Coelution (mixed spectra)</li> <li>Overannotation</li> </ol> <p>We will use tag <code>2</code> to mark poor quality peaks as we curate.</p> <p>We will use the <code>create_msdial_xml()</code> function to automate the process of tagging peaks found in the curation table to the relevant ionisation mode tag file.</p> Parameters for <code>create_msdial_xml()</code> Parameter Description <code>curation_table</code> The <code>data.frame</code> curation table output from the <code>make_curation_table()</code> function. <code>path_to_folder</code> The file path to the main MS-DIAL project folder (i.e. where your tag files are). <code>experiment_name</code> The base file name of your tag files. E.g. <code>'metab_stool'</code> for tag files named <code>metab_stool_pos_tags.xml</code> and <code>metab_stool_neg_tags.xml</code>. <code>peak_id_column</code> Default: <code>1</code> \u2013 the number of the column containing the annotation IDs. <code>ionisation</code> Default: <code>c('pos', 'neg')</code> \u2013 the ionisation markers at the end of the tag files. <code>force_proceed</code> Default: <code>FALSE</code> \u2013 an option for whether to force the override of existing files. This is not recommended. Remove the existing update to the tags file, and rename the <code>_BACKUP.xml</code> file to its original name. Then rerun \u2013 this is a safer option. Update the XML tag files<pre><code># Prepare the relevant XML files for MS-DIAL\ncreate_msdial_xml(curation_table = curation_table,\n                  path_to_folder = here('input', '01_LCMS', 'Raw'),\n                  experiment_name = 'metab_stool',\n                  peak_id_column = 1,\n                  ionisation = c('pos', 'neg'))\n</code></pre>"},{"location":"LCMS/manual-peak-curation/#peak-curation-in-ms-dial","title":"Peak curation in MS-DIAL \ud83d\udcc8\ud83d\udd0d","text":"<p>Now that the tag files have been update to indicate which peaks we need to manually curate, we can return to MS-DIAL and check each of the peaks.</p> <p>As you can see below, some of the tags are now highlighted in the peak spot table, which is just what we wanted!</p> <p>Note</p> <p>You should not see anything highlighted in the \"Low quality spectrum\" column at this stage. These images were taken after manual peak checking.</p> <p></p> <p>You can then select the \u2611\ufe0f option next to <code>Tag filter</code> at the top left of the peak spot table to filter for just the peaks requiring curation.</p> <p></p> <p>At this point you can simply work your way through each peak to determine whether it should be retained for downstream analysis.  Mark peaks as having low quality by pressing <code>Ctrl + 2</code> or by clicking the <code>L</code> button.</p>"},{"location":"LCMS/manual-peak-curation/#identifying-good-quality-peaks","title":"Identifying good quality peaks \u2b50\ud83d\udd0e","text":"<p>How do I know what to keep?</p> <p>The simple question you should ask yourself is: \"Would I call this a peak?\".</p> <p>If not, then mark it as low quality, and continue with the next spectrum. This task is not something to labour over \u2013 you will get the hang of things quickly as you see more examples and learn how different spectra look.</p> <p>To help you out, some examples of spectra are shown below.</p> <p></p> <p>Remember too to open the <code>Peak curation (sample table)</code> of the aligned spots. This will give you a much better idea about how your peaks look in individual samples, and can often provide a clearer picture. For example, the left-most \"good\" peak above has one sample with far greater intensity than the other which may skew our impression. The peak curation sample table however would allow us to see that peaks look good in all samples, and importantly that it is also present in the majority of samples (if not all).</p> <p></p>"},{"location":"LCMS/manual-peak-curation/#filter-the-summarizedexperiment-object-for-the-best-features","title":"Filter the <code>SummarizedExperiment</code> object for the best features \u2697\ufe0f","text":"<p>At this point, all of the peaks have been manually checked in both the positive and negative ionisation modes and it is time to read the results back into R. Then we can filter for the best feature for each named metabolite/lipid. You can</p>"},{"location":"LCMS/manual-peak-curation/#read-in-the-manual-curation-results-from-ms-dial","title":"Read in the manual curation results from MS-DIAL \ud83d\udce5\ud83d\udccb","text":"<p>We can now use the <code>read_msdial_xml()</code> function to load in the tag files from MS-DIAL. Recall that we need the curation table we prepared earlier too, so this also needs to be retrieved.</p> Parameters for <code>read_msdial_xml()</code> Parameter Description <code>curation_table</code> The <code>data.frame</code> curation table output from the <code>make_curation_table()</code> function. <code>path_to_folder</code> The file path to the main MS-DIAL project folder (i.e. where your tag files are). <code>experiment_name</code> The base file name of your tag files. E.g. <code>'metab_stool'</code> for tag files named <code>metab_stool_pos_tags.xml</code> and <code>metab_stool_neg_tags.xml</code>. <code>ionisation</code> Default: <code>c('pos', 'neg')</code> \u2013 the ionisation markers at the end of the tag files. <p>Manually check feature names</p> <p>This is entirely up to you, but at this point, you may also wish to double check that your have the optimal <code>shortname</code> values. You can simply add an additional column to a <code>.csv</code> export of your curation table, and adjust anything as required. This can be useful if you identify features that are the same, but have slightly different names or capitalisation etc.</p> Filter out low quality peaks<pre><code># Recover the SummarizedExperiment and curation table\nmetab_stool_glog &lt;- readRDS(here('output', '01_Preprocessing', 'metab_stool_glog_anno.rds'))\ncuration_dir &lt;- here('input', '01_LCMS', 'manual_curation')\ncuration_table &lt;- readRDS(here(curation_dir, 'curation_table.rds'))\n\n# Read in the XML tag files from MS-DIAL\nmanual_curation_xml &lt;- read_msdial_xml(curation_table = curation_table,\n                                       path_to_folder = here('input', '01_LCMS', 'Raw',\n                                       experiment_name = 'metab_stool',\n                                       ionisation = c('pos', 'neg')))\n\n# Filter out the low quality peaks\nmetab_stool_glog &lt;- metab_stool_glog[!manual_curation_xml$Anno2,]\n\n# Save intermediate object\nsaveRDS(metab_stool_glog, here('output', '01_Preprocessing', 'metab_stool_glog_tmp_curated.rds'))\n</code></pre> <p>Why do we filter by <code>Anno2</code>?</p> <p>Each of the tags is assigned to an annotation number. Here, <code>Anno2</code> refers to tag <code>2</code> which, as we saw above, represents low quality peaks. <code>Anno1</code> on the other hand would represent confirmed hits (i.e. the ones we were manually checking).</p>"},{"location":"LCMS/manual-peak-curation/#selecting-the-best-features","title":"Selecting the best features \ud83c\udfaf\u2728","text":"<p>Now we will select for the best features from both the positive and negative ionisation modes so we are left with a single representation of each metabolite/lipid. We find this to be preferable to avoid cases where there are multiple features with the same name and pattern \u2013 this is particularly noticeable on heatmaps when you can see bands of features with identical patterns.</p> <p>To identify the best feature, we will select the highest value from the multiple of <code>Fill %</code> and <code>S/N ratio</code>. From there, we will create a logical vector to indicate whether a feature should be retained.</p> Best feature selection<pre><code># Recover metabolomics data\nmetab_stool_glog &lt;- readRDS(here('output', '01_Preprocessing', 'metab_stool_glog_tmp_curated.rds'))\n\n# Create a data.frame with the necessary feature metadata to identify best features\nelement_meta &lt;- data.frame(metab_stool_glog@elementMetadata) %&gt;%\n    mutate(original_order = 1:nrow(metab_stool_glog@elementMetadata)) %&gt;%\n    dplyr::select(original_order, info.Alignment.ID, ionisation,\n                  info.Fill.., info.S.N.average, shortname) %&gt;%\n    arrange(shortname = gsub('_pos.*|_neg.*', '', shortname)) # remove ionisation\n\n# Filter for the highest multiple of 'Fill %' * 'Signal-to-noise ratio'\nelement_meta_top &lt;- element_meta %&gt;%\n    mutate(metab_score = info.Fill.. * info.S.N.average) %&gt;%\n    arrange(desc(metab_score)) %&gt;%\n    group_by(shortname) %&gt;%\n    slice_head(n = 1) %&gt;%\n    ungroup()\n\n# Create a vector of \"best\" features to keep\nfeature_keep &lt;- element_meta %&gt;%\n    left_join(element_meta_top %&gt;% dplyr::select(original_order, metab_score),\n              by = 'original_order') %&gt;%\n    mutate(keep = ifelse(is.na(metab_score), FALSE, TRUE)) %&gt;%\n    arrange(original_order) %&gt;%\n    pull(keep)\n</code></pre> <p>Now we can use the \"keep vector\" to filter the <code>SummarizedExperiment</code> object, remove the QC samples, and save the object to an <code>.rds</code> file.</p> Filter SummarizedExperiment and save to file<pre><code># Filter the SummarizedExperiment object\nmetab_stool_glog &lt;- metab_stool_glog[feature_keep,]\n\n# Remove ionisation from the shortname values\nmetab_stool_glog@elementMetadata$shortname &lt;-\n    gsub('_pos.*|_neg.*', '', metab_stool_glog@elementMetadata$shortname)\n\n# Remove the QC samples\nmetab_stool_glog &lt;- metab_stool_glog[, !str_detect(colnames(metab_stool_glog), 'QC')]\n\n# Save to object\nsaveRDS(metab_stool_glog, here('output', '01_Preprocessing', 'metab_stool_glog_curated.rds'))\n</code></pre> <p>Why would we use Fill% and S/N ratio to select the best features?</p> <ul> <li>Fill %: this refers to the percentage of samples in which a given feature (peak) is detected across the entire dataset. It provides an indication of the feature's prevalence or coverage among the samples.<ul> <li>High Fill % indicates that the feature is detected in most or all samples, suggesting it may be a consistently present metabolite or compound.</li> <li>Low Fill % indicates that the feature is detected in only a few samples, which could imply it is rare, a contaminant, or specific to certain conditions or samples.</li> </ul> </li> <li>S/N ratio: signal-to-noise ratio represents the relative strength of a detected signal compared to the background noise level. It is calculated as the ratio of the intensity of the analyte's signal to the baseline noise level.<ul> <li>High S/N ratio indicates a strong, reliable signal that stands out clearly from the background noise. Typically considered robust and accurate.</li> <li>Low S/N ratio suggests a weak signal relative to noise, which may be less reliable and could represent a borderline or questionable detection.</li> </ul> </li> </ul>"},{"location":"LCMS/manual-peak-curation/#next-steps","title":"Next steps \u27a1\ufe0f","text":"<p>Congratulations! The pre-processing steps are complete, and you now have a prepared <code>SummarizedExperiment</code> that has undergone quality control, secondary annotation, and manual curation.</p> <p>From here, you can move onto downstream analysis of the data.</p>"},{"location":"LCMS/manual-peak-curation/#rights","title":"Rights","text":"<ul> <li>Copyright \u00a9\ufe0f 2024 Mucosal Immunology lab, Monash University, Melbourne, Australia.</li> <li>HMDB version 5.0: Wishart DS, Guo A, Oler E, Wang F, Anjum A, Peters H, Dizon R, Sayeeda Z, Tian S, Lee BL, Berjanskii M. HMDB 5.0: the human metabolome database for 2022. Nucleic acids research. 2022 Jan 7;50(D1):D622-31.</li> <li>License: This pipeline is provided under the MIT license.</li> <li>Authors: M. Macowan, B. Cardwell, and C. Pattaroni</li> </ul>"},{"location":"LCMS/msdial-processing/","title":"Processing metabolome and lipidome data with MS-DIAL","text":"<p>MS-DIAL provides a pipeline for untargeted metabolomics. Here we will discuss the process for generating intensity height tables from raw LC-MS data, and subsequent export for downstream analysis and secondary annotation methods.</p>"},{"location":"LCMS/msdial-processing/#requirements","title":"Requirements","text":"<p>Conversion of .RAW files</p> <p>As of MS-DIAL version 4.70, it is no longer a requirement to convert your .RAW LC-MS files into .ABF format, however if you are using an older version of MS-DIAL, you will need to download the ABF file converter.</p> <ul> <li>ABF File Converter (depending on MS-DIAL version).</li> <li>MS-DIAL software for Windows.</li> <li>MassBank MS/MS positive and negative database files.</li> <li>The LC-MS files all present in a single folder.</li> <li>The LC-MS standards file generated during data collection (should contain columns for: metabolite name, m/z, and retention time).</li> </ul>"},{"location":"LCMS/msdial-processing/#setting-up-a-new-project","title":"Setting up a new project \ud83c\udfd7\ufe0f","text":"<p>Note</p> <p>The instructions given here are for MS-DIAL version 5.</p> <p>Begin by opening MS-DIAL and starting a new project using the icon in the centre of the screen.</p> <p>MS-DIAL will prompt you to enter a <code>Project title</code> and a <code>Project file path</code>. Please remember that the project file path must contain all of your raw LCMS files \u2013 you can navigate to this folder using the <code>'Browse'</code> button.</p> <p>Change the default project name</p> <p>It is recommended that you change the default <code>.mdproject</code> file name generated by MS-DIAL to something more easily recognisable in the future, such as <code>date_sampletype_ionisationmode.mtd</code>.</p> <p></p>"},{"location":"LCMS/msdial-processing/#raw-measurement-files","title":"Raw measurement files \ud83d\uddc2\ufe0f","text":"<p>The next screen will ask you to define your <code>'Analysis file paths'</code>. Click the browse button, and select all of your LC-MS files. Note that these must be located in the same folder as your project in order to be valid.</p> <p>Once you have done this, you will need to change the <code>'Type'</code> column to indicate whether that sample is a sample, blank, QC, or standard. It is also recommended, for quick analysis later on, to change the values in the <code>'Class ID'</code> column.</p> <p>For example, in the <code>'Class ID'</code> column below, it is indicated what the sample type is (if you have multiple groups, you can specify these here), and of note, while <code>MS2</code> samples are given a <code>'Type'</code> of <code>QC</code>, you can specify that they are their own class in this column.</p> <p>If you were provided with information about the order in which your samples were actually run, you can input this in the <code>Analytical order</code> column. This will allow retention time correction. However, as the majority of our annotations will be provided by the Human Metabolome Database (HMDB), which does not take retention time into account, this step will be less important downstream. One benefit however will be for the alignment of peaks in the event you have significant retention time drift over the course of data acquisition.</p> <p>Once you have finished, click <code>'Next'</code> to continue.</p> <p></p>"},{"location":"LCMS/msdial-processing/#measurement-parameters","title":"Measurement parameters \ud83d\udcd0","text":"<p>The next screen will ask you to select various parameters regarding how your data was acquired.</p> <p>Firstly, you can define a project name. It is recommended that you change the default <code>.mddata</code> file name generated by MS-DIAL to something more easily recognisable in the future, such as <code>date_sampletype_ionisationmode.mddata</code>.</p> <p>File name suffix</p> <p>No matter how you decide to name your files, ensure that you use the same name for both positive and negative ionisations, and end each with <code>_pos</code> and <code>_neg</code> respectively. If you don't, you won't be able to make easy use of the XML curation function to speed up your manual curation later on.</p> <ul> <li>If you use <code>_positive</code> and <code>_negative</code> like the example below, you'll also be fine.</li> <li>The key is the make sure the file base name is identical.</li> </ul> <p>For our data, most of the default options will be appropriate. </p> <ul> <li>Data type for both <code>MS1</code> and <code>MS2</code> should be set to <code>Profile data</code> for our data, acquired using a Thermo Fisher Scientific LCMS apparatus.</li> <li>Ensure you choose the correct <code>'Ion mode'</code> at the bottom left, either positive or negative ionisation. and that you select the correct <code>'Target omics'</code> from either metabolomics or lipidomics.</li> </ul> <p>Click <code>Next</code> once complete.</p> <p></p>"},{"location":"LCMS/msdial-processing/#analysis-parameter-settings","title":"Analysis parameter settings \ud83d\udd27","text":"<p>Each tab below covers the settings for a tab on MS-DIAL. Input the correct settings, and then proceed to running your samples.</p> Data collection \ud83d\udccaPeak detection \ud83d\udcc8\ud83d\udd0dSpectrum deconvolution \ud83c\udf08\ud83d\udd2cIdentification \ud83c\udd94\ud83d\udd0eAdduct ion \u2795\u269b\ufe0fAlignment parameters \ud83d\udd17\ud83d\udccf <p>Load parameters</p> <p>If you have a parameter configuration file, you can load it in via the <code>Load parameter</code> button in the bottom-left of the window.</p> <p>We begin the setting of analysis parameters by inputting the data collection parameters.</p> <p>Mass Accuracy:</p> Metabolomics Lipidomics MS1 tolerance 0.002 Da 0.002 Da MS2 tolerance 0.002 Da 0.002 Da <p>Data collection parameters:</p> Metabolomics Lipidomics Retention time begin 0 min 0 min Retention time end 40 min 40 min MS1 mass range begin 50 Da 200 Da MS1 mass range end 1000 Da 1300 Da MS/MS mass range begin 50 Da 200 Da MS/MS mass range end 1000 Da 1300 Da Execute retention time correction FALSE <p>Retention time correction option</p> <p>You can choose here to perform retention time correction. This should typically be set to <code>FALSE</code> for at least your initial processing, and always set to <code>FALSE</code> if you do not have information about the analytical order of your samples.</p> <p>Isotope recognition:</p> Metabolomics Lipidomics Maximum charged number 2 2 Consider Cl and Br elements FALSE FALSE <p>Multithreading (will depend on your machine):</p> Metabolomics Lipidomics Number of threads 8 8 Execute retention time corrections FALSE FALSE <p></p> <p>Next we select the minimum peak height threshold. Peaks below this threshold will not be retained. A value of <code>100,000</code> is recommended for data acquired by Thermo Fisher Scientific machines. However, this will vary by apparatus, and may require data-dependant tuning.</p> <p>We will leave the <code>'Mass slice width'</code> value to the default, along with all options in the drop-down <code>'Advanced'</code> menu.</p> Metabolomics Lipidomics Minimum peak height 100000 100000 Mass slice width 0.05 Da 0.05 Da <p></p> <p>The default values are suitable.</p> Metabolomics Lipidomics Sigma window value 0.5 0.5 MS/MS abundance cut off 0 0 Exclude after precursor ion TRUE TRUE Keep the isotopic ions until 5 Da 5 Da Keep the isotopic ions w/o MS2Dec FALSE FALSE <p></p> <p>In this tab, you will add your databases as appropriate. One of these will be the appropriate <code>.msp</code> MassBank database file (either positive or negative depending on the ionisation mode you are currently running).</p> <p>Click the green tick next the <code>Database setting</code> heading. In the <code>DataBase</code> box, select <code>Msp</code> as your database type and then navigate and select the appropriate MassBank file using the <code>Browse</code> button. The <code>Database name</code> and other values will be automatically populated.</p> <p>We will now adjust a number of parameters (the lipidomics columns are left blank below as you will not need to input or change anything).</p> <p>Retention time tolerance</p> <p>Keep in mind that the <code>Retention time tolerance</code> parameter can be adjusted in a project-specific manner. Using a value of about 2 minutes has worked well in the past, but may require fine-tuning. Setting the value lower will result in a greater number of highly similar features, and may result in issues downstream during manual curation steps.</p> <p>MS/MS identification setting:</p> Metabolomics Lipidomics Accurate mass tolerance (MS1) 0.002 Da Accurate mass tolerance (MS2) 0.002 Da Retention time tolerance 2 min <p>We will leave the other drop-down menus with their default settings.</p> <p></p> <p>In addition, you can select a text file containing the standards run during data acquisition. This file should contain three columns, including the metabolite name, m/z, and retention time (in that order). These columns should be named <code>Metabolite</code>, <code>MZ</code>, and <code>RT</code>.</p> <p>Click the green tick again to add another database. This time, choose <code>Text</code> as your database type. Navigate and select your standards file, and the other fields will be populated.</p> <p>Once again, we will change a few parameters:</p> <p>Tolerance:</p> Metabolomics Lipidomics Accurate mass tolerance (MS1) 0.002 Da Retention time tolerance 2 min <p></p> <p>Here we will select the appropriate adduct ion settings for our runs.</p> MetabolomicsLipidomics Metabolomics Positive Metabolomics Negative [M+H]+ [M-H]- [M+NH4]+ [M+Na-2H]- [M+Na]+ [M+Cl]- Lipidomics Positive Lipidomics Negative [M+H]+ [M-H]- [M+NH4]+ <p></p> <p>Here, you should rename the <code>'Result name:'</code> to something more recognisable. For example, <code>alignment_data_sampletype_ionisation</code>.</p> <p>Set the <code>'Reference file:'</code> to your second QC sample. The first QC sample is typically different than the others, and this will affect your results.</p> <p>The <code>'Retention time tolerance:'</code> parameter will be data-dependant, however a good start is to try either <code>2 min</code> or <code>1 min</code>.</p> Metabolomics Lipidomics Retention time tolerance 2 min 2 min MS1 tolerance 0.002 Da 0.002 Da <p>There are also some other parameters we need to check in the <code>'Advanced'</code> drop-down menu. Because we don't remove the features based on blank information, the following three tick boxes are 'greyed out', and not available to change. You do not need to change the default options for these.</p> <p>Gap filling</p> <p>We will use <code>Gap filling by compulsion</code>, as it improves peak alignment (often resulting in fewer highly similar features), but you may choose to skip this step. The choice will largely be data-dependent, and with a higher retention time tolerance, you may not see much of a difference.</p> Metabolomics Lipidomics Retention time factor 0.5 0.5 MS1 factor 0.5 0.5 Peak count filter 0 % 0 % N% detected in at least one group 0 % 0 % Remove features based on blank information FALSE FALSE Sample max / blank average 5 5 Keep 'reference matched' metabolite features FALSE FALSE Keep 'suggested (w/o MS2)' metabolite features FALSE FALSE Keep removable features and assign the tag FALSE FALSE Gap filling by compulsion TRUE TRUE <p></p>"},{"location":"LCMS/msdial-processing/#run-the-pipeline","title":"Run the pipeline \ud83c\udfc3\ud83d\udcbb","text":"<p>Once you have finished setting up the analysis parameters, click <code>'Finish'</code> to begin the run.</p> <p>When the run finishes, the data will appear on the screen. On the left-hand side of the screen, you will see an <code>'Alignment navigator'</code> box. Double click the file that is inside to load all of your data at once.</p>"},{"location":"LCMS/msdial-processing/#viewing-and-exporting-data","title":"Viewing and exporting data \ud83d\udc40\ud83d\udcbe","text":"<p>The <code>'Show ion table'</code> button in the middle of the screen is a good place to start to see how many metabolites/lipids (features) were found during the run. You can click the <code>'Metabolite name'</code> column to group all features that were annotated.</p>"},{"location":"LCMS/msdial-processing/#aligned-spot-information","title":"Aligned spot information \ud83d\udccd\ud83d\udcdd","text":"<p>If you want to view more information about a specific feature, you can single click on the row within the ion table list.</p> <p>At the top of the screen, the default image shown is the <code>'Bar chart of aligned spot (OH)'</code>. This will show you the average intensities of each of your <code>'Class IDs'</code>.</p> <p>To view the actual peak itself, you can click the adjacent <code>'EIC of aligned spot'</code> tab to view and assess the quality of the peak. Good quality peaks, especially for smaller weight features, are tight and well-aligned (see the image below).</p> <p>Next, if you want to see the individual peaks for each of your samples, you can right-click on the peak window and click the <code>Peak curation (Sample table)</code> option. This will show you the peak for that feature for each sample so you can further assess their quality. This will be necessary for manual curation of features later to confirm confidence in the annotation of significant findings.</p> <p>After downstream pre-processing, you will come back to MS-DIAL and manually curate each of the features, removing those with poor-quality spectra. It should be noted that without manual curation, your ordination plots (e.g. PCoA) will be affected by remaining poor quality features.</p> <p></p>"},{"location":"LCMS/msdial-processing/#exporting-for-downstream-processing-and-analysis","title":"Exporting for downstream processing and analysis \ud83d\udce4","text":"<p>There are a few useful things we can routinely export:</p> <ul> <li>The raw data height matrix (contains all of our intensity values and feature information).</li> <li>Analysis parameters (to streamline future analysis set-up).</li> </ul> <p>Export as CSV</p> <p>For consistency and easy import into R, best practice is to save the height matrix output in <code>.csv</code> format.</p>"},{"location":"LCMS/msdial-processing/#height-table-export","title":"Height table export \ud83d\udcca\ud83d\udcc4","text":"<p>To export the raw height matrix table, navigate to the <code>'Alignment result'</code> option within the <code>'Export'</code> tab in the top menu.</p> <p>Select the directory for export and leave all other options to their defaults. Change the <code>Export format</code> to <code>csv</code>, and click <code>'Export'</code>.</p> <p></p>"},{"location":"LCMS/msdial-processing/#parameter-export","title":"Parameter export \ud83d\udee0\ufe0f\ud83d\udce4","text":"<p>To export the analysis parameters, open the green tab at the top-left of the screen with the new project icon, select <code>Save</code>, and then select the <code>Save parameter option</code>.</p> <p></p>"},{"location":"LCMS/msdial-processing/#next-steps","title":"Next steps \u27a1\ufe0f","text":"<p>Now that you have your height matrices (positive and negative ionisation modes), you can proceed to automated quality control steps, using Peak Matrix Processing.</p>"},{"location":"LCMS/msdial-processing/#rights","title":"Rights","text":"<ul> <li>Copyright \u00a9\ufe0f 2024 Mucosal Immunology Lab, Monash University, Melbourne, Australia.</li> <li><code>pmp</code> package: Jankevics A, Lloyd GR, Weber RJM (2021). pmp: Peak Matrix Processing and signal batch correction for metabolomics datasets. R package version 1.4.0.</li> <li>License: This pipeline is provided under the MIT license.</li> <li>Authors: M. Macowan and C. Pattaroni</li> </ul>"},{"location":"LCMS/pmp-quality-control/","title":"Peak Matrix Processing for raw MS-DIAL height matrices","text":"<p>The raw height matrices output by MS-DIAL will typically contain thousands (if not tens of thousands) of so-called \"uninformative\" and \"irreproducible\" features. Features such as these could hinder downstream analyses, including statistical analysis, biomarker discovery, or pathway inference.</p> <p>Therefore the common practice is to apply peak matrix validation and filtering procedures as described in Di Guida et al.  (2016), Broadhurst et al. (2018), and Schiffman et al. (2019).</p> <p>Functions within the <code>pmp</code> (Peak Matrix Processing) package are designed to help users to prepare data for further statistical data analysis in a fast,  easy to use and reproducible manner.</p>"},{"location":"LCMS/pmp-quality-control/#preprocessing-script","title":"Preprocessing script \ud83d\udcdc","text":"<p>An R function to handle the various steps is available as <code>pmp_preprocess()</code>, and will be explained in-depth below.</p> Parameters for <code>pmp_preprocess</code> Parameter Description <code>pos_df</code> The positive ionisation intensity <code>data.frame</code>. <code>neg_df</code> The negative ionisation intensity <code>data.frame</code>. <code>metadata</code> The metadata <code>data.frame</code>. <code>samples_key</code> Default: <code>'Sample'</code> \u2013 a universal string that is unique to the start of all samples (and not blanks or QCs etc.). <code>intens_cols</code> A vector of column numbers corresponding to columns with intensity values. <code>info_cols</code> A vector of column numbers corresponding to the peak information. <code>blankFC</code> Default: <code>5</code> \u2013 the minimum fold change above the average intensity of blanks in order for peaks (and samples) to be retained. <code>max_perc_mv</code> Default: <code>0.8</code> \u2013 the maximum percentage of peaks that can be missing for a sample to be retained. <code>missingPeaksFraction</code> Default: <code>0.8</code> \u2013 the maximum percentage of samples that can contain a missing value for a peak to be retained. <code>max_rsd</code> Default: <code>25</code> \u2013 the maximum relative standard deviation of intensity values for a given peak within specified QC samples for that peak to be retained. <code>mv_imp_rowmax</code> Default: <code>0.7</code> \u2013 the maximum percentage of missing values allowable in a given row for imputation. <code>mv_imp_colmax</code> Default: <code>0.7</code> \u2013 the maximum percentage of missing values allowable in a given column for imputation. <code>mv_imp_method</code> Default: <code>knn</code> \u2013 the method to be used for imputation."},{"location":"LCMS/pmp-quality-control/#r-processing","title":"R processing \ud83e\uddee","text":""},{"location":"LCMS/pmp-quality-control/#environment-setup","title":"Environment setup \ud83d\udee0\ufe0f\ud83c\udf31","text":"<p>Firstly, we need to load the required packages. We will also display their versions for future reference.</p> Load the required packages<pre><code># Get the R version\nversion$version.string\n\n# Define a vector of required packages\npkgs &lt;- c('here', 'data.table', 'tidyverse', 'kableExtra', 'ggplot2', 'pmp', 'SummarizedExperiment', 'S4Vectors')\n\n# For each of the packages, load it and display the version number\nfor (pkg in pkgs) {\n  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n  print(paste0(pkg, ': ', packageVersion(pkg)))\n}\n\n# Set the seed for generation of pseudo-random numbers\nset.seed(2)\n</code></pre> <p>Example output:</p> <pre><code>[1] \"R version 4.0.5 (2021-03-31)\"\n[1] \"here: 1.0.1\"\n[1] \"data.table: 1.14.0\"\n[1] \"tidyverse: 1.3.1\"\n[1] \"kableExtra: 1.3.4\"\n[1] \"ggplot2: 3.3.5\"\n[1] \"pmp: 1.2.1\"\n[1] \"SummarizedExperiment: 1.20.0\"\n[1] \"S4Vectors: 0.28.1\"\n</code></pre>"},{"location":"LCMS/pmp-quality-control/#data-import","title":"Data import \ud83d\udce5","text":"<p>The data we will be importing is the height matrix data output from the MS-DIAL pipeline. This output will be imported into a <code>SummarizedExperiment</code> object for pre-processing using the <code>pmp</code> package.</p>"},{"location":"LCMS/pmp-quality-control/#metadata-optional-at-this-stage","title":"Metadata (optional at this stage) \ud83d\uddc2\ufe0f","text":"<p>Why should I include metadata now?</p> <p>While this step is optional at this stage, the benefit of supplying metadata to the <code>SummarizedExperiment</code> object is that your metadata will also be filtered by the pre-processing steps to match the remaining samples and features at the end.</p> <p>Firstly, we will import and prepare the metadata component of our <code>SummarizedExperiment</code> object. For simplicity and economy of code, we can specify the column types of our metadata using the <code>col_types</code> parameter in the <code>read_csv()</code> function. These will usually be one of:</p> <ul> <li><code>'c'</code> = character</li> <li><code>'f'</code> = factor</li> <li><code>'n'</code> = numeric</li> <li><code>'l'</code> = logical (i.e. boolean)</li> </ul> <p>In this example, we will be working with LCMS data from a set of stool samples that were collected from different patients.</p> <p>There are two metadata files: one that contains information about the samples themselves  (e.g. sample name and ID, sampling site, and age at collection), and one that contains information about the patient  (e.g. gender, disease type, other clinical metadata etc.). We will combine these two tables to produce a single metadata file for input into the <code>SummarizedExperiment</code> object.</p> Import clinical metadata<pre><code># Import the patient metadata (the 'here' function makes defining file paths much easier)\nmetadata_patient &lt;- read_csv(here::here('data', 'patient_metadata.csv'), col_types = 'ffffnnccfnc')\n\n# Import the metabolomics sample metadata\nmetadata_metab_stool &lt;- read_csv(here::here('data', 'metadata_metabolomics.csv'), col_types = 'ffffn') %&gt;%\n  filter(origin == 'stool') %&gt;% # filter for the stool metabolomics metadata alone (not from other sites)\n  left_join(metadata_patient, by = c('patient' = 'ID')) %&gt;% # match patient metadata onto the sample metadata rows using a matched column\n  column_to_rownames(var = 'sample') # choose the value that matches the MS-DIAL height matrix column names to be the rownames\n</code></pre>"},{"location":"LCMS/pmp-quality-control/#stool-metabolomics-data","title":"Stool metabolomics data \ud83d\udca9","text":"<p>We will load in both the positive and negative ionisation mode height matrices from the MS-DIAL pipeline. We need to remove the MS/MS columns, as these were acquired in a different manner to the other columns.</p> <p>Then, we can <code>rbind</code> the two tables together after denoting the original ionisation mode in the row names. Because the QC data is incorporated within each feature (row), <code>pmp</code> can pre-process all of our data at once, and normalise the entire dataset.</p> Load the peak height tables from MS-DIAL<pre><code># Load the metabolomics height data\nmetab_stool_pos &lt;- read_csv(here::here('data', 'stool_metabolites_height_positive.csv'), skip = 4) # skip the first 4 rows\nmetab_stool_neg &lt;- read_csv(here::here('data', 'stool_metabolites_height_negative.csv'), skip = 4)\n</code></pre>"},{"location":"LCMS/pmp-quality-control/#run-the-preprocessing-script","title":"Run the preprocessing script \ud83c\udfc3","text":"<p>At this point, we can simply run the <code>pmp_preprocess()</code> function. You will just need to identify which of your columns relate to what, and this could change depending on the exact MS-DIAL version you are using.</p> <ol> <li>Information columns: these contain general information including mass, retention time, signal-to-noise ratio, spectrum fill percentage etc. These may be columns <code>1:32</code>.</li> <li>Intensity columns: these contain the actual intensity values associated with each sample-peak pair, and will vary depending on your LCMS run. </li> </ol> <p>Don't include the averaged columns</p> <p>Ensure you do not include the group averages, and only include individual samples, blanks, and QC samples</p> <pre><code># Process stool data\nmetab_stool_pmp &lt;- pmp_preprocess(pos_df = metab_stool_data_pos,\n                                  neg_df = metab_stool_data_neg,\n                                  metadata = metab_metadata,\n                                  samples_key = 'stool',\n                                  intens_cols = c(33:38, 45:60, 67:76),\n                                  info_cols = 1:32,\n                                  mv_imp_method = 'rf')\n\n# Save output data\nsaveRDS(metab_stool_pmp, here::here('output', 'temp', 'metab_stool_pmp_temp.rds'))\n</code></pre> <p>Skip ahead</p> <p>If you run the script here, you can skip down to the principle component analysis step.</p>"},{"location":"LCMS/pmp-quality-control/#detailed-steps","title":"Detailed steps \ud83d\udcdd","text":"<p>If you have a more complex setup, or would rather just do things manually to fine tune each step, we describe here the individual steps that the <code>pmp_process()</code> function is performing.</p>"},{"location":"LCMS/pmp-quality-control/#continue-manual-data-import","title":"Continue manual data import \ud83d\udd04","text":"Combine ionisation modes and prepare intensity and peak tables<pre><code># Remove the MS/MS samples (not acquired in the same way)\nmetab_stool_pos &lt;- metab_stool_pos[, !(names(metab_stool_pos) %in% c('MSMS_pos', 'MSMS_neg'))]\nmetab_stool_neg &lt;- metab_stool_neg[, !(names(metab_stool_neg) %in% c('MSMS_pos', 'MSMS_neg'))]\n\n# Separate into intensity and information data.frames for the SummarizedExperiment object\nmetab_stool_pos_counts &lt;- as.matrix(metab_stool_pos[, c(33:54, 62:165)]) # you need to find the column numbers that contain Blanks, QCs, and samples\nmetab_stool_neg_counts &lt;- as.matrix(metab_stool_neg[, c(33:54, 62:165)])\n\nmetab_stool_pos_info &lt;- metab_stool_pos[, 1:32]\nmetab_stool_neg_info &lt;- metab_stool_neg[, 1:32]\n\n# Rename the data to indicate ionisation mode (using the MS-DIAL alignment ID)\nstool_pos_rownames &lt;- paste0(metab_stool_pos_info$`Alignment ID`, '_pos')\nstool_neg_rownames &lt;- paste0(metab_stool_neg_info$`Alignment ID`, '_neg')\n\nrownames(metab_stool_pos_counts) &lt;- stool_pos_rownames\nrownames(metab_stool_pos_info) &lt;- stool_pos_rownames\nrownames(metab_stool_neg_counts) &lt;- stool_neg_rownames\nrownames(metab_stool_neg_info) &lt;- stool_neg_rownames\n\n# Merge the postive and negative ionisation modes using rbind\nmetab_stool_counts &lt;- rbind(metab_stool_pos_counts, metab_stool_neg_counts)\nmetab_stool_info &lt;- rbind(metab_stool_pos_info, metab_stool_neg_info)\n</code></pre> <p>Now that we have our intensity and feature information datasets, before we can import them into a <code>SummarizedExperiment</code> object, we need to define class and group vectors so that <code>pmp</code> knows whether our variables are samples, QCs, or blanks.</p> <p>The easiest way to achieve this is by getting the first two characters (a substring) of our column names, and using these as indicators of the classes: <code>'Bl'</code> = blank, <code>'QC'</code> = QC etc.</p> Define sample classes and groups<pre><code># Create class and group vectors\nmetab_stool_class &lt;- substr(colnames(metab_stool_counts), start = 1, stop = 2)\nmetab_stool_group &lt;- substr(colnames(metab_stool_counts), start = 1, stop = 2)\n</code></pre> Why didn't this work for me? <p>If you have samples that start with the same two letters as our ideal blanks (<code>'Bl'</code>) and quality control (<code>'QC'</code>) samples, then this process will not work. For example, if you have blood/serum samples that start with <code>'Bl'</code>, then these will need to be renamed. Alternatively, you may wish to manually define your classes and groups.</p>"},{"location":"LCMS/pmp-quality-control/#import-into-a-summarizedexperiment-object","title":"Import into a SummarizedExperiment object \ud83d\udce6","text":"<p>Now that we have our individual components ready, we can check that the metadata matches our sample data, and then import everything into a single, unified <code>SummarizedExperiment</code> object for pre-processing with the <code>pmp</code> package.</p> Assemble a SummarizedExperiment object<pre><code># Reorder the metadata rows so that they match the column order of samples in the counts data.frame (ignoring blanks and QCs)\nmetadata_metab_stool &lt;- metadata_metab_stool[colnames(metab_stool_counts)[23:126], ] # use only column containing sample intensities\n\n# Check that the metadata matches the samples\nidentical(rownames(metadata_metab_stool), colnames(metab_stool_counts)[23:126]) # should return 'TRUE'\n\n# Create the SummarizedExperiment (SE) object\nmetab_stool_SE &lt;- SummarizedExperiment(assays = list(counts = metab_stool_counts),\n                                       metadata = list(metadata_metab_stool),\n                                       rowData = list(info = metab_stool_info),\n                                       colData = DataFrame(class = metab_stool_class))\n</code></pre>"},{"location":"LCMS/pmp-quality-control/#filtering-on-blank-intensity","title":"Filtering on blank intensity \ud83e\uddea\u26d4","text":"<p>The first filtering steps we will carry out are to replace any so-called \"missing\" values (i.e. 0 values) with <code>NA</code> so they will be compatible with downstream filtering steps.</p> <p>After this, we can filter peaks based on the intensity values of our blanks.</p> Filter on blank intensity<pre><code># Check the original number of features\ndim(metab_stool_SE)\n\n# Replace missing values with NA to be compatible with downstream filtering\nassay(metab_stool_SE) &lt;- replace(assay(metab_stool_SE), assay(metab_stool_SE) == 0, NA)\n\n# Filter peaks and optionally samples based on blanks\nmetab_stool_filt &lt;- filter_peaks_by_blanks(df = metab_stool_SE,\n                                           fold_change = 5, # 5-fold change\n                                           classes = metab_stool_SE$class,\n                                           remove_samples = TRUE,\n                                           remove_peaks = TRUE,\n                                           blank_label = 'Bl') # from the class vector\n\n# Check the number of features/samples remaining\ndim(metab_stool_filt)\n</code></pre> <p>Example output (dimension checks):</p> <pre><code>[1] 81334   126 # Original\n[1] 71700   115 # After blank intensity filtering\n</code></pre>"},{"location":"LCMS/pmp-quality-control/#filtering-on-missing-values-and-qc-intensity-variation","title":"Filtering on missing values and QC intensity variation \ud83e\uddfa\ud83d\udd0e","text":"<p>Next, we can perform a few filtering steps based on missing values and degree of variation in the QC samples.</p> <ul> <li><code>filter_samples_by_mv</code>: removal of samples containing a user-defined maximum percentage of missing values see documentation. E.g. setting the <code>max_perc_mv</code> argument to 0.8 will remove all samples with at least 80% missing values.</li> <li><code>filter_peaks_by_fraction</code>: removal of peaks based upon the relative proportion of samples containing non-missing values see documentation.</li> <li><code>filter_peaks_by_rsd</code>: removal of peaks based upon relative standard deviation of intensity values for a given feature within specified QC samples see documentation.</li> </ul> <p>Relative standard deviation</p> <p>As the name suggests, this value is dependent on the standard deviation that exists in your QC samples. As such, if there is substantial variation in your QC samples you have a couple of options:</p> <ul> <li>Increase the <code>max_rsd</code> value \u2013 this is a threshold of the QC RSD% value, so maybe try <code>40</code> if <code>25</code> is too low.</li> <li>Identify problematic QC samples \u2013 see if you can work out what the issue is, and remove that particular QC sample if appropriate.</li> </ul> Continue filtering process<pre><code># Filter samples based on the percentage of missing values\nmetab_stool_filt &lt;- filter_samples_by_mv(df = metab_stool_filt,\n                                         max_perc_mv = 0.8) # remove samples where &gt;80% of values are missing \n\n# Check the number of features/samples\ndim(metab_stool_filt)\n\n# Filter peaks based on missing values across all samples\nmetab_stool_filt &lt;- filter_peaks_by_fraction(df = metab_stool_filt,\n                                             min_frac = 0.8, # features should have values for &gt;80% of samples\n                                             classes = metab_stool_filt$class,\n                                             method = 'across')\n\n# Check the number of features/samples\ndim(metab_stool_filt)\n\n# Filter peaks based on the percentage of variation in the QC samples\nmetab_stool_filt &lt;- filter_peaks_by_rsd(df = metab_stool_filt,\n                                        max_rsd = 25,\n                                        classes = metab_stool_filt$class,\n                                        qc_label = 'QC')\n\n# Check the number of features/samples\ndim(metab_stool_filt)\n</code></pre> <p>Example output (dimension checks):</p> <pre><code>[1] 71700   115 # After filtering for missing values within peaks\n[1] 17315   115 # After filtering for missing values across samples\n[1] 11648   115 # After filtering out high variability in QC samples\n</code></pre>"},{"location":"LCMS/pmp-quality-control/#pqn-normalisation-and-glog-scaling","title":"PQN normalisation and glog scaling \ud83d\udcca\u2696\ufe0f","text":"<p>Finally we can normalise our data using probabilistic quotient normalisation (PQN) (see documentation), followed by missing value imputation (various method choices available; see documentation),  and then transform the data using a variance-stabling generalised logarithmic transformation (see documentation).</p> <p>What is a glog transformation?</p> <p>Generalised logarithmic (glog) transformation is a statistical technique that applies a modified logarithmic function to raw intensity data, thereby stabilising variance across both low and high intensity mass spectral features. The <code>glog_transformation</code> function uses QC samples to optimise the scaling factor lambda, and using the <code>glog_plot_optimised_lambda</code> function it\u2019s possible to visualise if the optimisation of the given parameter has converged at the minima.</p> PQN normalise, impute missing values, and glog transform data<pre><code># PQN data normalisation\nmetab_stool_norm &lt;- pqn_normalisation(df = metab_stool_filt,\n                                      classes = metab_stool_filt$class,\n                                      qc_label = 'QC')\n\n# Missing values imputation\nmetab_stool_imp &lt;- mv_imputation(df = metab_stool_norm,\n                                 rowmax = 0.7, # max % of missing data allowed in any row\n                                 colmax = 0.7, # max % of missing data allowed in any column\n                                 method = 'knn') # or rf, bcpa, sv, mn, md.\n\n# Data scaling\nmetab_stool_glog &lt;- glog_transformation(df = metab_stool_imp,\n                                        classes = metab_stool_imp$class,\n                                        qc_label = 'QC')\n\nopt_lambda_stool &lt;- processing_history(metab_stool_glog)$glog_transformation$lambda_opt\n\nglog_plot_optimised_lambda(df = metab_stool_imp,\n                           optimised_lambda = opt_lambda_stool,\n                           classes = metab_stool_imp$class,\n                           qc_label = 'QC')\n</code></pre> <p>Example plot:</p> <p>We can also check the number of assigned metabolites that remain.</p> Check number of named features<pre><code># Recover RDS object if you used the pmp_preprocess function\nmetab_stool_pmp &lt;- readRDS(here::here('output', 'temp', 'metab_stool_pmp_temp.rds'))\nmetab_stool_glog &lt;- metab_stool_pmp$glog_results\n\n# Number of assigned metabolites\ntable(rowData(metab_stool_glog)@listData[['info.Metabolite name']] != 'Unknown')\n</code></pre> <p>Example output (189 metabolites with assignments remain):</p> <pre><code>FALSE  TRUE \n11459   189 \n</code></pre>"},{"location":"LCMS/pmp-quality-control/#principle-component-analysis","title":"Principle component analysis \ud83d\udcc8","text":"<p>We can now visualise the output data using a principle component analysis plot (PCA) plot.</p> Generate a PCA plot to visualise QC clustering<pre><code># Recover RDS object if you used the pmp_preprocess function\nmetab_stool_pmp &lt;- readRDS(here::here('output', 'temp', 'metab_stool_pmp_temp.rds'))\nmetab_stool_glog &lt;- metab_stool_pmp$glog_results\n\n# Perform the PCA and retrieve the explained variance values\nPCA_stool &lt;- prcomp(t(assay(metab_stool_glog)), center = TRUE)\nvarexp_stool &lt;- c(summary(PCA_stool)$importance[2,1]*100,\n                  summary(PCA_stool)$importance[2,2]*100)\n\n# Create a dataset for plotting\ndata_PCA_stool &lt;- cbind(data.frame(Samples = rownames(PCA_stool$x),\n                                   PC1 = PCA_stool$x[,1],\n                                   PC2 = PCA_stool$x[,2]),\n                        class = metab_stool_glog$class)\n\n# Plot results\n(PCA_stool_plot &lt;- ggplot(data_PCA_stool, aes(x = PC1, y = PC2)) +\n    geom_point(aes(fill = class, color = factor(class))) +\n    stat_ellipse(aes(fill = class), geom = 'polygon', type = 't', level = 0.9, alpha = 0.2) +\n    labs(title = 'Stool Metabolomics',\n         x = paste0('PC1 ', round(varexp_stool[1], 2), '%'),\n         y = paste0('PC2 ', round(varexp_stool[2], 2), '%'))\n)\n</code></pre> <p>Example plot:</p>"},{"location":"LCMS/pmp-quality-control/#next-steps","title":"Next steps \u27a1\ufe0f","text":"<p>Now that you have finished initial quality control, and have an imputed, glog-normalised dataset, you can proceed to secondary MS1 feature identification.</p>"},{"location":"LCMS/pmp-quality-control/#rights","title":"Rights","text":"<ul> <li>Copyright \u00a9\ufe0f 2024 Mucosal Immunology Lab, Monash University, Melbourne, Australia.</li> <li><code>pmp</code> package: Jankevics A, Lloyd GR, Weber RJM (2021). pmp: Peak Matrix Processing and signal batch correction for metabolomics datasets. R package version 1.4.0.</li> <li>License: This pipeline is provided under the MIT license.</li> <li>Authors: M. Macowan and C. Pattaroni</li> </ul>"},{"location":"LCMS/secondary-feature-annotation/","title":"Secondary MS1 feature annotation","text":"<p>After processing LCMS data through the MS-DIAL pipeline, secondary annotations for MS1 features can be obtained by matching mass data to external databases. </p> What do you mean by MS1 data? <ul> <li>What is MS1 data?<ul> <li>MS1 data represents the initial scan in mass spectrometry, where ions are detected based on their mass-to-charge ratio (m/z). It provides an overview of all molecular features in a sample, including their m/z values and intensities, enabling detection and quantification of compounds.</li> </ul> </li> <li>How does that differ from MS/MS (MS2) data?<ul> <li>While MS1 provides a broad profile of ions without structural information, MS/MS involves selecting specific ions from the MS1 scan (precursor ions), fragmenting them, and analysing the resulting product ions. This fragmentation reveals structural details, enabling precise compound identification and differentiation of isomers.</li> </ul> </li> </ul> <p>Version 2 of the secondary annotation functions available</p> <p>Please update your script files for the <code>add_hmdb()</code> and <code>add_lmsd()</code> functions for new runs. Updated with additional adduct values for automated adduct-specific correction to monoisotopic masses. This is important for accurate secondary assignments.</p> <p>17 May, 2025</p>"},{"location":"LCMS/secondary-feature-annotation/#secondary-annotation-with-hmdb","title":"Secondary annotation with HMDB \ud83d\udcd8","text":"<p>You should currently have a <code>SummarizedExperiment</code> object that has been pre-processed with <code>pmp</code>. The next stage is to match the MS1 mass data for each feature to the Human Metabolome Database (HMDB) database file.</p> <p>Prepared HMDB database file</p> <p>Download the prepared HMDB database file in RDS format here.</p> <ul> <li>Version 5.0 (November 2023) \u2013 pre-formatted and filtered to include only annotated or documented features.</li> </ul>"},{"location":"LCMS/secondary-feature-annotation/#appending-hmdb-annotations-to-summarizedexperiment-objects","title":"Appending HMDB annotations to <code>SummarizedExperiment</code> objects","text":"<p>The first step is to load the formatted HMDB <code>data.frame</code> into your R session.</p> Load HMDB database into your R session<pre><code># Load HMDB dataset\nhmdb_df &lt;- readRDS(here::here('hmdb', 'hmdb_metabolites_detect_quant_v5_20231102.rds'))\n</code></pre> <p>Then, using the <code>add_hmdb()</code> function, we can search the HMDB annotations in the <code>data.frame</code> and add them to our <code>SummarizedExperiment</code> objects, as shown below with an example stool metabolomics object.</p> Parameters for <code>add_hmdb()</code> Parameter Description <code>metab_SE</code> The <code>SummarizedExperiment</code> object you want to annotate with HMDB. <code>hmdb</code> The HMDB database object. <code>mass_tol</code> Default: <code>0.002</code> \u2013 the mass tolerance allowed for annotation. <code>cores</code> The number of parallel processes to use if desired. <p>Run time</p> <p>Running <code>add_hmdb()</code>, especially without parallel processing, can take a very long time.</p> <p>Which <code>SummarizedExperiment</code> should I use?</p> <p>If you are using the output of the <code>pmp_preprocess()</code> function, you should extract and annotate the <code>glog_results</code>. The standard practice is to use the glog-transformed data from here on.</p> Add HMDB secondary annotations<pre><code># Extract the glog data from the pmp_preprocess output\nmetab_stool_glog &lt;- metab_stool_pmp$glog_results\n\n# Search annotations in HMDB and add to the SE objects\nmetab_stool_glog &lt;- add_hmdb(metab_SE = metab_stool_glog,\n                             hmdb = hmdb_df, \n                             mass_tol = 0.002,\n                             cores = 6)\n</code></pre>"},{"location":"LCMS/secondary-feature-annotation/#secondary-annotation-with-lipid-maps","title":"Secondary annotation with LIPID MAPS \ud83e\uddc8","text":"<p>After processing lipidomics data through MS-DIAL, you can enhance the annotations of MS1 features by leveraging the LIPID MAPS Structure Database (LMSD). At this point, you should have a <code>SummarizedExperiment</code> object containing preliminary annotations and those from the HMDB database. The next step involves matching the MS1 mass data of each feature to the entries in the LMSD database.</p> <p>Prepared LMSD database file</p> <p>Download the prepared LMSD database file in RDS format here.</p> <ul> <li>Version 2022-02-16</li> </ul> <p>Should I run this section?</p> <p>You should definitely run this step if you have lipidomics data to process. If you are processing metabolomics data, you can skip this section.</p>"},{"location":"LCMS/secondary-feature-annotation/#appending-lipid-maps-annotations-to-summarizedexperiment-objects","title":"Appending LIPID MAPS annotations to <code>SummarizedExperiment</code> objects","text":"<p>The first step is to load the formatted LMSD <code>data.frame</code> into your R session.</p> Load LMSD database into your R session<pre><code># Load LMSD dataset\nlmsd_df &lt;- readRDS(here::here('lmsd', 'LMSD_231107.rds'))\n</code></pre> <p>Then, using the <code>add_lmsd()</code> function, we can search the LIPID MAPS annotations in the <code>data.frame</code> and add them to our <code>SummarizedExperiment</code> objects, as shown below with an example stool metabolomics object.</p> Parameters for <code>add_lmsd()</code> Parameter Description <code>metab_SE</code> The <code>SummarizedExperiment</code> object you want to annotate with HMDB. <code>lmsd</code> The LMSD database object. <code>mass_tol</code> Default: <code>0.002</code> \u2013 the mass tolerance allowed for annotation. <code>cores</code> The number of parallel processes to use if desired. <p>Run time</p> <p>Running <code>add_lmsd()</code>, especially without parallel processing, can take a very long time.</p> <pre><code># Search annotations in LMSD and add to the SE objects\n# Create list for all distinct Lipid Maps matching mz in tolerance range 0.002, an aggregated df of distinct lipids and a df to replace SummarizedExperiment metadata [rowData(metab_glog)]\nlmsd_ann_list &lt;- add_lmsd(metab_SE = metab_stool_glog, \n                          lmsd = lmsd_df, \n                          mass_tol = 0.002,\n                          cores = 6) \n\n# Use metadata_lmsd_table to replace the existing SE object metadata\nrowData(metab_stool_glog) &lt;- lmsd_ann_list$metadata_lmsd_table\n</code></pre>"},{"location":"LCMS/secondary-feature-annotation/#comparing-annotations-from-different-databases","title":"Comparing annotations from different databases \u2696\ufe0f\ud83d\udcda","text":"<p>To compare the assigned annotations from each of the methods the compare_annotations_SE() function. It will produce a <code>data.frame</code> containing only features with at least one annotation, and allow us see whether the annotations typically agree with each other.</p> Parameters for <code>compare_annotations_SE()</code> Parameter Description <code>metab_SE</code> The <code>SummarizedExperiment</code> object with secondary annotations. These should include HMDB for metabolomics data, and both HMDB and LMSD for lipidomics data. <code>mode</code> Either <code>'metabolomics'</code> or <code>'lipidomics'</code> depending on your dataset. <code>agg_lmsd_ann</code> The aggregated LMSD annotations you generated using the <code>add_lmsd()</code> function, i.e. <code>lmsd_ann_list$agg_lmsd_df</code>. Only required for <code>mode</code> = <code>'lipidomics'</code> MetabolomicsLipidomics Compare metabolite annotations<pre><code># Prepare data.frame with alignment IDs and annotations, and filter for at least one annotation\nanno_df_metab &lt;- compare_annotations_SE(metab_SE = metab_stool_glog,\n                                        mode = 'metabolomics')\n</code></pre> Compare lipid annotations<pre><code># Prepare data.frame with alignment IDs and annotations, and filter for at least one annotation\nanno_df_lipid &lt;- compare_annotations_SE(metab_SE = lipid_stool_glog,\n                                        mode = 'lipidomics',\n                                        agg_lmsd_ann = lmsd_ann_list$agg_lmsd_df)\n</code></pre>"},{"location":"LCMS/secondary-feature-annotation/#keeping-only-annotated-features","title":"Keeping only annotated features \ud83c\udff7\ufe0f\u2705","text":"<p>From here, we can filter our <code>SummarizedExperiment</code> object for features with at least one annotation. While the other features likely represent interesting metabolites and lipids, without an available annotation, they won't be interpretable downstream.</p> <p>We can achieve this providing our <code>SummarizedExperiment</code> object to the <code>keep_annotated_SE()</code> function, which will output a filtered <code>SummarizedExperiment</code> object.</p> Parameters for <code>keep_annotated_SE()</code> Parameter Description <code>metab_SE</code> The <code>SummarizedExperiment</code> object with secondary annotations. These should include HMDB for metabolomics data, and both HMDB and LMSD for lipidomics data. <code>mode</code> Either <code>'metabolomics'</code> or <code>'lipidomics'</code> depending on your dataset. MetabolomicsLipidomics <p>The function will create a new <code>rowData</code> element called <code>shortname</code>, and also assign this value as the preferred row name.</p> <ul> <li>It uses the following naming hierarchy to decide an appropriate name: HMDB &gt; MS-DIAL.</li> </ul> Remove unannotated features<pre><code># Keep only annotated rows and generate shortname column\nmetab_stool_glog &lt;- keep_annotated(metab_SE = metab_stool_glog,\n                                   mode = 'metabolomics')\n\n# Save the object\nsaveRDS(metab_stool_glog, here('output', '01_Preprocessing', 'metab_stool_glog_anno.rds'))\n</code></pre> <p>Are the other annotations still there?</p> <p>Yes! While the shorter names are succinct and useful for plotting, you can view the additional annotations at any time and alter as required.</p> <pre><code># Get HMDB and KEGG annotations\nhmdb_annotations &lt;- rowData(metab_stool_glog)$HMDB\nkegg_annotations &lt;- rowData(metab_stool_glog)$KEGG\n</code></pre> <p>The function will create a new <code>rowData</code> element called <code>shortname</code>, and also assign this value as the preferred row name.</p> <ul> <li>It uses the following naming hierarchy to decide an appropriate name: MS-DIAL &gt; LMSD &gt; HMDB.</li> <li>MS-DIAL has its own lipid database that is effective and, because it can also utilise MS/MS data in its annotations, is preferred here.</li> </ul> Remove unannotated features<pre><code># Keep only annotated rows and generate shortname column\nlipid_stool_glog &lt;- keep_annotated(metab_SE = lipid_stool_glog,\n                                   mode = 'lipidomics')\n\n# Save the object\nsaveRDS(lipid_stool_glog, here('output', '01_Preprocessing', 'lipid_stool_glog_anno.rds'))\n</code></pre> <p>Are the other annotations still there?</p> <p>Yes! While the shorter names are succinct and useful for plotting, you can view the additional annotations at any time and alter as required.</p> <pre><code># Get HMDB and KEGG annotations\nhmdb_annotations &lt;- rowData(lipid_stool_glog)$HMDB\nkegg_annotations &lt;- rowData(lipid_stool_glog)$KEGG\n</code></pre>"},{"location":"LCMS/secondary-feature-annotation/#next-steps","title":"Next steps \u27a1\ufe0f","text":"<p>You now have a normalised, imputed, dataset that has undergone secondary annotation and been filtered for annotated features. It is now time to proceed to manual curation of the annotated spectra.</p>"},{"location":"LCMS/secondary-feature-annotation/#rights","title":"Rights","text":"<ul> <li>Copyright \u00a9\ufe0f 2024 Mucosal Immunology lab, Monash University, Melbourne, Australia.</li> <li>HMDB version 5.0: Wishart DS, Guo A, Oler E, Wang F, Anjum A, Peters H, Dizon R, Sayeeda Z, Tian S, Lee BL, Berjanskii M. HMDB 5.0: the human metabolome database for 2022. Nucleic acids research. 2022 Jan 7;50(D1):D622-31.</li> <li>License: This pipeline is provided under the MIT license.</li> <li>Authors: M. Macowan and C. Pattaroni</li> </ul>"},{"location":"Microbiome/dada2_pipeline/","title":"DADA2 pipeline for processing raw 16S sequencing data","text":""},{"location":"Microbiome/dada2_pipeline/#overview","title":"Overview \ud83d\udcd6","text":"<p>Here we will run through the process for generating a sequencing variants table from raw FASTQ files generated by paired-end Illumina sequencing. For reproducibility, we also provide an <code>Rmarkdown</code> file that covers all the steps to produce the outputs required for assembling a <code>phyloseq</code> container object for your microbiome data.</p> Output Description <code>seqtab_nochim.rds</code> The clean counts table matrix <code>taxonomy_species.rds</code> The SILVA-assigned taxonomy table <code>tree.rds</code> The de novo phylogenetic tree <p>We have a Nextflow pipeline for that!</p> <p>If you are familiar with the <code>bash</code> command line interface, you may also want to try the Nextflow pipeline we have built which will automate the processes contained in this guide.</p>"},{"location":"Microbiome/dada2_pipeline/#preparation","title":"Preparation \ud83c\udfd7\ufe0f","text":""},{"location":"Microbiome/dada2_pipeline/#conda-environment","title":"Conda environment \ud83d\udc0d","text":"<p>Since we require the use of <code>illumina-utils</code> for demultiplexing our raw reads, it is recommended to create a conda environment to keep package versions separate from the global <code>base</code> environment. For faster environment creation, we will use the Miniforge3 package manager.</p> How can I install Miniforge3? Linux and MacOS \ud83d\udc27\ud83c\udf4fMacOS (Homebrew) \ud83c\udf7aWindows \ud83e\ude9f <ul> <li>Open a new terminal window, and download the appropriate installer for your computer's architecture using the <code>wget</code> command.</li> </ul> <pre><code>wget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\n</code></pre> <ul> <li>Run the installation script:</li> </ul> <pre><code>bash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre> <ul> <li>The interactive installation will prompt you to initialize conda with your shell.</li> </ul> <ul> <li>Ensure you have the Homebrew package manager installed.</li> </ul> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <ul> <li>Install miniforge:</li> </ul> <pre><code>brew install --cask miniforge\n</code></pre> <ul> <li>Download and run the Windows installer.</li> <li>Follow the prompts, taking note to:<ul> <li><code>Create start menu shortcuts</code></li> <li><code>Add Miniforge3 to my PATH environment variable</code> (unselected by default)</li> </ul> </li> </ul> <p>Info</p> <p>The latter option above is not selected by default due to potential conflicts with other software. Without Miniforge3 on the path, the most convenient way to use the installed software (such as commands conda and mamba) will be via the \"Miniforge Prompt\" installed to the start menu.</p> <p>Warning</p> <p>There are known issues with the usage of special characters and spaces in the installation location, see for example #484 on the Miniforge3 GitHub. We recommend users install in a directory without any such characters in the name.</p> <p>Let's create a new environment that houses the tools we need to run the pipeline.</p> Create a new environment for the DADA2 pipeline<pre><code># Create a new conda environment named dada2\nmamba create -n dada2 python=3.8 parallel git -y\n\n# Activate the environment\nmamba activate dada2\n\n# Install illumina-utils\npip install illumina-utils\n</code></pre>"},{"location":"Microbiome/dada2_pipeline/#sequencing-files","title":"Sequencing files \ud83e\uddec\ud83d\uddd2\ufe0f","text":"<p>Before starting, make sure you have the four files listed below for each run you want to process:</p> File Description <code>R1.fastq.gz</code> FASTQ file for the forward read <code>R2.fastq.gz</code> FASTQ file for the reverse read <code>Index.fastq.gz</code> FASTQ file for the index read <code>barcode_to_sample_[runNN].txt</code> A text file with no column names that maps index barcodes to samples <p>Things to pay attention to:</p> <ul> <li>FASTQ files must use the Phred+33 quality score format and sequences headers (@ lines) must fit the standard format of the CASAVA 1.8 output:</li> </ul> <pre><code>@EAS139:136:FC706VJ:2:2104:15343:197393 1:N:0:0\n</code></pre> <ul> <li>The <code>barcode_to_sample_[runNN].txt</code> file must contain two tab-delimited columns: the first for samples names and the second for samples barcodes. Avoid special characters, and ensure you do not include column names.</li> </ul> <p>An example directory layout is shown below, and is the format that the <code>Rmarkdown</code> file will be expecting. This layout future-proofs the directory by allowing other inputs, such as metadata or extra omics, to be stored in the same <code>input</code> folder without everything becoming too cluttered.</p> <pre><code>ProjectDirectory/\n\u251c\u2500\u2500 DADA2_pipeline.Rmd\n\u2514\u2500\u2500 input/\n    \u2514\u2500\u2500 01_dada2/\n        \u2514\u2500\u2500 run_data/\n            \u251c\u2500\u2500 run01/\n            \u2502   \u251c\u2500\u2500 R1.fastq.gz\n            \u2502   \u251c\u2500\u2500 R2.fastq.gz\n            \u2502   \u251c\u2500\u2500 Index.fastq.gz\n            \u2502   \u2514\u2500\u2500 barcode_to_sample_run01.txt\n            \u251c\u2500\u2500 run02/ #(if present)\n            \u2502   \u251c\u2500\u2500 R1.fastq.gz\n            \u2502   \u251c\u2500\u2500 R2.fastq.gz\n            \u2502   \u251c\u2500\u2500 Index.fastq.gz\n            \u2502   \u2514\u2500\u2500 barcode_to_sample_run02.txt\n            \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"Microbiome/dada2_pipeline/#silva-database","title":"SILVA database \ud83e\udda0\ud83d\uddc3\ufe0f","text":"<p>You will also require a DADA2-formatted reference database for assigning taxonomy and species-level annotation. The different options are available here.</p> <p>Make a note of the paths to each of these files, as you will need to enter these in the <code>Rmarkdown</code> file. We advise storing them in a dedicated databases folder rather than in your individual project directory, but that's up to you!</p> <p>Download the DADA2-formatted SILVA database</p> <p>We recommend the SILVA database: DADA2-formatted SILVA database version 138.2.</p> <ul> <li>You will require the following database file, which can be downloaded directly by clicking the link.<ul> <li><code>silva_nr99_v138.2_toSpecies_trainset.fa.gz</code></li> </ul> </li> </ul>"},{"location":"Microbiome/dada2_pipeline/#install-raxml","title":"Install RAxML \ud83c\udf33","text":"<p>RAxML (Randomised Axelerated Maximum Likelihood) is a widely used software for phylogenetic tree inference, and can be easily installed, assuming you have root user access.</p> How do I install RAxML? Linux \ud83d\udc27MacOS (Homebrew) \ud83c\udf7a Install multi-threaded RAxML<pre><code># Install a C compiler and make if they are not already available\nsudo apt update\nsudo apt install build-essential\n\n# Download RAxML\ncd ~/Downloads\ngit clone https://github.com/stamatak/standard-RAxML.git\ncd standard-RAxML\n\n# Compile the multi-threaded version\nmake -f Makefile.PTHREADS.gcc\n\n# Move the executable to a system directory\nsudo mv raxmlHPC-PTHREADS /usr/bin/\nsudo chmod +x /usr/bin/raxmlHPC-PTHREADS\n\n# Check that the executable is correctly installed and accessible\nraxmlHPC-PTHREADS -v # should show the version\n</code></pre> <ul> <li>Ensure you have the Homebrew package manager installed.</li> </ul> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <ul> <li>Then you can install RAxML as follows:</li> </ul> Install multi-threaded RAxML<pre><code># Install a C compiler and make if they are not already available\nbrew install gcc make\n\n# Download RAxML\ncd ~/Downloads\ngit clone https://github.com/stamatak/standard-RAxML.git\ncd standard-RAxML\n\n# Compile the multi-threaded version\nmake -f Makefile.PTHREADS.mac_gcc\n\n# Move the executable to a system directory\nsudo mv raxmlHPC-PTHREADS /usr/local/bin\nsudo chmod +x /usr/local/bin/raxmlHPC-PTHREADS\n\n# If /usr/local/bin is not in your PATH variable, add it now\necho 'export PATH=\"$PATH:/usr/local/bin\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n\n# Verify installation\nraxmlHPC-PTHREADS -v # should show the version\n</code></pre>"},{"location":"Microbiome/dada2_pipeline/#usage","title":"Usage \ud83c\udfc3","text":"<p>Open the <code>Rmarkdown</code> file in RStudio and follow the instructions in the text and comments. Hopefully this will all be quite self-explanatory.</p> <p>At the end of the pipeline, your outputs will be in a handy location for further downstream processing.</p> Output Description <code>input/01_dada2/seqtab_nochim.rds</code> The clean counts table matrix <code>input/01_dada2/taxonomy_species.rds</code> The SILVA-assigned taxonomy table <code>input/01_dada2/tree.rds</code> The de novo phylogenetic tree What if my 16S data is already demultiplexed? <p>This is no problem! Organise everything according to the directory layout above. Then, inside each run, create a folder called <code>demultiplexed</code> and store your files there. Everything else should work as planned, and you can skip the demultiplexing steps at the beginning of the pipeline.</p>"},{"location":"Microbiome/dada2_pipeline/#workflow-overview","title":"Workflow overview \ud83d\udcd6","text":"<p>In this section, we will walk through the steps that are performed throughout the pipeline for reference. Most of this information is present in the <code>Rmarkdown</code> file as well.</p>"},{"location":"Microbiome/dada2_pipeline/#demultiplexing","title":"Demultiplexing \ud83d\uddd2\ufe0f\ud83e\ude93","text":"<p>Demultiplexing is performed using the <code>iu-demultiplex</code> command from the illumina-utils FASTQ files processing toolbox. If multiple runs have to be processed, this will be done in parallel. </p> <p>The <code>-j</code> argument in the <code>parallel</code> command specifies the number of computing cores to use. You may edit it to your need (considering both available CPUs and memory).</p> <p>File naming</p> <p>Please ensure that the files names are consistent in each run. The 4 mandatory files are <code>R1.fastq.gz</code>, <code>R2.fastq.gz</code>, <code>Index.fastq.gz</code>, and <code>barcode_to_sample_[runNN].txt</code></p> Demultiplexing code <ul> <li>Firstly we want to check that we don't have any duplicated sample names in our <code>barcode_to_sample_[runNN].txt</code> mapping files.</li> </ul> Check repeating samples<pre><code># Retrieve the barcode to sample mapping file paths\nrun_data_fp &lt;- here::here('input', '01_dada2', 'run_data')\nbarcode_files &lt;- here::here(run_data_fp,\n                            list.files(run_data_fp, pattern = 'barcode', recursive = TRUE))\n\n# Read in each of the mapping files to a data.frame\nsample_names_dir &lt;- ldply(barcode_files, function(f) {\n    dat &lt;- read_tsv(f, col_names = c('sample_id', 'barcode'), col_types = 'cc')\n    return(dat)\n})\n\n# Check for duplicated sample names\nduplicate_sample_names_dir &lt;- sample_names_dir[duplicated(sample_names_dir[,1]) | duplicated(sample_names_dir[,1], fromLast = TRUE),]\nduplicate_sample_names_dir\n</code></pre> <ul> <li>If there are any names returned above, change them in your mapping file, and re-run the code above.</li> <li>Then we can move on to the demultiplexing step itself, which is handled using <code>bash</code>, which assumes you are using a Unix command line interface.</li> </ul> Demultiplex the runs<pre><code># Activate conda environment if available\nif command -v conda &gt;/dev/null 2&gt;&amp;1; then\nconda activate dada2\nfi\n\nls -d input/01_dada2/run_data/* | parallel -j -2 '\nrun_dir=\"{}\"\noutputdir=\"demultiplexed\"\n\n# Create the output directory if it does not exist\nif [[ ! -d \"${run_dir}/${outputdir}\" ]]; then\n    mkdir \"${run_dir}/${outputdir}\"\nfi\n\n# Enable nullglob so that the for loop does not run if no .fastq.gz files are found\nshopt -s nullglob\nfor f in \"${run_dir}\"/*.fastq.gz; do\n    # Only gunzip if the uncompressed file does not exist\n    if [ ! -f \"${f%.gz}\" ]; then\n    gunzip \"$f\"\n    fi\ndone\n\n# Run demultiplexing; note that the glob is unquoted so it can expand properly\niu-demultiplex -s ${run_dir}/barcode_to_sample* \\\n                --r1 \"${run_dir}/R1.fastq\" \\\n                --r2 \"${run_dir}/R2.fastq\" \\\n                -i \"${run_dir}/Index.fastq\" \\\n                -x \\\n                -o \"${run_dir}/${outputdir}\"\n'\n</code></pre>"},{"location":"Microbiome/dada2_pipeline/#quality-check","title":"Quality check \ud83d\udcc9\ud83d\udd0d","text":"<p>The DADA2 <code>plotQualityProfile</code> function plots a visual summary of the distribution of quality scores as a function of sequence position for the input fastq file.</p> <p>This can take minutes to hours depending on the number of samples and the resources available.</p> Quality check code Run `plotQualityProfile` for each run<pre><code># Identify the run directories\nruns.dirs &lt;- list.dirs(here::here('input', '01_dada2', 'run_data'), recursive = FALSE)\nruns &lt;- basename(runs.dirs)\n\n# Generate the quality profile plots\nplots &lt;- foreach(i = 1:length(runs), .packages = c('dada2', 'ggplot2')) %dopar% {\np &lt;- list()\np[[1]] &lt;- plotQualityProfile(paste(paste0(runs.dirs[i], '/demultiplexed'), \n                                    list.files(paste0(runs.dirs[i], '/demultiplexed/'), pattern = 'R1.fastq'), \n                                    sep = '/'), n = 1e+06, aggregate = TRUE) +\n    ggtitle(paste('Forward reads |', runs[i]))\np[[2]] &lt;- plotQualityProfile(paste(paste0(runs.dirs[i], '/demultiplexed'), \n                                    list.files(paste0(runs.dirs[i], '/demultiplexed/'), pattern = 'R2.fastq'), \n                                    sep = '/'), n = 1e+06, aggregate = TRUE) +\n    ggtitle(paste('Reverse reads |', runs[i]))\np\n}\n\nplots\n\n# Store the quality profile in the run directory\nfor (i in 1:length(runs)) {\nsaveRDS(plots[[i]], file.path(runs.dirs[i], 'quality_score.pdf.rds'))\npdf(file.path(runs.dirs[i], 'quality_score.pdf'))\ninvisible(lapply(plots[[i]], print))\ninvisible(dev.off())\n}\n</code></pre> Combine plots for all runs<pre><code># Recover variables for next chunk\nruns.dirs &lt;- list.dirs(here::here('input', '01_dada2', 'run_data'), recursive = FALSE)\nruns &lt;- basename(runs.dirs)\nplots &lt;- foreach(i=1:length(runs)) %dopar% {\nreadRDS(file.path(runs.dirs[i], 'quality_score.pdf.rds'))\n}\n\nnplot.pp &lt;- 4  # number of plots per page\nncol.pp &lt;- 2  # number of columns in a page\nfig &lt;- foreach(i=seq(1, length(unlist(plots, recursive = F)), by=nplot.pp), .packages = c('ggpubr')) %dopar% {\nggarrange(plotlist=unlist(plots, recursive = F)[i:(i+nplot.pp-1)], ncol=ncol.pp, nrow=nplot.pp/ncol.pp)\n}\ninvisible(lapply(fig, print))\n</code></pre> <ul> <li>In gray-scale is a heat map of the frequency of each quality score at each base position. The median quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The reverse reads are usually of worse quality, especially at the end, which is common in Illumina sequencing.</li> </ul> Save outputs to PDF<pre><code># Store the quality profile summary\nif (!dir.exists(here('input', '01_dada2', 'figures'))) {\ndir.create(here('input', '01_dada2', 'figures'))\n}\npdf(here::here('input', '01_dada2', 'figures', 'quality_score.pdf'), paper='a4')\ninvisible(lapply(fig, print))\ninvisible(dev.off())\n</code></pre>"},{"location":"Microbiome/dada2_pipeline/#quality-filtering-and-trimming","title":"Quality filtering and trimming \ud83e\uddec\u2702\ufe0f","text":"<p>The DADA2 <code>filterAndTrim</code> function trims sequences to a specified length, removes sequences shorter than that length, and filters based on the number of ambiguous bases, a minimum quality score, and the expected errors in a read. Based on the quality profiles, adjust the trimming (for each run). Your reads must still overlap after truncation in order to merge them later (the basic rule is <code>truncLen</code> must be large enough to maintain <code>20</code> + <code>biological.length.variation</code> nucleotides of overlap between them).</p> Quality filtering and trimming code Set up the required parameters<pre><code># Recover variables for next chunk\nruns.dirs &lt;- list.dirs(here::here('input', '01_dada2', 'run_data'), recursive = F)\nruns &lt;- basename(runs.dirs)\n\n# Set up parameters for filtering and trimming (first parameter stands for R1, second for R2)\ntruncLen &lt;- c(240, 240)\nmaxEE &lt;- c(4,4)\ntrimLeft &lt;- c(54, 54)\ntruncQ &lt;- c(2,2)\nmaxN &lt;- c(0,0)\nrm.phix &lt;- TRUE\n</code></pre> <p>Tunable parameters</p> Parameter Description <code>truncLen</code> Truncates reads to the specified length (R1, R2). Ensures uniform length but may discard short reads. <code>maxEE</code> Filters out reads with more than the specified number of expected errors. Lower values improve accuracy but reduce read count. <code>trimLeft</code> Removes the specified number of bases from the start of each read. Useful for removing primers and non-informative regions. <code>truncQ</code> Truncates reads at the first base with a quality score \u2264 <code>truncQ</code>. Helps remove poor-quality tails. <code>maxN</code> Discards reads containing more than the specified number of ambiguous bases (<code>N</code>). Ensures high-quality sequences. <code>rm.phix</code> Removes reads matching the PhiX genome, commonly used as a sequencing control, to reduce contamination. Perform filtering and trimming<pre><code># For each run, store the filtered sequences in a new directory named 'filtered'\nfilterAndTrim.out &lt;- vector('list', length(runs))\nfor(i in 1:length(runs)) {\nfwd.fn &lt;- sort(list.files(file.path(runs.dirs[i], 'demultiplexed'), pattern = 'R1.fastq'))\nrev.fn &lt;- sort(list.files(file.path(runs.dirs[i], 'demultiplexed'), pattern = 'R2.fastq'))\nfilterAndTrim.out[[i]] &lt;- filterAndTrim(\n                fwd=file.path(runs.dirs[i], 'demultiplexed', fwd.fn),\n                filt=file.path(runs.dirs[i], 'filtered', fwd.fn),\n                rev=file.path(runs.dirs[i], 'demultiplexed', rev.fn),\n                filt.rev=file.path(runs.dirs[i], 'filtered', rev.fn),\n                truncLen=truncLen,\n                trimLeft = trimLeft,\n                maxEE=maxEE,\n                truncQ=truncQ,\n                maxN=maxN,\n                rm.phix=rm.phix,\n                compress=TRUE,\n                verbose=TRUE,\n                multithread=nc)\n}\n\n# Store the filtering report in the run directory\nfilt.plots &lt;- foreach(i=1:length(runs), .packages = c('ggplot2', 'reshape2')) %do% {\nsaveRDS(filterAndTrim.out[[i]], file.path(runs.dirs[i], 'filtering_report.rds'))\ndata &lt;- as.data.frame(filterAndTrim.out[[i]])\nrow.names(data) &lt;- gsub('_R1_001.fastq|-R1.fastq', '', row.names(data))\ndata$reads.in &lt;- data$reads.in - data$reads.out\np &lt;- ggplot(melt(as.matrix(data)), aes(x=Var1, y=value, fill=Var2)) +\n    geom_bar(stat='identity') +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +\n    labs(title = runs[i], x = 'Samples', y = 'Reads', fill = NULL) +\n    theme(axis.title.x = element_blank())\nsaveRDS(p, file.path(runs.dirs[i], 'filtering_report.pdf.rds'))\npdf(file.path(runs.dirs[i], 'filtering_report.pdf'))\nprint(p)\ninvisible(dev.off())\np\n}\npdf(here::here('input', '01_dada2', 'figures', 'filtering_report.pdf'), width = 12, height = 8)\ninvisible(lapply(filt.plots, print))\ninvisible(dev.off())\n</code></pre> Save the filtering report to PDF<pre><code># Recover variables for next chunck\nruns.dirs &lt;- list.dirs(here::here('input', '01_dada2', 'run_data'), recursive = F)\nruns &lt;- basename(runs.dirs)\n\nfilt.plots &lt;- foreach(i=1:length(runs.dirs)) %dopar% {\nreadRDS(file.path(runs.dirs[i], 'filtering_report.pdf.rds'))\n}\n\ninvisible(lapply(filt.plots, print))\n</code></pre> <p>If too few reads are passing the filter, consider whether you can relax the <code>maxEE</code> parameter, or perhaps reduce the <code>truncLen</code> to remove the lower quality tail ends of reads.</p>"},{"location":"Microbiome/dada2_pipeline/#sequencing-error-model-generation","title":"Sequencing error model generation \ud83d\udd2c\ud83d\udcc9","text":"<p>The DADA2 algorithm makes use of a parametric error model, and every amplicon dataset has a different set of error rates. The <code>learnErrors</code> method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution.</p> Sequencing error model code Generate the sequencing error models<pre><code># Recover variables\nruns.dirs &lt;- list.dirs(here::here('input', '01_dada2', 'run_data'), recursive = F)\nruns &lt;- basename(runs.dirs)\n\n# Identify all of the forward and reverse sequencing files\nerr.model &lt;- foreach(i = 1:length(runs), .packages = c('dada2', 'ggplot2')) %dopar% {\nfwd.fn &lt;- sort(list.files(file.path(runs.dirs[i], 'filtered'), pattern = 'R1.fastq'))\nrev.fn &lt;- sort(list.files(file.path(runs.dirs[i], 'filtered'), pattern = 'R2.fastq'))\nerr &lt;- list()\nerr[[1]] &lt;- learnErrors(file.path(runs.dirs[i], 'filtered', fwd.fn), nbases=1e8, multithread=nc)\nerr[[2]] &lt;- learnErrors(file.path(runs.dirs[i], 'filtered', rev.fn), nbases=1e8, multithread=nc)\nerr\n}\n# Plot the error model\nerr.plots &lt;- foreach(i = 1:length(runs), .packages = c('dada2', 'ggplot2')) %do% {\np &lt;- list()\np[[1]] &lt;- plotErrors(err.model[[i]][[1]], nominalQ=TRUE) +\n                ggtitle(paste(runs[i], '| forward reads'))\np[[2]] &lt;- plotErrors(err.model[[i]][[2]], nominalQ=TRUE) +\n                ggtitle(paste(runs[i], '| reverse reads'))\np\n}\n\n# Store the error model in the run directory\nfor (i in 1:length(runs)) {\nsaveRDS(err.model[[i]], file.path(runs.dirs[i], 'error_model.rds'))\nsaveRDS(err.plots[[i]], file.path(runs.dirs[i], 'error_model.pdf.rds'))\npdf(file.path(runs.dirs[i], 'error_model.pdf'))\ninvisible(lapply(err.plots[[i]], print))\ninvisible(dev.off())\n}\n</code></pre> Combine the error models into a condensed summary<pre><code># Recover variables\nruns.dirs &lt;- list.dirs(here::here('input', '01_dada2', 'run_data'), recursive = F)\nruns &lt;- basename(runs.dirs)\nerr.plots &lt;- foreach(i=1:length(runs)) %dopar% {\nreadRDS(file.path(runs.dirs[i], 'error_model.pdf.rds'))\n}\n\nnplot.pp &lt;- 2  # number of plots per page\nncol.pp &lt;- 2  # number of columns in a page\nfig &lt;- foreach(i=seq(1, length(unlist(err.plots, recursive = F)), by=nplot.pp), .packages = c('ggpubr')) %dopar% {\nggarrange(plotlist=unlist(err.plots, recursive = F)[i:(i+nplot.pp-1)], ncol=ncol.pp, nrow=nplot.pp/ncol.pp)\n}\ninvisible(lapply(fig, print))\n</code></pre> <p>Transitions (A\u2192C, A\u2192G, \u2026) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.</p> Save the error model summary to PDF<pre><code># Store the error model summary\npdf(here::here('input', '01_dada2', 'figures', 'error_model.pdf'), width = 12, height = 6)\ninvisible(lapply(fig, print))\ninvisible(dev.off())\n</code></pre>"},{"location":"Microbiome/dada2_pipeline/#count-table-generation","title":"Count table generation \ud83e\uddee","text":"<p>A table with amplicon sequence variants is constructed. To avoid overloading memory, runs and samples are processed sequentialy. The process starts with sequences dereplication, then it goes through Amplicon Sequence Variants (ASVs) inference and ends with Paired-Ends (PE) merging.</p> <ul> <li>Dereplication combines all identical sequencing reads into into \u201cunique sequences\u201d with a corresponding \u201cabundance\u201d equal to the number of reads with that unique sequence. Dereplication in the DADA2 pipeline has one crucial addition from other pipelines: DADA2 retains a summary of the quality information associated with each unique sequence. The consensus quality profile of a unique sequence is the average of the positional qualities from the dereplicated reads. The consensus scores are then used by the error model of the dada function.</li> <li>Amplicon sequence variants (ASV) inference is the core stage of the DADA2 package (the <code>dada</code> function). It will assign all reads to an error-corrected sequence using the models of the error rates of the previous step.</li> <li>Paired-Ends (PE) merging performs a global ends-free alignment between paired forward and reverse reads and merges them together if they exactly overlap. It requires that the input forward and reverse reads are in the same order. Note that merging in the DADA2 pipeline happens after denoising, hence the strict requirement of exact overlap since it is expected that nearly all substitution errors have already been removed.</li> </ul> Count table generation code Generate the counts table<pre><code># Recover variables\nruns.dirs &lt;- list.dirs(here::here('input', '01_dada2', 'run_data'), recursive = F)\nruns &lt;- basename(runs.dirs)\nerr.model &lt;- foreach(i=1:length(runs)) %dopar% {\nreadRDS(file.path(runs.dirs[i], 'error_model.rds'))\n}\n\nfor(i in 1:length(runs)) {\n\nfwd.fn &lt;- sort(list.files(file.path(runs.dirs[i], 'filtered'), pattern = 'R1.fastq'))\nrev.fn &lt;- sort(list.files(file.path(runs.dirs[i], 'filtered'), pattern = 'R2.fastq'))\nsample.names &lt;- sapply(strsplit(basename(fwd.fn), 'R1.fastq'), `[`, 1)\nsample.names.rev &lt;- sapply(strsplit(basename(rev.fn), 'R2.fastq'), `[`, 1)\nif (!identical(sample.names, sample.names.rev)) stop('Forward and reverse files do not match.')\nnames(fwd.fn) &lt;- sample.names\nnames(rev.fn) &lt;- sample.names\n\nmerged &lt;- vector('list', length(sample.names))\nnames(merged) &lt;- sample.names\nfor(j in 1:length(sample.names)) {\n    derep &lt;- vector('list', 2)\n    derep[[1]] &lt;- derepFastq(file.path(runs.dirs[i], 'filtered', fwd.fn[j]))\n    derep[[2]] &lt;- derepFastq(file.path(runs.dirs[i], 'filtered', rev.fn[j]))\n    asv &lt;- vector('list', 2)\n    asv[[1]] &lt;- dada(derep[[1]], err=err.model[[i]][[1]], pool = TRUE, multithread=nc)\n    asv[[2]] &lt;- dada(derep[[2]], err=err.model[[i]][[2]], pool = TRUE, multithread=nc)\n    merged[[sample.names[j]]] &lt;- mergePairs(asv[[1]], derep[[1]], asv[[2]], derep[[2]])\n}\n\nst &lt;- makeSequenceTable(merged)\nsaveRDS(st, file.path(runs.dirs[i], 'seqtab.rds'))\n}\n</code></pre> <p>Info</p> <p>Most of your reads should successfully merge. If that is not the case, upstream parameters may need to be revisited.</p>"},{"location":"Microbiome/dada2_pipeline/#merge-runs","title":"Merge runs \ud83e\udd1d\ud83e\uddea","text":"<p>At this point, all of the individual per-run <code>seqtab</code> tables are merged into a single global <code>seqtab</code>.</p> Merge runs code Merge runs<pre><code># Recover variables\nruns.dirs &lt;- list.dirs(here::here('input', '01_dada2', 'run_data'), recursive = F)\n\nseqtab.fps &lt;- file.path(runs.dirs, 'seqtab.rds')\nif (length(seqtab.fps) == 1) {\nseqtab &lt;- readRDS(seqtab.fps[[1]])\n} else {\nseqtab &lt;- mergeSequenceTables(tables = seqtab.fps)\n}\n\n# Save data into a new directory named 'data'\nsaveRDS(seqtab, here::here('input', '01_dada2', 'seqtab.rds'))\n</code></pre>"},{"location":"Microbiome/dada2_pipeline/#chimera-screening","title":"Chimera screening \ud83e\udd84","text":"<p>The <code>dada</code> algorithm models and removes substitution errors, but chimeras are another importance source of spurious sequences in amplicon sequencing. </p> <p>What are chimeras?</p> <p>Chimeras are artifacts generated during PCR amplification when an incomplete amplicon acts as a primer for a subsequent extension step. This occurs when a partially synthesized DNA fragment anneals to a different template in later cycles, leading to the formation of a hybrid sequence. As a result, the final amplicon consists of segments from two distinct parental sequences, producing a misleading chimeric read that does not represent a true biological variant.</p> Chimera screening code Screen and remove chimeric reads<pre><code># Recover variables\nseqtab &lt;- readRDS(here::here('input', '01_dada2', 'seqtab.rds'))\n\n# Remove chimeric reads\nseqtab_nochim &lt;- removeBimeraDenovo(seqtab, method='per-sample', multithread=nc, verbose = TRUE)\nsaveRDS(seqtab_nochim, here::here('input', '01_dada2', 'seqtab_nochim.rds'))\nfwrite(as.data.frame(seqtab_nochim), here::here('input', '01_dada2', 'seqtab_nochim.txt'), quote = F, sep = '\\t')\n\n# Inspect distribution of sequence lengths after chimera removal\ndistrib &lt;- table(nchar(getSequences(seqtab_nochim)))\ndistrib_plot &lt;- function(){\nplot(distrib, xlab = 'Read length', ylab = 'Number of ASVs')\n}\nsaveRDS(distrib, here::here('input', '01_dada2', 'length_distribution.rds'))\npdf(here::here('input', '01_dada2', 'figures', 'length_distribution.pdf'))\ndistrib_plot()\ninvisible(dev.off())\n</code></pre> Check the post-chimera removal dimensions and inspect length distributions<pre><code># Recover variables\nseqtab &lt;- readRDS(here::here('input', '01_dada2', 'seqtab.rds'))\nseqtab_nochim &lt;- readRDS(here::here('input', '01_dada2', 'seqtab_nochim.rds'))\ndistrib &lt;- readRDS(here::here('input', '01_dada2', 'length_distribution.rds'))\ndistrib.plot &lt;- function(){\nplot(distrib, xlab = 'Read length', ylab = 'Number of ASVs')\n}\n\n# Check the dimensions of the table before chimera removal\ndim(seqtab)\n\n# Check the dimensions of the table after chimera removal\ndim(seqtab_nochim)\n\n# Check the distribution of sequence lengths\ndistrib.plot()\n</code></pre>"},{"location":"Microbiome/dada2_pipeline/#reads-tracking","title":"Reads tracking \ud83d\udcca\ud83d\udee4\ufe0f","text":"<p>As a final check of our progress, we look at the number of reads that made it through each step in the pipeline. </p> <p>Double check the retention of reads</p> <p>Outside of filtering (first step) there should be no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification.</p> Reads tracking code Generate reads tracking plot and save to PDF<pre><code># Recover variables\nseqtab &lt;- readRDS(here::here('input', '01_dada2', 'seqtab.rds'))\nseqtab_nochim &lt;- readRDS(here::here('input', '01_dada2', 'seqtab_nochim.rds'))\n\ntrack.plots &lt;- foreach(i=1:length(runs), .packages = c('ggplot2', 'reshape2')) %do% {\nfiltering &lt;- readRDS(file.path(runs.dirs[i], 'filtering_report.rds'))\nrow.names(filtering) &lt;- gsub('_R1_001.fastq|-R1.fastq', '', row.names(filtering))\ntrack &lt;- cbind(filtering[row.names(filtering) %in% row.names(seqtab),],\n                rowSums(seqtab[row.names(seqtab) %in% row.names(filtering), ]),\n                rowSums(seqtab_nochim[row.names(seqtab_nochim) %in% row.names(filtering), ]))\ncolnames(track) &lt;- c('Input', 'Filtered', 'Merged', 'Non chimeric')\nfor (j in (ncol(track)-1):1) {\n    for (k in (j+1):ncol(track)) {\n    track[, j] &lt;- track[, j] - track[, k]\n    }\n}\np &lt;- ggplot(melt(as.matrix(track)), aes(x=Var1, y=value, fill=Var2)) +\n    geom_bar(stat='identity') +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 4)) +\n    labs(title = runs[i], x = 'Samples', y = 'Reads', fill = NULL)\nsaveRDS(p, file.path(runs.dirs[i], 'read_tracking_report.pdf.rds'))\npdf(file.path(runs.dirs[i], 'read_tracking_report.pdf'))\nprint(p)\ninvisible(dev.off())\np\n}\npdf(here::here('input', '01_dada2', 'figures', 'read_tracking_report.pdf'), width = 8, height = 6)\ninvisible(lapply(track.plots, print))\ninvisible(dev.off())\n</code></pre> View the reads tracking plot<pre><code># Recover variables\nruns.dirs &lt;- list.dirs(here::here('input', '01_dada2', 'run_data'), recursive = F)\ntrack.plots &lt;- foreach(i=1:length(runs.dirs)) %dopar% {\nreadRDS(file.path(runs.dirs[i], 'read_tracking_report.pdf.rds'))\n}\n\ninvisible(lapply(track.plots, print))\n</code></pre>"},{"location":"Microbiome/dada2_pipeline/#taxonomy-assignment","title":"Taxonomy Assignment \ud83e\uddec\ud83c\udff7\ufe0f","text":"<p>The DADA2 package provides a native implementation of the naive Bayesian classifier method for this purpose.  The <code>assignTaxonomy</code> function takes as input a set of sequences to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least minBoot bootstrap confidence.</p> <p>Update your SILVA database version</p> <p>As of SILVA version 138.2, the DADA2-formatted version can assign to the species level in a single step. Please update your database file if you are using an older version.</p> Taxonomy assignment code Assign taxonomy to the species level<pre><code># Recover variables\nseqtab_nochim &lt;- readRDS(here::here('input', '01_dada2', 'seqtab_nochim.rds'))\n\n# Path to the DADA2-formatted reference database\n# Replace the existing path with the path to your SILVA training set\ndb_fp &lt;- here::here('..', '..', '20_Databases', 'Silva', 'silva_nr99_v138.2_toSpecies_trainset.fa.gz')\n\ntaxonomy &lt;- assignTaxonomy(seqtab_nochim, db_fp, minBoot = 50, tryRC = TRUE, multithread=nc)\nsaveRDS(taxonomy, here::here('input', '01_dada2', 'taxonomy_species.rds'))\nfwrite(as.data.frame(taxonomy), here::here('input', '01_dada2', 'taxonomy_species.txt'), quote = F, sep = '\\t')\n</code></pre> <p>Trying the reverse complement</p> <p>The <code>tryRC = TRUE</code> option ensures that both the original sequences and their reverse complements are considered when matching against the reference database. This is useful when the sequencing orientation is unknown or inconsistent, helping to improve taxonomic assignment accuracy by allowing for potential reverse-complement matches.</p>"},{"location":"Microbiome/dada2_pipeline/#export-in-qiime-classic-otu-table-like-format","title":"Export in Qiime classic OTU table-like format \ud83d\udcbe\ud83d\udd20","text":"<p>The DADA2 pipeline provides results as a count table of ASVs per samples and a taxonomic classification of each ASV in two separate files. As detailed in the DADA2 tutorial, these two objects can easily be used with the phyloseq R package for subsequent data analysis.</p> <ul> <li>For compatibility with other data analysis tools, a count table in a tab-delimited text format matching the Qiime classic OTU table format is also created.</li> <li>The table contains samples in columns and ASVs in rows. The taxonomy at the species level is added as an extra 'taxonomy' column as well as a 'sequence' column. The first columns contains mock OTU IDs.</li> </ul> Qiime-style OTU table export code Export OTU table in classic Qiime format<pre><code># Recover variables\nseqtab_nochim &lt;- readRDS(here::here('input', '01_dada2', 'seqtab_nochim.rds'))\ntaxonomy_species &lt;- readRDS(here::here('input', '01_dada2', 'taxonomy_species.rds'))\n\n# Define a function to create the classic OTU table\ndada2otu &lt;- function(seqtab=NULL, taxonomy=NULL) {\nout &lt;- as.data.frame(cbind(c(1:nrow(taxonomy)), t(as.data.frame(seqtab)), \n                            apply(as.data.frame(taxonomy), 1, paste, collapse = '; '), colnames(seqtab)))\nrow.names(out) &lt;- c(1:nrow(out))\nnames(out) &lt;- c('#OTU ID', row.names(as.data.frame(seqtab)), 'taxonomy', 'sequence')\nreturn(out)\n}\n\n# Export OTU table\nfwrite(dada2otu(seqtab_nochim, taxonomy.species), here::here('input', '01_dada2', 'otu_table.txt'), quote = F, sep = '\\t', buffMB = 100)\n\n# Export count table\ntotal_counts &lt;- as.data.frame(cbind(row.names(as.data.frame(seqtab_nochim)), rowSums(seqtab_nochim)))\nnames(total_counts) &lt;- c('SampleID', 'Total count')\nfwrite(total_counts, here::here('input', '01_dada2', 'total_counts.txt'), quote = F, sep = '\\t')\n</code></pre>"},{"location":"Microbiome/dada2_pipeline/#create-a-phylogenetic-tree","title":"Create a phylogenetic tree \ud83c\udf33\ud83e\uddec","text":"<p>The DADA2 sequence inference method is reference-free, so we must construct the phylogenetic tree relating the inferred sequence variants de novo. We begin by performing a multiple-alignment using the <code>DECIPHER</code> R package.  The <code>phangorn</code> R package is then used to construct a phylogenetic tree. Here we first construct a neighbor-joining tree, and then fit a GTR+G+I (Generalized time-reversible with Gamma rate variation) maximum likelihood tree using the neighbor-joining tree as a starting point.  Adapted from Callahan et al. pipeline. </p> De novo phylogenetic tree code Create phylogenetic tree using RAxML<pre><code># Recover variables for next chunk\nseqtab_nochim &lt;- readRDS(here::here('input', '01_dada2', 'seqtab_nochim.rds'))\ntaxonomy_species &lt;- readRDS(here::here('input', '01_dada2', 'taxonomy_species.rds'))\n\n# Extract ASV sequences\nseqs &lt;- colnames(seqtab_nochim)\nnames(seqs) &lt;- seqs\n\n# Align sequences\nalignment &lt;- AlignSeqs(DNAStringSet(seqs), anchor = NA)\nphang_align &lt;- phyDat(as(alignment, 'matrix'), type ='DNA')\n\n# Convert phyDat class to DNAbin format\nalignment_dnabin &lt;- as.DNAbin(phang_align)\n\n# Prepare suitable names for RAxML\nseq_names_tidy &lt;- data.frame(original = rownames(alignment_dnabin)) %&gt;%\nmutate(raxml_names = 'seq') %&gt;%\nmutate(raxml_names = gsub('\\\\.', '_', make.unique(raxml_names)))\nrownames(alignment_dnabin) &lt;- seq_names_tidy$raxml_names\n\n# Run RAxML using the raxml function\n# Set threads to the number of **physical** cores you have\nfitGTR &lt;- raxml(DNAbin = alignment_dnabin,\n                m = 'GTRGAMMAI',\n                f = 'a',\n                N = 100,\n                p = 12345,\n                x = 12345,\n                threads = 24, \n                exec = '/usr/bin/raxmlHPC-PTHREADS',\n                file = 'dada2')\n\n# Move the best tree to your dada2 input folder from the project directory\n# Remove the other unnecessary files\n# Read in the resulting tree from the RAxML output file\nfile.copy(from = here('RAxML_bestTree.dada2'),\n        to = here('input', '01_dada2', 'RAxML_bestTree.dada2'))\nfile.remove(list.files(pattern = 'RAxML|dada2.phy'))\ntree &lt;- read.tree(here('input', '01_dada2', 'RAxML_bestTree.dada2'))\n\n# Correct the tip labels\ntip_labels &lt;- data.frame(raxml_names = tree$tip.label) %&gt;%\nleft_join(seq_names_tidy, by = 'raxml_names')\ntree$tip.label &lt;- tip_labels$original\n\n# Force dichotomy (resolve any multifurcations)\ntree_dich &lt;- ape::multi2di(tree)\n\n# Re-root the tree explicitly using an outgroup (i.e. the first tip)\ntree_rooted &lt;- root(tree_dich,\n                    outgroup = tree_dich$tip.label[1],\n                    resolve.root = TRUE)\n\n# Save to disk\nsaveRDS(tree_rooted, here('input', '01_dada2', 'tree.rds'))\n</code></pre>"},{"location":"Microbiome/dada2_pipeline/#rights","title":"Rights","text":"<ul> <li>Copyright \u00a9\ufe0f 2025 Mucosal Immunology Lab, Monash University, Melbourne, Australia and Service de Pneumologie, Centre Hospitalier Universitaire Vaudois (CHUV), Switzerland.</li> <li>Licence: This pipeline is provided under the MIT license.</li> <li>Pipeline authors: A. Rapin, C. Pattaroni, A. Butler, B.J. Marsland, M. Macowan</li> <li>This document: M. Macowan</li> </ul> <ul> <li>Illumina Utils: Meren Lab</li> <li>DADA2: Benjamin Callahan et al.</li> <li>SILVA: arb-silva.de</li> <li>RAxML: Exelixis Lab</li> <li>phangorn: Klaus Schliep et al.</li> <li>ips: Christoph Heibl et al.</li> </ul>"},{"location":"NextFlow/dada2_16S/","title":"DADA2 16S rRNA amplicon sequencing pre-processing","text":""},{"location":"NextFlow/dada2_16S/#introduction","title":"Introduction","text":"<p>nf-mucimmuno/dada2_16S is a bioinformatics pipeline that can be used to run the popular DADA2 pre-processing pipeline for bacterial 16S rRNA amplicon sequencing data. It can handle multiple runs to generate a single unified output. It takes a samplesheet and either pre- or post-demultiplexed data (depending on what you have available), performs quality profiling, filtering and trimming, sequencing error modelling, sample inference, and merging of paired ends. From there, it combines all runs together, removes chimeras, assigns SILVA taxonomy, and generates a de novo phylogenetic tree using RAxML.</p> <p></p> <p>Tip</p> <p>This is essentially the Nextflow version of our DADA2 pipeline guide, which you can explore here.</p>"},{"location":"NextFlow/dada2_16S/#usage","title":"Usage","text":""},{"location":"NextFlow/dada2_16S/#download-the-repository","title":"Download the repository \ud83d\udcc1","text":"<p>This repository contains the relevant Nextflow workflow components, including a conda environment and submodules, to run the pipeline. To retrieve this repository alone, run the <code>retrieve_me.sh</code> script above.</p> <p>Git version requirements</p> <p>Git <code>sparse-checkout</code> is required to retrieve just the nf-mucimmuno/scRNAseq pipeline. It was only introduced to Git in version 2.27.0, so ensure that the loaded version is high enough (or that there is a version loaded on the cluster at all). As of July 2024, the M3 MASSIVE cluster has version 2.38.1 available.</p> <pre><code># Check git version\ngit --version\n\n# Load git module if not loaded or insufficient version\nmodule load git/2.38.1\n</code></pre> <p>First, create a new bash script file.</p> <pre><code># Create and edit a new file with nano\nnano retrieve_me.sh\n</code></pre> <p>Add the contents to the file, save, and close.</p> <pre><code>#!/bin/bash\n\n# Define variables\nREPO_URL=\"https://github.com/mucosal-immunology-lab/nf-mucimmuno\"\nREPO_DIR=\"nf-mucimmuno\"\nSUBFOLDER=\"dada2_16S\"\n\n# Clone the repository with sparse checkout\ngit clone --no-checkout $REPO_URL\ncd $REPO_DIR\n\n# Initialize sparse-checkout and set the desired subfolder\ngit sparse-checkout init --cone\ngit sparse-checkout set $SUBFOLDER\n\n# Checkout the files in the subfolder\ngit checkout main\n\n# Move the folder into the main folder and delete the parent\nmv $SUBFOLDER ../\ncd ..\nrm -rf $REPO_DIR\n\necho \"Subfolder '$SUBFOLDER' has been downloaded successfully.\"\n</code></pre> <p>Then run the script to retrieve the repository into a new folder called <code>dada2_16S</code>, which will house your workflow files and results.</p> <pre><code># Run the script\nbash retrieve_me.sh\n</code></pre>"},{"location":"NextFlow/dada2_16S/#create-the-conda-environment","title":"Create the conda environment \ud83d\udc0d","text":"<p>To create the conda environment, use the provided environment <code>.yaml</code> file. Then activate it to access required functions.</p> <pre><code># Create the environment\nmamba env create -f environment.yaml\n\n# Activate the environment\nmamba activate nextflow-dada2\n</code></pre>"},{"location":"NextFlow/dada2_16S/#folder-structure","title":"Folder structure \ud83d\uddc2\ufe0f","text":"<p>Because you specify the full directory path for your raw input data, you can technically house them however and wherever you like. However, below is an example of how to store runs depending on whether you have demultiplexed files or the pre-demultiplexed sequencing files.</p> <p>The pipeline just looks for a <code>demultiplexed</code> folder inside each run</p> <p>Essentially if there's an existing folder called <code>demultiplexed</code>, demultiplexing is skipped for that run, and the existing files are used.</p> <ul> <li><code>run01</code> requires demultiplexing, and as such simply includes <code>R1.fastq.gz</code>, <code>R2.fastq.gz</code>, and <code>Index.fastq.gz</code>. It also importantly contains a barcode mapping file which provides a link between the unique adapters and each sample.</li> <li><code>run02</code> however is already demultiplexed, and therefore only requires the forward (<code>R1</code>) and reverse (<code>R2</code>) reads for each sample.</li> </ul> <p>Make sure you correctly name your barcode mapping file!</p> <p>For runs requiring demultiplexing, the barcode mapping file must include the string <code>barcode_to_sample</code> in its filename.</p> <pre><code>dada2_16S/\n    \u2514\u2500\u2500 data/\n        \u251c\u2500\u2500 run01/\n        \u2502   \u251c\u2500\u2500 barcode_to_sample_run01.txt\n        \u2502   \u251c\u2500\u2500 Index.fastq.gz\n        \u2502   \u251c\u2500\u2500 R1.fastq.gz\n        \u2502   \u2514\u2500\u2500 R2.fastq.gz\n        \u2514\u2500\u2500 run02/\n            \u2514\u2500\u2500 demultiplexed/\n                \u251c\u2500\u2500 sample1-R1.fastq.gz\n                \u251c\u2500\u2500 sample1-R2.fastq.gz\n                \u251c\u2500\u2500 sample2-R1.fastq.gz\n                \u251c\u2500\u2500 sample2-R2.fastq.gz\n                \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"NextFlow/dada2_16S/#prepare-your-sample-sheet","title":"Prepare your sample sheet \u270f\ufe0f","text":"<p>This pipeline requires a sample sheet to identify where your sequencing data is located. You can also change the name of your run to something some specific if you desire from the original folder name.</p> <p>Your sample sheet should look as follows, ensuring you use the exact column names as below.</p> <p>File paths on M3</p> <p>Remember that on the M3 MASSIVE cluster, you need to use the full file path \u2013 relative file paths don't usually work.</p> <pre><code>run_name,folder_path\nrun_01,/path_to_pipeline/dada2_16S/data/run01\nrun_02,/path_to_pipeline/dada2_16S/data/run02\n</code></pre> <p>An example is provided here.</p>"},{"location":"NextFlow/dada2_16S/#running-the-pipeline","title":"Running the pipeline \ud83c\udfc3","text":"<p>Now you can run the pipeline. You will need to set up a parent job to run each of the individual jobs \u2013 this can be either an interactive session, or an sbatch job. For example:</p> <pre><code># Start an interactive session with minimal resources\nsmux n --time=3-00:00:00 --mem=16GB --ntasks=1 --cpuspertask=2 -J nf-STARsolo\n</code></pre> <p>Set the correct sample sheet location</p> <p>Make sure you alter the <code>nextflow.config</code> file to provide the path to your sample sheet, unless it is <code>./samplesheet.csv</code> which is the default for the cluster profile. Stay within the top <code>cluster</code> profile section to alter settings for Slurm-submitted jobs.</p> <p>Inside your interactive session, be sure to activate your <code>nextflow-dada2</code> environment from above. Then, inside the dada2_16S folder, begin the pipeline using the following command (ensuring you use the <code>cluster</code> profile to make use of the Slurm workflow manager).</p> <pre><code># Activate conda environment\nmamba activate nextflow-dada2\n\n# Begin running the pipeline\nnextflow run dada2_pipeline.nf -resume -profile cluster\n</code></pre>"},{"location":"NextFlow/dada2_16S/#customisation","title":"Customisation \u2699\ufe0f","text":"<p>There are some customisation options that are available within the <code>nextflow.config</code> file. While the defaults should be suitable for those with access to the M3 MASSIVE cluster genomics partition, for those without access, of for those who require different amounts of resources, there are ways to change these.</p> <p>To adjust the <code>cluster</code> profile settings, stay within the appropriate section at the top of the file.</p> Parameters Option Description sample_sheet The file path to your sample sheet (default: <code>'./samplesheet.csv'</code>) outdir A new folder name to be created for your results (default: <code>'results'</code>) filter_and_trim.truncLen Reads should be truncated after this many bases (default: <code>'240,240'</code>) filter_and_trim.maxEE After truncation, reads with higher than <code>maxEE</code> expected error will be discarded (default: <code>'4,4'</code>) filter_and_trim.trimLeft The number of bases that should be trimmed from the left-hand side of the read \u2013 helps to remove custom primers and improve data retention, but may need to be set to zeros depending on your data (default: <code>'54,54'</code>) filter_and_trim.truncQ Reads will be truncated at the first instance of a quality score less than or equal to this (default: <code>'2,2'</code>) filter_and_trim.maxN After truncation, sequences with more than <code>maxN</code> <code>N</code> bases will be discarded (default: <code>'0,0'</code>) assign_taxonomy.trainSet_link The link to the DADA2-formatted SILVA training dataset assign_taxonomy.trainSet_file The desired file name for the DADA2-formatted SILVA training dataset assign_taxonomy.assignSpecies_link The link to the DADA2-formatted species assignment dataset assign_taxonomy.assignSpecies_file The desired file name for the DADA2-formatted species assignment dataset <p>The DADA2-ready SILVA taxonomic data is sourced from here. If new updates become available, feel free to update these values (or let us know so we can update).</p> <p>Read-specific filter and trim settings</p> <p>Although the defaults use the same parameter values for both forward and reverse reads, you can set specific ones. For example, in the <code>filter_and_trim.truncLen</code> parameter of <code>'240,240'</code>, the first value is for the forward (<code>R1</code>) read, and the second is for the reverse (<code>R2</code>) read. These can be set independently of each other.</p> Process <p>These settings relate to resource allocation and cluster settings. DE_NOVO_PHYLO_TREE is the only step that will likely longer than 4 hours for typical runs, and therefore the default option is to run this steps on the <code>comp</code> partition. Depending on how many runs you have, other steps may exceed the wall-time too; adjust as required.</p> Option Description executor The workload manager (default: <code>'slurm'</code>) conda The conda environment to use (default: <code>'./environment.yaml'</code>) queueSize The maximum number of jobs to be submitted at any time (default: <code>12</code>) submitRateLimit The rate allowed for job submission \u2013 either a number of jobs per second (e.g. 20sec) or a number of jobs per time period (e.g. 20/5min) (default: <code>'1/2sec'</code>) memory The maximum global memory allowed for Nextflow to use (default: <code>'320 GB'</code>) DEMULTIPLEX.memory Memory for DEMULTIPLEX step to use (default: <code>'52 GB'</code>) DEMULTIPLEX.cpus Number of CPUs for DEMULTIPLEX step to use (default: <code>'8'</code>) DEMULTIPLEX.clusterOptions Specific cluster options for DEMULTIPLEX step to use (default: <code>'--time=4:00:00 --partition=genomics --partition=genomics'</code>) ERROR_MODEL.memory Memory for ERROR_MODEL step to use (default: <code>'52 GB'</code>) ERROR_MODEL.cpus Number of CPUs for ERROR_MODEL step to use (default: <code>'8'</code>) ERROR_MODEL.clusterOptions Specific cluster options for ERROR_MODEL step to use (default: <code>'--time=4:00:00 --partition=genomics --partition=genomics'</code>) MERGE_PAIRED_ENDS.memory Memory for MERGE_PAIRED_ENDS step to use (default: <code>'52 GB'</code>) MERGE_PAIRED_ENDS.cpus Number of CPUs for MERGE_PAIRED_ENDS step to use (default: <code>'8'</code>) MERGE_PAIRED_ENDS.clusterOptions Specific cluster options for MERGE_PAIRED_ENDS step to use (default: <code>'--time=4:00:00 --partition=genomics --partition=genomics'</code>) REMOVE_CHIMERAS.memory Memory for REMOVE_CHIMERAS step to use (default: <code>'52 GB'</code>) REMOVE_CHIMERAS.cpus Number of CPUs for REMOVE_CHIMERAS step to use (default: <code>'8'</code>) REMOVE_CHIMERAS.clusterOptions Specific cluster options for REMOVE_CHIMERAS step to use (default: <code>'--time=4:00:00 --partition=genomics --partition=genomics'</code>) ASSIGN_TAXONOMY.memory Memory for ASSIGN_TAXONOMY step to use (default: <code>'52 GB'</code>) ASSIGN_TAXONOMY.cpus Number of CPUs for ASSIGN_TAXONOMY step to use (default: <code>'8'</code>) ASSIGN_TAXONOMY.clusterOptions Specific cluster options for ASSIGN_TAXONOMY step to use (default: <code>'--time=4:00:00 --partition=genomics --partition=genomics'</code>) DE_NOVO_PHYLO_TREE.memory Memory for v step to use (default: <code>'160 GB'</code>) DE_NOVO_PHYLO_TREE.cpus Number of CPUs for DE_NOVO_PHYLO_TREE step to use (default: <code>'24'</code>) DE_NOVO_PHYLO_TREE.clusterOptions Specific cluster options for DE_NOVO_PHYLO_TREE step to use (default: <code>'--time=24:00:00'</code>)"},{"location":"NextFlow/dada2_16S/#outputs","title":"Outputs","text":"<p>Several outputs will be copied from their respective Nextflow <code>work</code> directories to the output folder of your choice (default: <code>results</code>).</p>"},{"location":"NextFlow/dada2_16S/#phyloseq-inputs","title":"Phyloseq inputs \ud83d\udce6","text":"<p>The main outputs of interest for downstream processing are the inputs to create your <code>phyloseq</code> object.</p> Output Description <code>seqtab_nochim.rds</code> The clean counts table matrix <code>taxonomy_species.rds</code> The SILVA-assigned taxonomy table <code>tree.rds</code> The de novo phylogenetic tree"},{"location":"NextFlow/dada2_16S/#qc-and-filtering-plots","title":"QC and filtering plots \ud83d\udcca","text":"<p>There are also a collection of quality control and filtering plots available in this directory too. Below is an example of the output structure for running a single run.</p> <pre><code>dada2_16S/\n    \u251c\u2500\u2500 run01/\n    \u2502   \u2514\u2500\u2500 filtering_report/\n    \u2502       \u2514\u2500\u2500 run01_filtered/\n    \u2502           \u251c\u2500\u2500 error_model.pdf\n    \u2502           \u251c\u2500\u2500 error_model.pdf.rds\n    \u2502           \u251c\u2500\u2500 error_model.rds\n    \u2502           \u251c\u2500\u2500 filtering_report.pdf\n    \u2502           \u251c\u2500\u2500 filtering_report.pdf.rds\n    \u2502           \u251c\u2500\u2500 sample1-R1.fastq\n    \u2502           \u251c\u2500\u2500 sample1-R2.fastq\n    \u2502           \u251c\u2500\u2500 sample2-R1.fastq\n    \u2502           \u251c\u2500\u2500 sample2-R2.fastq\n    \u2502           \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 aggregated_error_plots.pdf\n    \u251c\u2500\u2500 aggregated_filtering_plots.pdf\n    \u251c\u2500\u2500 aggregated_quality_profiles.pdf\n    \u251c\u2500\u2500 length_distribution.pdf\n    \u251c\u2500\u2500 length_distribution.rds\n    \u251c\u2500\u2500 read_tracking_report.pdf\n    \u251c\u2500\u2500 seqtab_nochim.rds\n    \u251c\u2500\u2500 seqtab_nochim.txt\n    \u251c\u2500\u2500 seqtab.rds\n    \u251c\u2500\u2500 taxonomy_species.rds\n    \u251c\u2500\u2500 taxonomy_species.txt\n    \u251c\u2500\u2500 taxonomy.rds\n    \u251c\u2500\u2500 taxonomy.txt\n    \u2514\u2500\u2500 tree.rds\n</code></pre>"},{"location":"NextFlow/dada2_16S/#rights","title":"Rights","text":"<ul> <li>Copyright \u00a9\ufe0f 2025 Mucosal Immunology Lab, Monash University, Melbourne, Australia.</li> <li>Licence: This pipeline is provided under the MIT license.</li> <li>Authors: M. Macowan</li> </ul> <ul> <li>Illumina Utils: Meren Lab</li> <li>DADA2: Benjamin Callahan et al.</li> <li>SILVA: arb-silva.de</li> <li>RAxML: Exelixis Lab</li> <li>phangorn: Klaus Schliep et al.</li> <li>ips: Christoph Heibl et al.</li> </ul>"},{"location":"NextFlow/metagenomics/","title":"Shotgun metagenomics sequencing pre-processing","text":""},{"location":"NextFlow/metagenomics/#introduction","title":"Introduction \ud83d\udcd6","text":"<p>nf-mucimmuno/metagenomics is a bioinformatics pipeline that can be used for pre-processing of shotgun metagenomics sequencing reads. It takes a sample sheet and raw sequencing <code>.fastq.gz</code> files, performs quality profiling, filtering and trimming, removal of low complexity reads, and host decontamination. MultiQC is run on the FastQC outputs both before and after TrimGalore! for visual inspection of sample quality \u2013 output <code>.html</code> files are collected in the results. Reads are then assigned taxonomic classifications using Kraken2 and abundance-corrected via Bracken.</p> <p></p>"},{"location":"NextFlow/metagenomics/#usage","title":"Usage","text":""},{"location":"NextFlow/metagenomics/#download-the-repository","title":"Download the repository \ud83d\udcc1","text":"<p>This repository contains the relevant Nextflow workflow components, including a conda environment and submodules, to run the pipeline. To retrieve this repository alone, run the <code>retrieve_me.sh</code> script. Alternatively, you can prepare your own download script by following the instructions below.</p> <p>Git version requirements</p> <p>Git <code>sparse-checkout</code> is required to retrieve just the nf-mucimmuno/metagenomics pipeline. It was only introduced to Git in version 2.27.0, so ensure that the loaded version is high enough (or that there is a version loaded on the cluster at all). As of July 2024, the M3 MASSIVE cluster has version 2.38.1 available.</p> <pre><code># Check git version\ngit --version\n\n# Load git module if not loaded or insufficient version\nmodule load git/2.38.1\n</code></pre> <p>First, create a new bash script file.</p> <pre><code># Create and edit a new file with nano\nnano retrieve_me.sh\n</code></pre> <p>Add the contents to the file, save, and close.</p> <pre><code>#!/bin/bash\n\n# Define variables\nREPO_URL=\"https://github.com/mucosal-immunology-lab/nf-mucimmuno\"\nREPO_DIR=\"nf-mucimmuno\"\nSUBFOLDER=\"metagenomics\"\n\n# Clone the repository with sparse checkout\ngit clone --no-checkout $REPO_URL\ncd $REPO_DIR\n\n# Initialize sparse-checkout and set the desired subfolder\ngit sparse-checkout init --cone\ngit sparse-checkout set $SUBFOLDER\n\n# Checkout the files in the subfolder\ngit checkout main\n\n# Move the folder into the main folder and delete the parent\nmv $SUBFOLDER ../\ncd ..\nrm -rf $REPO_DIR\n\necho \"Subfolder '$SUBFOLDER' has been downloaded successfully.\"\n</code></pre> <p>Then run the script to retrieve the repository into a new folder called <code>metagenomics</code>, which will house your workflow files and results.</p> <pre><code># Run the script\nbash retrieve_me.sh\n</code></pre>"},{"location":"NextFlow/metagenomics/#create-the-conda-environment","title":"Create the conda environment \ud83d\udc0d","text":"<p>To create the conda environment, use the provided environment <code>.yaml</code> file. Then activate it to access required functions.</p> <pre><code># Create the environment\nmamba env create -f environment.yaml\n\n# Activate the environment\nmamba activate nextflow-metaG\n</code></pre>"},{"location":"NextFlow/metagenomics/#setting-up-your-folders","title":"Setting up your folders \ud83d\uddc2\ufe0f","text":"<p>Because you specify the full directory path for your raw input data, you can technically house them however and wherever you like. However, below is an example of how to store your raw sequencing files.</p> <pre><code>metagenomics/\n    \u251c\u2500\u2500 modules/\n    \u251c\u2500\u2500 data/\n    \u2502   \u2514\u2500\u2500 raw_fastq/\n    \u2502       \u251c\u2500\u2500 sample1_R1.fastq.gz\n    \u2502       \u251c\u2500\u2500 sample1_R2.fastq.gz\n    \u2502       \u251c\u2500\u2500 sample2_R1.fastq.gz\n    \u2502       \u251c\u2500\u2500 sample2_R2.fastq.gz\n    \u2502       \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 metaG_pipeline.nf\n    \u251c\u2500\u2500 nextflow.config\n    \u251c\u2500\u2500 retrieve_me.sh\n    \u2514\u2500\u2500 sample_sheet.csv\n</code></pre> <p>You don't have to have paired-end reads</p> <p>Although the example folder structure above shows paired-end reads, the pipeline will handle both single- and paired-end reads. You will be able to define this all in your sample sheet, as shown below.</p>"},{"location":"NextFlow/metagenomics/#prepare-your-sample-sheet","title":"Prepare your sample sheet \ud83d\udcdd","text":"<p>This pipeline requires a sample sheet to identify where your sequencing data is located. You can also change the name of each sample to something some specific if you desire from the original folder name.</p> <p>Your sample sheet should look as follows, ensuring you use the exact column names as below. You can also just edit the sample sheet already in the folder.</p> <pre><code>id,fastq_1,fastq_2\nsample1,./data/raw_fastq/sample1.fastq.gz,\nsample2,./data/raw_fastq/sample2_R1.fastq.gz,./data/raw_fastq/sample2_R2.fastq.gz\nsample3,./data/raw_fastq/sample3_R1.fastq.gz,./data/raw_fastq/sample3_R2.fastq.gz\n</code></pre> <p>Sample sheets columns</p> <p>You simply require the column names <code>id</code>, <code>fastq_1</code>, and <code>fastq_2</code>. If you are working with single-ended data, simply leave the <code>fastq_2</code> column blank for the relevant samples.</p>"},{"location":"NextFlow/metagenomics/#running-the-pipeline","title":"Running the pipeline \ud83c\udfc3","text":"<p>Now you can run the pipeline. You will need to set up a parent job to run each of the individual jobs \u2013 this can be either an interactive session, or an sbatch job. For example:</p> <pre><code># Start an interactive session with minimal resources\nsmux n --time=3-00:00:00 --mem=16GB --ntasks=1 --cpuspertask=2 -J nf-metaG\n</code></pre> <p>Set the correct sample sheet location</p> <p>Make sure you alter the <code>nextflow.config</code> file to provide the path to your sample sheet, unless it is <code>./sample_sheet.csv</code> which is the default for the cluster profile. Stay within the top <code>cluster</code> profile section to alter settings for Slurm-submitted jobs.</p> <p>Inside your interactive session, be sure to activate your <code>nextflow-metaG</code> environment from above. Then, inside the metagenomics folder, begin the pipeline using the following command (ensuring you use the <code>cluster</code> profile to make use of the Slurm workflow manager).</p> <pre><code># Activate conda environment\nmamba activate nextflow-metaG\n\n# Begin running the pipeline\nnextflow run metaG_pipeline.nf -resume -profile cluster\n</code></pre>"},{"location":"NextFlow/metagenomics/#customisation","title":"Customisation \u2699\ufe0f","text":"<p>There are several customisation options that are available within the <code>nextflow.config</code> file. While the defaults should be suitable for those with access to the M3 MASSIVE cluster genomics partition, for those without access, of for those who require different amounts of resources, there are ways to change these.</p> <p>To adjust the <code>cluster</code> profile settings, stay within the appropriate section at the top of the file. The <code>local</code> settings are at the bottom.</p> Parameters Option Description samples_csv The file path to your sample sheet outdir A new folder name to be created for your results trimgalore.quality The minimum quality score for, after which reads will be truncated to remove low-quality read ends. (Default: <code>20</code>) trimgalore.length The minimum post-trimming read length required for a read to be retained. (Default: <code>25</code>) trimgalore.adapter Adapter sequence to be trimmed from R1. Defaults to the Illumina adapters. (Default: <code>'AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC'</code>) trimgalore.adapter2 Adapter sequence to be trimmed from R1. Defaults to the Illumina adapters. (Default: <code>'AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT'</code>) decontaminate.hostIndex The base file path to the host index <code>.b2tl</code> files. Be sure to just provide the base name, e.g. <code>'path/to/folder/chm13v2.0_GRCh38_full_plus_decoy'</code>. If you don't provide anything, the pipeline will generate the host index for you and place a copy in the results folder. (Default: <code>''</code>) taxonomy.kraken2_db The folder path to the kraken2 database if you have one prepared. If you don't provide anything, the pipeline will generate it for you and place a copy in the results folder. (Default: <code>''</code>) taxonomy.kmer_length The sliding kmer length for Kraken2 to use for generating the database. This will also be used for generating the Bracken-corrected database version. (Default: <code>35</code>) bracken.bracken_level The level to which Bracken taxonomy should be corrected. (Default: <code>'S'</code> - i.e. species) Process <p>These settings relate to resource allocation and cluster settings. Database building steps in particular take longer than 4 hours, and therefore the default option is to run these steps on the <code>comp</code> partition.</p> Option Description executor The workload manager (default: <code>'slurm'</code>) conda The conda environment to use (default: <code>'./environment.yaml'</code>) queueSize The maximum number of jobs to be submitted at any time (default: <code>12</code>) submitRateLimit The rate allowed for job submission \u2013 either a number of jobs per second (e.g. 20sec) or a number of jobs per time period (e.g. 20/5min) (default: <code>'1/2sec'</code>) memory The maximum global memory allowed for Nextflow to use (default: <code>'320 GB'</code>) FASTQC.memory Memory for FASTQC step to use (default: <code>'80 GB'</code>) FASTQC.cpus Number of CPUs for FASTQC step to use (default: <code>'8'</code>) FASTQC.clusterOptions Specific cluster options for FASTQC step to use (default: <code>'--time=8:00:00'</code>) TRIMGALORE.memory Memory for TRIMGALORE step to use (default: <code>'80 GB'</code>) TRIMGALORE.cpus Number of CPUs for TRIMGALORE step to use (default: <code>'8'</code>) TRIMGALORE.clusterOptions Specific cluster options for TRIMGALORE step to use (default: <code>'--time=8:00:00'</code>) KOMPLEXITY_FILTER.memory Memory for KOMPLEXITY_FILTER step to use (default: <code>'80 GB'</code>) KOMPLEXITY_FILTER.cpus Number of CPUs for KOMPLEXITY_FILTER step to use (default: <code>'12'</code>) KOMPLEXITY_FILTER.clusterOptions Specific cluster options for KOMPLEXITY_FILTER step to use (default: <code>'--time=4:00:00 --partition=genomics --partition=genomics'</code>) PREPARE_HOST_GENOME.memory Memory for PREPARE_HOST_GENOME step to use (default: <code>'40 GB'</code>) PREPARE_HOST_GENOME.cpus Number of CPUs for PREPARE_HOST_GENOME step to use (default: <code>'8'</code>) PREPARE_HOST_GENOME.clusterOptions Specific cluster options for PREPARE_HOST_GENOME step to use (default: <code>'--time=24:00:00'</code>) HOST_DECONTAMINATE.memory Memory for HOST_DECONTAMINATE step to use (default: <code>'80 GB'</code>) HOST_DECONTAMINATE.cpus Number of CPUs for HOST_DECONTAMINATE step to use (default: <code>'12'</code>) HOST_DECONTAMINATE.clusterOptions Specific cluster options for HOST_DECONTAMINATE step to use (default: <code>'--time=4:00:00 --partition=genomics --partition=genomics'</code>) PREPARE_KRAKEN2_DB.memory Memory for PREPARE_KRAKEN2_DB step to use (default: <code>'120 GB'</code>) PREPARE_KRAKEN2_DB.cpus Number of CPUs for PREPARE_KRAKEN2_DB step to use (default: <code>'24'</code>) PREPARE_KRAKEN2_DB.clusterOptions Specific cluster options for PREPARE_KRAKEN2_DB step to use (default: <code>'--time=24:00:00'</code>) CLASSIFY_KRAKEN2.memory Memory for CLASSIFY_KRAKEN2 step to use (default: <code>'120 GB'</code>) CLASSIFY_KRAKEN2.cpus Number of CPUs for CLASSIFY_KRAKEN2 step to use (default: <code>'16'</code>) CLASSIFY_KRAKEN2.clusterOptions Specific cluster options for CLASSIFY_KRAKEN2 step to use (default: <code>'--time=4:00:00 --partition=genomics --partition=genomics'</code>) MERGE_KRAKEN2_REPORTS.memory Memory for MERGE_KRAKEN2_REPORTS step to use (default: <code>'40 GB'</code>) MERGE_KRAKEN2_REPORTS.cpus Number of CPUs for MERGE_KRAKEN2_REPORTS step to use (default: <code>'6'</code>) MERGE_KRAKEN2_REPORTS.clusterOptions Specific cluster options for MERGE_KRAKEN2_REPORTS step to use (default: <code>'--time=4:00:00 --partition=genomics --partition=genomics'</code>) PREPARE_BRACKEN_DB.memory Memory for PREPARE_BRACKEN_DB step to use (default: <code>'120 GB'</code>) PREPARE_BRACKEN_DB.cpus Number of CPUs for PREPARE_BRACKEN_DB step to use (default: <code>'24'</code>) PREPARE_BRACKEN_DB.clusterOptions Specific cluster options for PREPARE_BRACKEN_DB step to use (default: <code>'--time=24:00:00'</code>) <p>Nextflow Error Status (137)</p> <p>Nextflow error status (137) relates to insufficent RAM allocated to the job. If you get this error, try allocating more resources to the job that failed.</p>"},{"location":"NextFlow/metagenomics/#outputs","title":"Outputs \ud83d\udce4","text":"<p>Several outputs will be copied from their respective Nextflow <code>work</code> directories to the output folder of your choice (default: <code>results</code>).</p> <p>The main outputs of interest for downstream processing are your Bracken-corrected <code>.tsv</code> file and your trimmed, decontaminated reads.</p> Output Description <code>filtered_combined_bracken_report.tsv</code> The Bracken-corrected clean counts table matrix <code>decontam/</code> A folder containing all of your trimmed, decontaminated <code>.fastq.gz</code> files \u2013 these can be used for other pipelines such as HUMAnN3 functional profiling. <p>Folder structure</p> <p>There are also a collection of quality control outputs and database files (if you didn't provide them to the pipeline) available in this directory too. If the pipeline generated the databases for you, it is recommended that you move these somewhere else so you can use them in future to save yourself some time. Below is an example of the output structure after running the pipeline.</p> <pre><code>results/\n    \u251c\u2500\u2500 kraken2/\n    \u2502   \u251c\u2500\u2500 combined_kraken2_report.tsv\n    \u2502   \u251c\u2500\u2500 filtered_combined_kraken2_report.tsv\n    \u2502   \u251c\u2500\u2500 sample1.kraken\n    \u2502   \u251c\u2500\u2500 sample1.report\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 bracken/\n    \u2502   \u251c\u2500\u2500 combined_bracken_report.tsv\n    \u2502   \u2514\u2500\u2500 filtered_combined_bracken_report.tsv\n    \u251c\u2500\u2500 decontam/\n    \u2502   \u251c\u2500\u2500 sample1_decontam.fq.gz\n    \u2502   \u251c\u2500\u2500 sample2_R1_decontam.fq.gz\n    \u2502   \u251c\u2500\u2500 sample2_R2_decontam.fq.gz\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 decontam_log/\n    \u2502   \u251c\u2500\u2500 sample1_bowtie2.log\n    \u2502   \u251c\u2500\u2500 sample2_bowtie2.log\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 kraken2_database/ # ~470 GB\n    \u2502   \u251c\u2500\u2500 database.kraken # 50.2 GB\n    \u2502   \u251c\u2500\u2500 database35mers.kmer_distrib\n    \u2502   \u251c\u2500\u2500 database35mers.kraken\n    \u2502   \u251c\u2500\u2500 hash.k2d # 90.9 GB\n    \u2502   \u251c\u2500\u2500 opts.k2d\n    \u2502   \u251c\u2500\u2500 seqid2taxid.map\n    \u2502   \u251c\u2500\u2500 taxo.k2d\n    \u2502   \u251c\u2500\u2500 library/ ... # ~ 223 GB\n    \u2502   \u2514\u2500\u2500 taxonomy/ ... # ~ 46.4 GB\n    \u251c\u2500\u2500 hostIndex/ # ~18.2 GB\n    \u2502   \u251c\u2500\u2500 chm13v2.0_GRCh38_full_plus_decoy.1.bt2l\n    \u2502   \u251c\u2500\u2500 chm13v2.0_GRCh38_full_plus_decoy.2.bt2l\n    \u2502   \u251c\u2500\u2500 chm13v2.0_GRCh38_full_plus_decoy.3.bt2l\n    \u2502   \u251c\u2500\u2500 chm13v2.0_GRCh38_full_plus_decoy.4.bt2l\n    \u2502   \u251c\u2500\u2500 chm13v2.0_GRCh38_full_plus_decoy.rev.1.bt2l\n    \u2502   \u251c\u2500\u2500 chm13v2.0_GRCh38_full_plus_decoy.rev.2.bt2l\n    \u2502   \u2514\u2500\u2500 chm13v2.0_GRCh38_full_plus_decoy.fasta\n    \u251c\u2500\u2500 reports/\n    \u2502   \u251c\u2500\u2500 pretrim_multiqc_report.html\n    \u2502   \u2514\u2500\u2500 posttrim_multiqc_report.html\n    \u251c\u2500\u2500 taxonomy.txt\n    \u2514\u2500\u2500 tree.rds\n</code></pre>"},{"location":"NextFlow/metagenomics/#rights","title":"Rights","text":"<ul> <li>Copyright \u00a9\ufe0f 2025 Mucosal Immunology Lab, Monash University, Melbourne, Australia.</li> <li>Licence: This pipeline is provided under the MIT license.</li> <li>Authors: M. Macowan</li> </ul> <ul> <li>FastQC: Andrews, S. (2010). FastQC: A Quality Control Tool for High Throughput Sequence Data [Online]. Available online at: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/</li> <li>MultiQC: Ewels P, Magnusson M, Lundin S, K\u00e4ller M. MultiQC: summarize analysis results for multiple tools and samples in a single report. Bioinformatics. 2016 Oct 1;32(19):3047-8. doi: 10.1093/bioinformatics/btw354. Epub 2016 Jun 16. PMID: 27312411; PMCID: PMC5039924.</li> <li>TrimGalore!: Felix Krueger, Frankie James, Phil Ewels, Ebrahim Afyounian, Michael Weinstein, Benjamin Schuster-Boeckler, Gert Hulselmans, &amp; sclamons. (2023). FelixKrueger/TrimGalore: v0.6.10 - add default decompression path (0.6.10). Zenodo. https://doi.org/10.5281/zenodo.7598955</li> <li>Komplexity: Clarke EL, Taylor LJ, Zhao C, Connell A, Lee JJ, Fett B, Bushman FD, Bittinger K. Sunbeam: an extensible pipeline for analyzing metagenomic sequencing experiments. Microbiome. 2019 Mar 22;7(1):46. doi: 10.1186/s40168-019-0658-x. PMID: 30902113; PMCID: PMC6429786.</li> <li>bowtie2: Langmead, B., &amp; Salzberg, S. L. (2012). Fast gapped-read alignment with Bowtie 2. Nature Methods, 9(4), 357-359.</li> <li>Kraken2: Wood, D.E., Lu, J. &amp; Langmead, B. Improved metagenomic analysis with Kraken 2. Genome Biol 20, 257 (2019). https://doi.org/10.1186/s13059-019-1891-0.</li> <li>KrakenTools: Lu J, Rincon N, Wood D E, Breitwieser F P, Pockrandt C, Langmead B, Salzberg S L, Steinegger M. Metagenome analysis using the Kraken software suite. Nature Protocols, doi: 10.1038/s41596-022-00738-y (2022).</li> <li>Bracken: Lu, J., Breitwieser, F. P., Thielen, P., &amp; Salzberg, S. L. (2017). Bracken: Estimating species abundance in metagenomics data. PeerJ, 5, e3276.</li> <li>T2T-CHM13v2.0 human genome: Rhie A, Nurk S, Cechova M, Hoyt SJ, Taylor DJ, et al. The complete sequence of a human Y chromosome. bioRxiv, 2022. See GitHub page.</li> <li>GRCh38 human genome: The Human Genome Project, currently maintained by the Genome Reference Consortium (GRC). See its NIH page.</li> </ul>"},{"location":"NextFlow/scRNAseq/","title":"Single-cell RNAseq FASTQ pre-processing","text":""},{"location":"NextFlow/scRNAseq/#introduction","title":"Introduction","text":"<p>nf-mucimmuno/scRNAseq is a bioinformatics pipeline that can be used to run quality control steps and alignment to a host genome using STARsolo. It takes a samplesheet and FASTQ files as input, performs FastQC, trimming and alignment, and produces an output <code>.tar.gz</code> archive containing the collected outputs from STARsolo, ready for further processing downstream in R. MultiQC is run on the FastQC outputs both before and after TrimGalore! for visual inspection of sample quality \u2013 output <code>.html</code> files are collected in the results.</p> <p></p> <p>Citation</p> <p>If you use this workflow and end up publishing something, please consider including a reference to our work! \ud83d\ude0e\ud83d\ude4f</p> <p>Macowan, M., Pattaroni, C., Cardwell, B. A., &amp; Marsland, B. (2024). nf-mucimmuno/scRNAseq (0.1.0). Zenodo. https://doi.org/10.5281/zenodo.14868176</p>"},{"location":"NextFlow/scRNAseq/#usage","title":"Usage","text":""},{"location":"NextFlow/scRNAseq/#download-the-repository","title":"Download the repository \ud83d\udcc1","text":"<p>This repository contains the relevant Nextflow workflow components, including a conda environment and submodules, to run the pipeline. To retrieve this repository alone, run the <code>retrieve_me.sh</code> script above.</p> <p>Git version requirements</p> <p>Git <code>sparse-checkout</code> is required to retrieve just the nf-mucimmuno/scRNAseq pipeline. It was only introduced to Git in version 2.27.0, so ensure that the loaded version is high enough (or that there is a version loaded on the cluster at all). As of July 2024, the M3 MASSIVE cluster has version 2.38.1 available.</p> <pre><code># Check git version\ngit --version\n\n# Load git module if not loaded or insufficient version\nmodule load git/2.38.1\n</code></pre> <p>First, create a new bash script file.</p> <pre><code># Create and edit a new file with nano\nnano retrieve_me.sh\n</code></pre> <p>Add the contents to the file, save, and close.</p> retrieve_me.sh<pre><code>#!/bin/bash\n\n# Define variables\nREPO_URL=\"https://github.com/mucosal-immunology-lab/nf-mucimmuno\"\nREPO_DIR=\"nf-mucimmuno\"\nSUBFOLDER=\"scRNAseq\"\n\n# Clone the repository with sparse checkout\ngit clone --no-checkout $REPO_URL\ncd $REPO_DIR\n\n# Initialize sparse-checkout and set the desired subfolder\ngit sparse-checkout init --cone\ngit sparse-checkout set $SUBFOLDER\n\n# Checkout the files in the subfolder\ngit checkout main\n\n# Move the folder into the main folder and delete the parent\nmv $SUBFOLDER ../\ncd ..\nrm -rf $REPO_DIR\n\n# Extract the larger gzipped CLS files\ngunzip -r \"$SUBFOLDER/modules/starsolo/CLS\"\n\necho \"Subfolder '$SUBFOLDER' has been downloaded successfully.\"\n</code></pre> <p>Then run the script to retrieve the repository into a new folder called <code>scRNAseq</code>, which will house your workflow files and results.</p> <pre><code># Run the script\nbash retrieve_me.sh\n</code></pre>"},{"location":"NextFlow/scRNAseq/#create-the-conda-environment","title":"Create the conda environment \ud83d\udc0d","text":"<p>To create the conda environment, use the provided environment <code>.yaml</code> file. Then activate it to access required functions.</p> <pre><code># Create the environment\nmamba env create -f environment.yaml\n\n# Activate the environment\nmamba activate nextflow-scrnaseq\n</code></pre>"},{"location":"NextFlow/scRNAseq/#prepare-the-genome","title":"Prepare the genome \ud83e\uddec","text":"<p>Create a new folder somewhere to store your genome files. Enter the new folder, and run the relevant code depending on your host organism. Run these steps in an interactive session with ~48GB RAM and 16 cores, or submit them as an sbatch job.</p> <p>Do these databases exist already?</p> <p>Please check if these are already available somewhere before regenerating them yourself!</p> <p>STAR should be loaded already via the conda environment for the genome indexing step. We will set <code>--sjdbOverhang</code> to 79 to be suitable for use with the longer <code>R2</code> FASTQ data resulting from BD Rhapsody single cell sequencing. This may require alteration for other platforms. Essentially, you just need to set <code>--sjdbOverhang</code> to the length of your R2 sequences minus 1.</p> Human genome files \ud83d\udc68\ud83d\udc69Mouse genome files \ud83d\udc01 01_retrieve_human_genome.sh<pre><code>#!/bin/bash\nVERSION=111\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/gtf/homo_sapiens/Homo_sapiens.GRCh38.$VERSION.gtf.gz\ngunzip *\n</code></pre> 01_retrieve_mouse_genome.sh<pre><code>#!/bin/bash\nVERSION=111\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/fasta/mus_musculus/dna/Mus_musculus.GRCm39.dna_sm.primary_assembly.fa.gz\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/gtf/mus_musculus/Mus_musculus.GRCm39.$VERSION.gtf.gz\ngunzip *\n</code></pre> <p>Then use STAR to prepare the genome index.</p> Human genome files \ud83d\udc68\ud83d\udc69Mouse genome files \ud83d\udc01 02_index_human_genome.sh<pre><code>#!/bin/bash\nVERSION=111\nSTAR \\\n    --runThreadN 16 \\\n    --genomeDir \"STARgenomeIndex79/\" \\\n    --runMode genomeGenerate \\\n    --genomeFastaFiles \"Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa\" \\\n    --sjdbGTFfile \"Homo_sapiens.GRCh38.$VERSION.gtf\" \\\n    --sjdbOverhang 79\n</code></pre> 02_index_mouse_genome.sh<pre><code>#!/bin/bash\nVERSION=111\nSTAR \\\n    --runThreadN 16 \\\n    --genomeDir \"STARgenomeIndex79/\" \\\n    --runMode genomeGenerate \\\n    --genomeFastaFiles \"Mus_musculus.GRCm39.dna_sm.primary_assembly.fa\" \\\n    --sjdbGTFfile \"Mus_musculus.GRCm39.$VERSION.gtf\" \\\n    --sjdbOverhang 79\n</code></pre>"},{"location":"NextFlow/scRNAseq/#prepare-your-sample-sheet","title":"Prepare your sample sheet \u270f\ufe0f","text":"<p>This pipeline requires a sample sheet to identify where your FASTQ files are located, and which cell label sequences (CLS) are being utilised.</p> <p>More information about the CLS tags used with BD Rhapsody single-cell RNAseq library preparation can be found here:</p> <ul> <li>BD Rhapsody Sequence Analysis Pipeline \u2013 User's Guide</li> <li>BD Rhapsody Cell Label Structure \u2013 Python Script</li> </ul> <p>More information about the CLS tags used with 10X Chromium single-cell RNAseq library preparation can be found here:</p> <ul> <li>10X Chromium Single Cell 3' Solution V2 and V3 guide (Teich Lab)</li> <li>10X Chromium V2 CLS sequences are 26bp long.</li> <li>10X Chromium V3 CLS sequences are 28bp long.</li> </ul> <p>The benefit of providing the name of the CLS bead versions in the sample sheet is that you can combine runs that utilise different beads together in the same workflow. Keep in mind that if you do this though, there may be some bead-related batch effects to address and correct downstream \u2013 it is always important to check for these effects when combining sequencing runs in any case. The current options are:</p> CLS option Description BD_Original The original BD rhapsody beads and linker sequences BD_Enhanced_V1 First version of enhanced beads with polyT and 5prime capture oligo types, shorter linker sequences, longer polyT, and 0-3 diversity insert bases at the beginning of the sequence BD_Enhanced_V2 Same structure as the enhanced (V1) beads, but with increased CLS diversity (384 vs. 96) 10X_Chromium_V2 Feature a 16 bp cell barcode and a 10 bp unique molecular identifier (UMI) 10X_Chromium_V3 Enhanced sequencing accuracy and resolution with a 16 bp cell barcode and a 12 bp UMI <p>Further, we also need to provide the path to the STAR genome index folder for each sample \u2013 while in many cases this value will remain constant, the benefit of providing this information is that you can process runs with different R2 sequence lengths at the same time. Recall from above that the genome index you use should use an <code>--sjdbOverhang</code> length that of your R2 sequences minus 1.</p> <p>Your sample sheet should look as follows, ensuring you use the exact column names as below. </p> <p>File paths on M3</p> <p>Remember that on the M3 MASSIVE cluster, you need to use the full file path \u2013 relative file paths don't usually work.</p> <pre><code>sample,fastq_1,fastq_2,CLS,GenomeIndex\nCONTROL_S1,CONTROL_S1_R1.fastq.gz,CONTROL_S1_R2.fastq.gz,BD_Enhanced_V2,mf33/Databases/ensembl/human/STARgenomeIndex79\nCONTROL_S2,CONTROL_S2_R1.fastq.gz,CONTROL_S1_R2.fastq.gz,BD_Enhanced_V2,mf33/Databases/ensembl/human/STARgenomeIndex79\nTREATMENT_S1,TREATMENT_S1_R1.fastq.gz,TREATMENT_S1_R2.fastq.gz,BD_Enhanced_V2,mf33/Databases/ensembl/human/STARgenomeIndex79\n</code></pre> <p>An example is provided here.</p>"},{"location":"NextFlow/scRNAseq/#running-the-pipeline","title":"Running the pipeline \ud83c\udfc3","text":"<p>Now you can run the pipeline. You will need to set up a parent job to run each of the individual jobs \u2013 this can be either an interactive session, or an sbatch job. For example:</p> <pre><code># Start an interactive session with minimal resources\nsmux n --time=3-00:00:00 --mem=16GB --ntasks=1 --cpuspertask=2 -J nf-STARsolo\n</code></pre> <p>Set the correct sample sheet location</p> <p>Make sure you alter the <code>nextflow.config</code> file to provide the path to your sample sheet, unless it is <code>./data/samplesheet.csv</code> which is the default for the cluster profile. Stay within the top <code>cluster</code> profile section to alter settings for Slurm-submitted jobs.</p> <p>Inside your interactive session, be sure to activate your <code>nextflow-scrnaseq</code> environment from above. Then, inside the scRNAseq folder, begin the pipeline using the following command (ensuring you use the <code>cluster</code> profile to make use of the Slurm workflow manager).</p> <pre><code># Activate conda environment\nmamba activate nextflow-scrnaseq\n\n# Begin running the pipeline\nnextflow run process_raw_reads.nf -resume -profile cluster\n</code></pre>"},{"location":"NextFlow/scRNAseq/#customisation","title":"Customisation \u2699\ufe0f","text":"<p>There are several customisation options that are available within the <code>nextflow.config</code> file. While the defaults should be suitable for those with access to the M3 MASSIVE cluster genomics partition, for those without access, of for those who require different amounts of resources, there are ways to change these.</p> <p>In order to work with different technologies, and accommodate for differences in cell label structure (CLS), the STAR parameters <code>--soloType</code> and <code>--soloCBmatchWLtype</code> are set in a CLS-dependent manner. This is required, because the BD Rhapsody system has a complex barcode structure. The 10X Chromium system on the other hand has a simple barcode structure with a single barcode and single UMI. Additionally, the <code>--soloCBmatchWLtype = EditDist2</code> only works with <code>--soloType = CB_UMI_Complex</code>, and therefore <code>--soloCBmatchWLtype = 1MM multi Nbase pseudocounts</code> is used for 10X Chromium runs.</p> <ul> <li>For BD Rhapsody sequencing: <code>--soloType = CB_UMI_Complex</code> and <code>--soloCBmatchWLtype = EditDist2</code>.</li> <li>For 10X Chromium sequencing: <code>--soloType = CB_UMI_Simple</code> and <code>--soloCBmatchWLtype = 1MM multi Nbase pseudocounts</code>.</li> <li>Additionally, 10X Chromium runs use <code>--clipAdapterType = CellRanger4</code>.</li> </ul> <p>To adjust the <code>cluster</code> profile settings, stay within the appropriate section at the top of the file.</p> Parameters <p>Visit STAR documentation for explanations of all available options for STARsolo.</p> Option Description samples_csv The file path to your sample sheet outdir A new folder name to be created for your results trimgalore.quality The minimum quality before a sequence is truncated (default: <code>20</code>) trimgalore.adapter A custom adapter sequence for the R1 sequences (default: <code>'AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC'</code>) trimgalore.adapter2 A custom adapter sequence for the R2 sequences (default: <code>'AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT'</code>) starsolo.soloUMIdedup The type of UMI deduplication (default: <code>'1MM_CR'</code>) starsolo.soloUMIfiltering The type of UMI filtering for reads uniquely mapping to genes (default: <code>'MultiGeneUMI_CR'</code>) starsolo.soloCellFilter The method type and parameters for cell filtering (default: <code>'EmptyDrops_CR'</code>) starsolo.soloMultiMappers The counting method for reads mapping for multiple genes (default: <code>'EM'</code>) Process <p>These settings relate to resource allocation and cluster settings. FASTQC and TRIMGALORE steps can take longer than 4 hours for typical single-cell RNAseq file, and therefore the default option is to run these steps on the <code>comp</code> partition.</p> Option Description executor The workload manager (default: <code>'slurm'</code>) conda The conda environment to use (default: <code>'./environment.yaml'</code>) queueSize The maximum number of jobs to be submitted at any time (default: <code>12</code>) submitRateLimit The rate allowed for job submission \u2013 either a number of jobs per second (e.g. 20sec) or a number of jobs per time period (e.g. 20/5min) (default: <code>'1/2sec'</code>) memory The maximum global memory allowed for Nextflow to use (default: <code>'320 GB'</code>) FASTQC.memory Memory for FASTQC step to use (default: <code>'80 GB'</code>) FASTQC.cpus Number of CPUs for FASTQC step to use (default: <code>8</code>) FASTQC.clusterOptions Specific cluster options for FASTQC step (default: <code>'--time=8:00:00'</code>) TRIMGALORE.memory Memory for TRIMGALORE step to use (default: <code>'80 GB'</code>) TRIMGALORE.cpus Number of CPUs for TRIMGALORE step to use (default: <code>8</code>) TRIMGALORE.clusterOptions Specific cluster options for TRIMGALORE step (default : <code>'--time=8:00:00'</code>) STARSOLO.memory Memory for STARSOLO step to use (default: <code>'80 GB'</code>) STARSOLO.cpus Number of CPUs for STARSOLO step to use (default: <code>12</code>) STARSOLO.clusterOptions Specific cluster options for STARSOLO step (default : <code>'--time=4:00:00 --partition=genomics --qos=genomics'</code>) COLLECT_EXPORT_FILES.memory Memory for COLLECT_EXPORT_FILES step to use (default: <code>'32 GB'</code>) COLLECT_EXPORT_FILES.cpus Number of CPUs for COLLECT_EXPORT_FILES step to use (default: <code>8</code>) COLLECT_EXPORT_FILES.clusterOptions Specific cluster options for COLLECT_EXPORT_FILES step (default : <code>'--time=4:00:00 --partition=genomics --qos=genomics'</code>)"},{"location":"NextFlow/scRNAseq/#outputs","title":"Outputs","text":"<p>Several outputs will be copied from their respective Nextflow <code>work</code> directories to the output folder of your choice (default: <code>results</code>).</p> <p>Alignment summary utility script</p> <p> There is also a utility script in the main <code>scRNAseq</code> directory called <code>collect_alignment_summaries.sh</code>. This will navigate into each of the sample folders inside <code>results/STARsolo</code>, and retrieve some key information for you to validate that the alignment worked successfully (from the <code>GeneFull_Ex50pAS</code> subfolder). This can otherwise take quite some time to go through each folder if you have a lot of samples.</p> <ul> <li>After running this, a new file called <code>AlignmentSummary.txt</code> will be generated in the <code>scRNAseq</code> directory. Each sample will be listed by name, with the number of reads, percentage of reads with valid barcodes, and estimated number of cells.</li> <li>It will be immediately obvious that something has gone wrong if you see that the percentage of reads with valid barcodes is very low (e.g. <code>0.02</code> = 2% valid barcodes) \u2013 this is usually paired with a very low estimated cell number.</li> <li>This could indicate that you have used the wrong barcode version for your runs, and therefore the associated barcode whitelist used by the pipeline was incorrect.</li> </ul> <p>A successful example is shown below </p> <pre><code>Sample: Healthy1\nNumber of Reads,353152389\nReads With Valid Barcodes,0.950799\nEstimated Number of Cells,6623\n\nSample: Healthy2\nNumber of Reads,344989615\nReads With Valid Barcodes,0.948577\nEstimated Number of Cells,6631\n# etc...\n</code></pre>"},{"location":"NextFlow/scRNAseq/#collected-export-files","title":"Collected export files \ud83d\udce6","text":"<p>The main output will be a single archive file called <code>export_files.tar.gz</code> that you will take for further downstream pre-processing. It contains STARsolo outputs for each sample, with the respective subfolders described below.</p>"},{"location":"NextFlow/scRNAseq/#reports","title":"Reports \ud83d\udcc4","text":"<p>Within the <code>reports</code> folder, you will find the MultiQC outputs from pre- and post-trimming.</p>"},{"location":"NextFlow/scRNAseq/#starsolo","title":"STARsolo \u2b50","text":"<p>Contains the outputs for each sample from STARsolo, including various log files and package version information.</p> <p>The main output of interest here is a folder called <code>{sample}.Solo.out</code>, which houses subfolders called <code>Gene</code>, <code>GeneFull_Ex50pAS</code>, and <code>Velocyto</code>. It is this main folder for each sample that is added to <code>export_files.tar.gz</code>. * As you will use the gene count data from <code>GeneFull_Ex50pAS</code> downstream, it is a good idea to check the <code>Summary.csv</code> within this folder for each sample to ensure mapping was successful (or use the utility script above).   * One of the key values to inspect is <code>Reads With Valid Barcodes</code>, which should be &gt;0.8 (indicating at least 80% of reads had valid barcodes).   * If you note that this value is closer to 0.02 (i.e. ~2% had valid barcodes), you should double-check to make sure you specified the correct BD Rhapsody beads version. For instance, if you specified <code>BD_Enhanced_V1</code> but actually required <code>BD_Enhanced_V2</code>, the majority of your reads will not match the whitelist, and therefore the reads will be considered invalid.</p> <p>Folder structure</p> <p>Below is an example of the output structure for running one sample. The STARsolo folder would contain additional samples as required.</p> <pre><code>scRNAseq\n\u2514\u2500\u2500 results/\n    \u251c\u2500\u2500 export_files.tar.gz\n    \u251c\u2500\u2500 reports/\n    \u2502   \u251c\u2500\u2500 pretrim_multiqc_report.html\n    \u2502   \u2514\u2500\u2500 posttrim_multiqc_report.html\n    \u2514\u2500\u2500 STARsolo/\n        \u2514\u2500\u2500 sample1/\n            \u251c\u2500\u2500 sample1.Solo.out/\n            \u2502   \u251c\u2500\u2500 Gene/\n            \u2502   \u2502   \u251c\u2500\u2500 filtered/\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 barcodes.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 features.tsv.gz\n            \u2502   \u2502   \u2502   \u2514\u2500\u2500 matrix.mtx.gz\n            \u2502   \u2502   \u251c\u2500\u2500 raw/\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 barcodes.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 features.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 matrix.mtx.gz\n            \u2502   \u2502   \u2502   \u2514\u2500\u2500 UniqueAndMult-EM.mtx.gz\n            \u2502   \u2502   \u251c\u2500\u2500 Features.stats\n            \u2502   \u2502   \u251c\u2500\u2500 Summary.csv\n            \u2502   \u2502   \u2514\u2500\u2500 UMIperCellSorted.txt\n            \u2502   \u251c\u2500\u2500 GeneFull_Ex50pAS/\n            \u2502   \u2502   \u251c\u2500\u2500 filtered/\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 barcodes.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 features.tsv.gz\n            \u2502   \u2502   \u2502   \u2514\u2500\u2500 matrix.mtx.gz\n            \u2502   \u2502   \u251c\u2500\u2500 raw/\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 barcodes.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 features.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 matrix.mtx.gz\n            \u2502   \u2502   \u2502   \u2514\u2500\u2500 UniqueAndMult-EM.mtx.gz\n            \u2502   \u2502   \u251c\u2500\u2500 Features.stats\n            \u2502   \u2502   \u251c\u2500\u2500 Summary.csv\n            \u2502   \u2502   \u2514\u2500\u2500 UMIperCellSorted.txt\n            \u2502   \u251c\u2500\u2500 Velocyto/\n            \u2502   \u2502   \u251c\u2500\u2500 filtered/\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 ambiguous.mtx.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 barcodes.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 features.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 spliced.mtx.gz\n            \u2502   \u2502   \u2502   \u2514\u2500\u2500 unspliced.mtx.gz\n            \u2502   \u2502   \u251c\u2500\u2500 raw/\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 ambiguous.mtx.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 barcodes.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 features.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 spliced.mtx.gz\n            \u2502   \u2502   \u2502   \u2514\u2500\u2500 unspliced.mtx.gz\n            \u2502   \u2502   \u251c\u2500\u2500 Features.stats\n            \u2502   \u2502   \u2514\u2500\u2500 Summary.csv\n            \u2502   \u2514\u2500\u2500 Barcodes.stats\n            \u251c\u2500\u2500 sample1.Log.final.out\n            \u251c\u2500\u2500 sample1.Log.out\n            \u251c\u2500\u2500 sample1.Log.progress.out\n            \u2514\u2500\u2500 versions.yml\n</code></pre>"},{"location":"NextFlow/scRNAseq/#rights","title":"Rights","text":"<ul> <li>Copyright \u00a9\ufe0f 2024 Mucosal Immunology Lab, Monash University, Melbourne, Australia.</li> <li>Licence: This pipeline is provided under the MIT license.</li> <li>Authors: M. Macowan</li> </ul> <ul> <li>FastQC: Babraham Bioinformatics</li> <li>MultiQC: National Genomics Infrastructure (SciLifeLab, Sweden), Seqera Labs et al.</li> <li>TrimGalore!: Babraham Bioinformatics</li> <li>STARsolo: Alex Dobin et al.</li> </ul>"},{"location":"NextFlow/tmp/","title":"Shotgun metagenomics sequencing pre-processing","text":"<ul> <li>Shotgun metagenomics sequencing pre-processing</li> <li>Introduction \ud83d\udcd6</li> <li>Usage<ul> <li>Download the repository :open_file_folder:</li> <li>Create the conda environment </li> <li>Setting up your folders \ud83d\uddc2\ufe0f</li> <li>Prepare your sample sheet </li> <li>Running the pipeline :running:</li> <li>Customisation </li> </ul> </li> <li>Outputs \ud83d\udce4<ul> <li>Folder structure \ud83d\uddc2\ufe0f</li> </ul> </li> </ul>"},{"location":"NextFlow/tmp/#introduction","title":"Introduction \ud83d\udcd6","text":"<p>nf-mucimmuno/metagenomics is a bioinformatics pipeline that can be used for pre-processing of shotgun metagenomics sequencing reads. It takes a sample sheet and raw sequencing <code>.fastq.gz</code> files, performs quality profiling, filtering and trimming, removal of low complexity reads, and host decontamination. Reads are then assigned taxonomic classifications using Kraken2 and abundance-corrected via Bracken.</p>"},{"location":"NextFlow/tmp/#usage","title":"Usage","text":""},{"location":"NextFlow/tmp/#download-the-repository","title":"Download the repository","text":"<p>This repository contains the relevant Nextflow workflow components, including a conda environment and submodules, to run the pipeline. To retrieve this repository alone, run the <code>retrieve_me.sh</code> script above.</p> <p> Git <code>sparse-checkout</code> is required to retrieve just the nf-mucimmuno/metagenomics pipeline. It was only introduced to Git in version 2.27.0, so ensure that the loaded version is high enough (or that there is a version loaded on the cluster at all). As of July 2024, the M3 MASSIVE cluster has version 2.38.1 available. </p> <pre><code># Check git version\ngit --version\n\n# Load git module if not loaded or insufficient version\nmodule load git/2.38.1\n</code></pre> <p>First, create a new bash script file.</p> <pre><code># Create and edit a new file with nano\nnano retrieve_me.sh\n</code></pre> <p>Add the contents to the file, save, and close.</p> <pre><code>#!/bin/bash\n\n# Define variables\nREPO_URL=\"https://github.com/mucosal-immunology-lab/nf-mucimmuno\"\nREPO_DIR=\"nf-mucimmuno\"\nSUBFOLDER=\"metagenomics\"\n\n# Clone the repository with sparse checkout\ngit clone --no-checkout $REPO_URL\ncd $REPO_DIR\n\n# Initialize sparse-checkout and set the desired subfolder\ngit sparse-checkout init --cone\ngit sparse-checkout set $SUBFOLDER\n\n# Checkout the files in the subfolder\ngit checkout main\n\n# Move the folder into the main folder and delete the parent\nmv $SUBFOLDER ../\ncd ..\nrm -rf $REPO_DIR\n\necho \"Subfolder '$SUBFOLDER' has been downloaded successfully.\"\n</code></pre> <p>Then run the script to retrieve the repository into a new folder called <code>metagenomics</code>, which will house your workflow files and results.</p> <pre><code># Run the script\nbash retrieve_me.sh\n</code></pre>"},{"location":"NextFlow/tmp/#create-the-conda-environment","title":"Create the conda environment","text":"<p>To create the conda environment, use the provided environment <code>.yaml</code> file. Then activate it to access required functions.</p> <pre><code># Create the environment\nmamba env create -f environment.yaml\n\n# Activate the environment\nmamba activate nextflow-metaG\n</code></pre>"},{"location":"NextFlow/tmp/#setting-up-your-folders","title":"Setting up your folders \ud83d\uddc2\ufe0f","text":"<p>Because you specify the full directory path for your raw input data, you can technically house them however and wherever you like. However, below is an example of how to store your raw sequencing files.</p> <pre><code>metagenomics/\n    \u251c\u2500\u2500 modules/\n    \u251c\u2500\u2500 data/\n    \u2502   \u2514\u2500\u2500 raw_fastq/\n    \u2502       \u251c\u2500\u2500 sample1_R1.fastq.gz\n    \u2502       \u251c\u2500\u2500 sample1_R2.fastq.gz\n    \u2502       \u251c\u2500\u2500 sample2_R1.fastq.gz\n    \u2502       \u251c\u2500\u2500 sample2_R2.fastq.gz\n    \u2502       \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 metaG_pipeline.nf\n    \u251c\u2500\u2500 nextflow.config\n    \u251c\u2500\u2500 retrieve_me.sh\n    \u2514\u2500\u2500 sample_sheet.csv\n</code></pre>"},{"location":"NextFlow/tmp/#prepare-your-sample-sheet","title":"Prepare your sample sheet","text":"<p>This pipeline requires a sample sheet to identify where your sequencing data is located. You can also change the name of each sample to something some specific if you desire from the original folder name.</p> <p>Your sample sheet should look as follows, ensuring you use the exact column names as below. You can also just edit the sample sheet already in the folder.</p> <pre><code>id,fastq_1,fastq_2\nsample1,./data/raw_fastq/sample1.fastq.gz,\nsample2,./data/raw_fastq/sample2_R1.fastq.gz,./data/raw_fastq/sample2_R2.fastq.gz\nsample3,./data/raw_fastq/sample3_R1.fastq.gz,./data/raw_fastq/sample3_R2.fastq.gz\n</code></pre> <p>An example is provided here.</p>"},{"location":"NextFlow/tmp/#running-the-pipeline-running","title":"Running the pipeline :running:","text":"<p>Now you can run the pipeline. You will need to set up a parent job to run each of the individual jobs \u2013 this can be either an interactive session, or an sbatch job. For example:</p> <pre><code># Start an interactive session with minimal resources\nsmux n --time=3-00:00:00 --mem=32GB --ntasks=1 --cpuspertask=2 -J nf-metaG\n</code></pre> <p>Make sure you alter the <code>nextflow.config</code> file to provide the path to your sample sheet, unless it is <code>./sample_sheet.csv</code> which is the default for the cluster profile. Stay within the top <code>cluster</code> profile section to alter settings for Slurm-submitted jobs.</p> <p>Inside your interactive session, be sure to activate your <code>nextflow-metaG</code> environment from above. Then, inside the metagenomics folder, begin the pipeline using the following command (ensuring you use the <code>cluster</code> profile to make use of the Slurm workflow manager).</p> <p> If you are not providing the paths to prepared databases, they will be generated on the fly (which naturally takes quite some time, particularly for the Kraken2 database). However, for the host decontamination step, please be aware that only the human genome will be generated \u2013 as such, if you are working with microbiome samples from another host species, please provide the path and base file name of a suitable bowtie2 indexed host genome (built with <code>--large-index</code>, i.e. <code>.bt2l</code> file types). </p> <pre><code># Activate conda environment\nmamba activate nextflow-metaG\n\n# Begin running the pipeline\nnextflow run metaG_pipeline.nf -resume -profile cluster\n</code></pre>"},{"location":"NextFlow/tmp/#customisation","title":"Customisation","text":"<p>There are some customisation options that are available within the <code>nextflow.config</code> file. While the defaults should be suitable for those with access to the M3 MASSIVE cluster genomics partition, for those without access, or for those who require different amounts of resources, there are ways to change these.</p> <p>To adjust the <code>cluster</code> profile settings, stay within the appropriate section at the top of the file. The <code>local</code> settings are at the bottom.</p> <p>Parameters</p> Option Description samples_csv The file path to your sample sheet outdir A new folder name to be created for your results trimgalore.quality The minimum quality score for, after which reads will be truncated to remove low-quality read ends. (Default: <code>20</code>) trimgalore.length The minimum post-trimming read length required for a read to be retained. (Default: <code>25</code>) trimgalore.adapter Adapter sequence to be trimmed from R1. Defaults to the Illumina adapters. (Default: <code>'AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC'</code>) trimgalore.adapter2 Adapter sequence to be trimmed from R1. Defaults to the Illumina adapters. (Default: <code>'AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT'</code>) decontaminate.hostIndex The base file path to the host index <code>.b2tl</code> files. Be sure to just provide the base name, e.g. <code>'path/to/folder/chm13v2.0_GRCh38_full_plus_decoy'</code>. If you don't provide anything, the pipeline will generate the host index for you and place a copy in the results folder. (Default: <code>''</code>) taxonomy.kraken2_db The folder path to the kraken2 database if you have one prepared. If you don't provide anything, the pipeline will generate it for you and place a copy in the results folder. (Default: <code>''</code>) taxonomy.kmer_length The sliding kmer length for Kraken2 to use for generating the database. This will also be used for generating the Bracken-corrected database version. (Default: <code>35</code>) bracken.bracken_level The level to which Bracken taxonomy should be corrected. (Default: <code>'S'</code> - i.e. species) <p>Process</p> Option Description executor The workload manager (default: <code>'slurm'</code>) conda The conda environment to use (default: <code>'./environment.yaml'</code>) queueSize The maximum number of jobs to be submitted at any time (default: <code>12</code>) submitRateLimit The rate allowed for job submission \u2013 either a number of jobs per second (e.g. 20sec) or a number of jobs per time period (e.g. 20/5min) (default: <code>'1/2sec'</code>) memory The maximum global memory allowed for Nextflow to use (default: <code>'320 GB'</code>) FASTQC.memory Memory for FASTQC step to use (default: <code>'80 GB'</code>) FASTQC.cpus Number of CPUs for FASTQC step to use (default: <code>'8'</code>) FASTQC.clusterOptions Specific cluster options for FASTQC step to use (default: <code>'--time=8:00:00'</code>) TRIMGALORE.memory Memory for TRIMGALORE step to use (default: <code>'80 GB'</code>) TRIMGALORE.cpus Number of CPUs for TRIMGALORE step to use (default: <code>'8'</code>) TRIMGALORE.clusterOptions Specific cluster options for TRIMGALORE step to use (default: <code>'--time=8:00:00'</code>) KOMPLEXITY_FILTER.memory Memory for KOMPLEXITY_FILTER step to use (default: <code>'80 GB'</code>) KOMPLEXITY_FILTER.cpus Number of CPUs for KOMPLEXITY_FILTER step to use (default: <code>'12'</code>) KOMPLEXITY_FILTER.clusterOptions Specific cluster options for KOMPLEXITY_FILTER step to use (default: <code>'--time=4:00:00 --partition=genomics --partition=genomics'</code>) PREPARE_HOST_GENOME.memory Memory for PREPARE_HOST_GENOME step to use (default: <code>'40 GB'</code>) PREPARE_HOST_GENOME.cpus Number of CPUs for PREPARE_HOST_GENOME step to use (default: <code>'8'</code>) PREPARE_HOST_GENOME.clusterOptions Specific cluster options for PREPARE_HOST_GENOME step to use (default: <code>'--time=24:00:00'</code>) HOST_DECONTAMINATE.memory Memory for HOST_DECONTAMINATE step to use (default: <code>'80 GB'</code>) HOST_DECONTAMINATE.cpus Number of CPUs for HOST_DECONTAMINATE step to use (default: <code>'12'</code>) HOST_DECONTAMINATE.clusterOptions Specific cluster options for HOST_DECONTAMINATE step to use (default: <code>'--time=4:00:00 --partition=genomics --partition=genomics'</code>) PREPARE_KRAKEN2_DB.memory Memory for PREPARE_KRAKEN2_DB step to use (default: <code>'120 GB'</code>) PREPARE_KRAKEN2_DB.cpus Number of CPUs for PREPARE_KRAKEN2_DB step to use (default: <code>'24'</code>) PREPARE_KRAKEN2_DB.clusterOptions Specific cluster options for PREPARE_KRAKEN2_DB step to use (default: <code>'--time=24:00:00'</code>) CLASSIFY_KRAKEN2.memory Memory for CLASSIFY_KRAKEN2 step to use (default: <code>'120 GB'</code>) CLASSIFY_KRAKEN2.cpus Number of CPUs for CLASSIFY_KRAKEN2 step to use (default: <code>'16'</code>) CLASSIFY_KRAKEN2.clusterOptions Specific cluster options for CLASSIFY_KRAKEN2 step to use (default: <code>'--time=4:00:00 --partition=genomics --partition=genomics'</code>) MERGE_KRAKEN2_REPORTS.memory Memory for MERGE_KRAKEN2_REPORTS step to use (default: <code>'40 GB'</code>) MERGE_KRAKEN2_REPORTS.cpus Number of CPUs for MERGE_KRAKEN2_REPORTS step to use (default: <code>'6'</code>) MERGE_KRAKEN2_REPORTS.clusterOptions Specific cluster options for MERGE_KRAKEN2_REPORTS step to use (default: <code>'--time=4:00:00 --partition=genomics --partition=genomics'</code>) PREPARE_BRACKEN_DB.memory Memory for PREPARE_BRACKEN_DB step to use (default: <code>'120 GB'</code>) PREPARE_BRACKEN_DB.cpus Number of CPUs for PREPARE_BRACKEN_DB step to use (default: <code>'24'</code>) PREPARE_BRACKEN_DB.clusterOptions Specific cluster options for PREPARE_BRACKEN_DB step to use (default: <code>'--time=24:00:00'</code>) <p>The required resources may vary for your particular data, and can be adjusted as needed. The database creation steps take a long time and a lot of resources to run. These steps really do benefit/require utilisation of a cluster environment.</p>"},{"location":"NextFlow/tmp/#outputs","title":"Outputs \ud83d\udce4","text":"<p>Several outputs will be copied from their respective Nextflow <code>work</code> directories to the output folder of your choice (default: <code>results</code>).</p> <p>The main outputs of interest for downstream processing are your Bracken-corrected <code>.tsv</code> file and your trimmed, decontaminated reads.</p> Output Description <code>filtered_combined_bracken_report.tsv</code> The Bracken-corrected clean counts table matrix <code>decontam/</code> A folder containing all of your trimmed, decontaminated <code>.fastq.gz</code> files \u2013 these can be used for other pipelines such as HUMAnN3 functional profiling."},{"location":"NextFlow/tmp/#folder-structure","title":"Folder structure \ud83d\uddc2\ufe0f","text":"<p>There are also a collection of quality control outputs and database files (if you didn't provide them to the pipeline) available in this directory too. If the pipeline generated the databases for you, it is recommended that you move these somewhere else so you can use them in future to save yourself some time. Below is an example of the output structure after running the pipeline.</p> <pre><code>results/\n    \u251c\u2500\u2500 kraken2/\n    \u2502   \u251c\u2500\u2500 combined_kraken2_report.tsv\n    \u2502   \u251c\u2500\u2500 filtered_combined_kraken2_report.tsv\n    \u2502   \u251c\u2500\u2500 sample1.kraken\n    \u2502   \u251c\u2500\u2500 sample1.report\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 bracken/\n    \u2502   \u251c\u2500\u2500 combined_bracken_report.tsv\n    \u2502   \u2514\u2500\u2500 filtered_combined_bracken_report.tsv\n    \u251c\u2500\u2500 decontam/\n    \u2502   \u251c\u2500\u2500 sample1_decontam.fq.gz\n    \u2502   \u251c\u2500\u2500 sample2_R1_decontam.fq.gz\n    \u2502   \u251c\u2500\u2500 sample2_R2_decontam.fq.gz\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 decontam_log/\n    \u2502   \u251c\u2500\u2500 sample1_bowtie2.log\n    \u2502   \u251c\u2500\u2500 sample2_bowtie2.log\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 kraken2_database/ # ~470 GB\n    \u2502   \u251c\u2500\u2500 database.kraken # 50.2 GB\n    \u2502   \u251c\u2500\u2500 database35mers.kmer_distrib\n    \u2502   \u251c\u2500\u2500 database35mers.kraken\n    \u2502   \u251c\u2500\u2500 hash.k2d # 90.9 GB\n    \u2502   \u251c\u2500\u2500 opts.k2d\n    \u2502   \u251c\u2500\u2500 seqid2taxid.map\n    \u2502   \u251c\u2500\u2500 taxo.k2d\n    \u2502   \u251c\u2500\u2500 library/ ... # ~ 223 GB\n    \u2502   \u2514\u2500\u2500 taxonomy/ ... # ~ 46.4 GB\n    \u251c\u2500\u2500 hostIndex/ # ~18.2 GB\n    \u2502   \u251c\u2500\u2500 chm13v2.0_GRCh38_full_plus_decoy.1.bt2l\n    \u2502   \u251c\u2500\u2500 chm13v2.0_GRCh38_full_plus_decoy.2.bt2l\n    \u2502   \u251c\u2500\u2500 chm13v2.0_GRCh38_full_plus_decoy.3.bt2l\n    \u2502   \u251c\u2500\u2500 chm13v2.0_GRCh38_full_plus_decoy.4.bt2l\n    \u2502   \u251c\u2500\u2500 chm13v2.0_GRCh38_full_plus_decoy.rev.1.bt2l\n    \u2502   \u251c\u2500\u2500 chm13v2.0_GRCh38_full_plus_decoy.rev.2.bt2l\n    \u2502   \u2514\u2500\u2500 chm13v2.0_GRCh38_full_plus_decoy.fasta\n    \u251c\u2500\u2500 reports/\n    \u2502   \u251c\u2500\u2500 pretrim_multiqc_report.html\n    \u2502   \u2514\u2500\u2500 posttrim_multiqc_report.html\n    \u251c\u2500\u2500 taxonomy.txt\n    \u2514\u2500\u2500 tree.rds\n</code></pre>"},{"location":"PublicDatasets/public-datasets/","title":"Public datasets","text":"<p>Here we provide a list of publicly-available datasets that we have generated and uploaded to repositories. Some of the data is yet to be released, and will be available following publication.</p>"},{"location":"PublicDatasets/public-datasets/#ncbi-sequencing-read-archive","title":"NCBI Sequencing Read Archive","text":"<p>The following datasets have been uploaded to the NCBI Sequencing Read Archive (SRA) database in their original FASTQ data format.</p>"},{"location":"PublicDatasets/public-datasets/#summary","title":"Summary","text":"Sequencing type Sequencing runs (uploaded) Bulk transcriptomics 425 Single-cell transcriptomics 2 Shotgun metagenomics 310 16S amplicon 1,099 ITS amplicon 373"},{"location":"PublicDatasets/public-datasets/#datasets","title":"Datasets","text":"Host organism Context BioProject Availability Bulk transcriptomics Single-cell transcriptomics Shotgun metagenomics 16S amplicon ITS amplicon Mouse High fat diet PRJNA1131116 \u2013 2024  Released 24 ileum luminal samples + 24 ileum mucosal samples + 22 colon luminal samples 30 stool samples Mouse Early life antibiotic treatment PRJNA1112091 \u2013 2024  Released 2 lung structural cell digests 96 stool samples 41 lung tissue samples + 30 BAL samples Mouse SHIP-deficient model of Crohn's-like ileitis and chronic lung inflammation PRJNA1086166 \u2013 2024  Released 24 stool samples Human Paediatric severe wheeze + asthma PRJNA1080233 \u2013 2024  Released 55 bronchial brushes 28 bronchial brushes Human Paediatric healthy + infant wheeze PRJNA1076275 \u2013 2024  Released 188 nasal swabs + 73 blood samples 320 nasal swabs 135 nasal swabs Human Infant cystic fibrosis PRJNA978345 \u2013 2024  Released 96 stool samples 75 BAL samples Rat Early life stress + mild traumatic brain injury PRJNA940177 \u2013 2024  Released 76 stool samples Mouse OTII cells Germinal centre expansion + IL-21 role PRJNA776662 \u2013 2021  Released 8 culture samples Human Early life + airways PRJNA694493 \u2013 2021  Released 85 nasal swabs 118 nasal swabs + 119 oropharyngeal swabs 119 nasal swabs + 119 oropharyngeal swabs Mouse Allergic airway inflammation PRJNA641984 \u2013 2020  Released 20 stool samples 127 stool samples Human Male-associated infertility PRJNA509076 \u2013 2018  Released 94 seminal fluid samples Human Early life + immune development PRJNA475630 \u2013 2018  Released 16 tracheal aspirates 45 tracheal aspirates"},{"location":"PublicDatasets/public-datasets/#european-nucleotide-archive","title":"European Nucleotide Archive","text":"<p>The following datasets have been uploaded to the European Nucleotide Archive (ENA) database in their original FASTQ data format.</p>"},{"location":"PublicDatasets/public-datasets/#summary_1","title":"Summary","text":"Sequencing type Sequencing runs (uploaded) 16S amplicon 1,179"},{"location":"PublicDatasets/public-datasets/#datasets_1","title":"Datasets","text":"Host organism Context Project ID 16S amplicon Availability Human Early life + atopic dermatitis PRJEB42268 \u2013 2022  Released 1,179 lateral upper arm swabs"},{"location":"RNAseq/rnaseq-nfcore/","title":"Processing RNA sequencing data with nf-core","text":""},{"location":"RNAseq/rnaseq-nfcore/#overview","title":"Overview","text":"<p>Here we will describe the process for processing RNA sequencing data using the nf-core/rnaseq pipeline. This document was written as of version 3.14.0</p> <p>nf-core/rnaseq is a bioinformatics pipeline that can be used to analyse RNA sequencing data obtained from organisms with a reference genome and annotation. It takes a samplesheet and FASTQ files as input, performs quality control (QC), trimming and (pseudo-)alignment, and produces a gene expression matrix and extensive QC report.</p> <p>Full details of the pipeline and the many customisable options can be view on the pipeline website.</p> <p></p>"},{"location":"RNAseq/rnaseq-nfcore/#installation","title":"Installation","text":"<p>In this section, we discuss the installation process on the M3 MASSIVE cluster.</p>"},{"location":"RNAseq/rnaseq-nfcore/#create-nextflow-environment","title":"Create nextflow environment \ud83d\udc0d","text":"<p>To begin with, we need to create a new environment using mamba. Mamba is recommended here over conda due to its massively improved dependency solving speeds and parallel package downloading (among other reasons).</p> <pre><code># Create environment\nmamba create -n nextflow nextflow \\\n    salmon=1.10.0 fq fastqc umi_tools \\\n    trim-galore bbmap sortmerna samtools \\\n    picard stringtie bedtools rseqc \\\n    qualimap preseq multiqc subread \\\n    ucsc-bedgraphtobigwig ucsc-bedclip \\\n    bioconductor-deseq2\n\n# Activate environment\nmamba activate nextflow\n</code></pre>"},{"location":"RNAseq/rnaseq-nfcore/#download-and-compile-rsem","title":"Download and compile RSEM","text":"<p>RSEM is a software package for estimating gene and isoform expression levels from RNA-Seq data.</p> <pre><code># Download RSEM\ngit clone https://github.com/deweylab/RSEM\n\n# Enter the directory (RSEM) and compile\ncd RSEM; make\n</code></pre> <p>Make note of this directory for your run script so you can add this to your PATH variable.</p>"},{"location":"RNAseq/rnaseq-nfcore/#prepare-your-sample-sheet","title":"Prepare your sample sheet \u270f\ufe0f","text":"<p>You will need to have a sample sheet prepared that contains a sample name, the <code>fastq.gz</code> file paths, and the strandedness of the read files.</p> <p>If you are working with a single-ended sequencing run, leave the <code>fastq_2</code> column empty, but the header still needs to be included.</p> <p>For example, <code>samplesheet.csv</code>:</p> <pre><code>sample,fastq_1,fastq_2,strandedness\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L003_R1_001.fastq.gz,AEG588A1_S1_L003_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L004_R1_001.fastq.gz,AEG588A1_S1_L004_R2_001.fastq.gz,auto\n</code></pre> <p>Each row represents a fastq file (single-end) or a pair of fastq files (paired end). Rows with the same sample identifier are considered technical replicates and merged automatically. The strandedness refers to the library preparation and will be automatically inferred if set to auto.</p>"},{"location":"RNAseq/rnaseq-nfcore/#run-the-pipeline","title":"Run the pipeline \ud83c\udf4f","text":""},{"location":"RNAseq/rnaseq-nfcore/#start-a-new-interactive-session","title":"Start a new interactive session","text":"<p>Firstly, we will start a new interactive session on the M3 MASSIVE cluster.</p> <pre><code>smux n --time=2-00:00:00 --mem=128GB --ntasks=1 --cpuspertask=24 -J nf-core/rnaseq\n</code></pre> <p>Once we are inside the interactive session, we need to select an appropriate version of the Java JDK to use. For the Nextflow pipeline we will be running, we need at least version 17+.</p> <pre><code># View available java JDK modules\nmodule avail java\n\n# Load an appropriate one (over version 17)\nmodule load java/openjdk-17.0.2\n\n# Can double-check the correct version is loaded\njava --version\n</code></pre>"},{"location":"RNAseq/rnaseq-nfcore/#test-your-set-up-optional","title":"Test your set-up (optional) \ud83e\uddba","text":"<p>This step is optional, but highly advisable for a first-time setup or when re-installing.</p> <pre><code>nextflow run nf-core/rnaseq -r 3.14.0 \\\n    -profile test \\\n    --outdir test \\\n    -resume \\\n    --skip-dupradar \\\n    --skip_markduplicates\n</code></pre> <p>Um... why are we skipping things?</p> <ul> <li>We skip the <code>dupradar</code> step, because to install <code>bioconductor-dupradar</code>, mamba wants to downgrade <code>salmon</code> to a very early version, which is not ideal </li> <li>We also skip the <code>markduplicates</code> step because it is not recommended to remove duplicates anyway due to normal biological duplicates (i.e. there won't just be 1 copy of a given gene in a complete sample) </li> </ul>"},{"location":"RNAseq/rnaseq-nfcore/#download-genome-files","title":"Download genome files \ud83e\uddec","text":"<p>To avoid issues with genome incompatibility with the version of STAR you are running, it is recommended to simply download the relevant genome fasta and GTF files using the following scripts, and then supply them directly to the function call.</p> Human genome files \ud83d\udc68\ud83d\udc69Mouse genome files \ud83d\udc01 01_retrieve_human_genome.sh<pre><code>#!/bin/bash\nVERSION=111\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/gtf/homo_sapiens/Homo_sapiens.GRCh38.$VERSION.gtf.gz\n</code></pre> 01_retrieve_mouse_genome.sh<pre><code>#!/bin/bash\nVERSION=111\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/fasta/mus_musculus/dna/Mus_musculus.GRCm39.dna_sm.primary_assembly.fa.gz\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/gtf/mus_musculus/Mus_musculus.GRCm39.$VERSION.gtf.gz\n</code></pre>"},{"location":"RNAseq/rnaseq-nfcore/#run-your-rna-sequencing-reads","title":"Run your RNA sequencing reads \ud83c\udfc3","text":"<p>To avoid typing the whole command out (and in case the pipeline crashes), create a script that will handle the process. Two examples are given here, with one for human samples, and one for mouse samples.</p> <ul> <li>You will need to replace the RSEM folder location with your own path from above.</li> <li>Using the <code>save_reference</code> option stores the formatted genome files to save time if you need to resume or restart the pipeline.</li> </ul> Human run script \ud83d\udc68\ud83d\udc69Mouse genome files \ud83d\udc01 02_run_rnaseq_human.sh<pre><code>#!/bin/bash\nmodule load java/openjdk-17.0.2\nexport PATH=$PATH:/home/mmacowan/mf33/tools/RSEM/\n\nnextflow run nf-core/rnaseq -r 3.14.0 \\\n    --input samplesheet.csv \\\n    --outdir rnaseq_output \\\n    --fasta /home/mmacowan/mf33/scratch_nobackup/RNA/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz \\\n    --gtf /home/mmacowan/mf33/scratch_nobackup/RNA/Homo_sapiens.GRCh38.111.gtf.gz \\\n    --skip_dupradar \\\n    --skip_markduplicates \\\n    --save_reference \\\n    -resume\n</code></pre> 02_run_rnaseq_mouse.sh<pre><code>#!/bin/bash\nmodule load java/openjdk-17.0.2\nexport PATH=$PATH:/home/mmacowan/mf33/tools/RSEM/\n\nnextflow run nf-core/rnaseq -r 3.14.0 \\\n    --input samplesheet.csv \\\n    --outdir rnaseq_output \\\n    --fasta /home/mmacowan/mf33/scratch_nobackup/RNA/Mus_musculus.GRCm39.dna_sm.primary_assembly.fa.gz \\\n    --gtf /home/mmacowan/mf33/scratch_nobackup/RNA/Mus_musculus.GRCm39.111.gtf.gz \\\n    --skip_dupradar \\\n    --skip_markduplicates \\\n    --save_reference \\\n    -resume\n</code></pre>"},{"location":"RNAseq/rnaseq-nfcore/#import-data-into-r","title":"Import data into R \ud83d\udce5","text":"<p>We have a standardised method for importing data into R. Luckily for us, the NF-CORE/rnaseq pipeline outputs are provided in <code>.rds</code> format as <code>SummarizedExperiment</code> objects, with bias-corrected gene counts without an offset.</p> <ul> <li><code>salmon.merged.gene_counts_length_scaled.rds</code></li> </ul> Tell me more! <ul> <li>There are two matrices provided to us: <code>counts</code> and <code>abundance</code>.<ul> <li>The <code>counts</code> matrix is a re-estimated counts table that aims to provide count-level data to be compatible with downstream tools such as DESeq2.</li> <li>The <code>abundance</code> matrix is the scaled and normalised transcripts per million (TPM) abundance. TPM explicitly erases information about library size. That is, it estimates the relative abundance of each transcript proportional to the total population of transcripts sampled in the experiment. Thus, you can imagine TPM, in a way, as a partition of unity \u2014 we want to assign a fraction of the total expression (whatever that may be) to transcript, regardless of whether our library is 10M fragments or 100M fragments.</li> </ul> </li> <li>The <code>tximport</code> package has a single function for importing transcript-level estimates. The type argument is used to specify what software was used for estimation. A simple list with matrices, <code>\"abundance\"</code>, <code>\"counts\"</code>, and <code>\"length\"</code>, is returned, where the transcript level information is summarized to the gene-level. Typically, abundance is provided by the quantification tools as TPM (transcripts-per-million), while the counts are estimated counts (possibly fractional), and the <code>\"length\"</code> matrix contains the effective gene lengths. The <code>\"length\"</code> matrix can be used to generate an offset matrix for downstream gene-level differential analysis of count matrices.</li> </ul>"},{"location":"RNAseq/rnaseq-nfcore/#r-code-for-import-and-voom-normalisation","title":"R code for import and voom-normalisation","text":"<p>Here we show our standard process for preparing RNAseq data for downstream analysis.</p> Human \ud83d\udc68\ud83d\udc69Mouse \ud83d\udc01 Prepare Voom-normalised DGE List<pre><code># Load R packages\npkgs &lt;- c('knitr', 'here', 'SummarizedExperiment', 'biomaRt', 'edgeR', 'limma')\npacman::p_load(char = pkgs)\n\n# Import the bias-corrected counts from STAR Salmon\nrna_data &lt;- readRDS(here('input', 'salmon.merged.gene_counts_length_scaled.rds'))\n\n# Get Ensembl annotations\nensembl &lt;- useMart('ensembl', dataset = 'hsapiens_gene_ensembl')\n\nensemblIDs &lt;- rownames(rna_data)\n\ngene_list &lt;- getBM(attributes = c('ensembl_gene_id', 'hgnc_symbol', 'gene_biotype'),\n                filters = 'ensembl_gene_id', values = ensemblIDs, mart = ensembl)\ncolnames(gene_list) &lt;- c(\"gene_id\", \"hgnc_symbol\", \"gene_biotype\")\ngene_list &lt;- filter(gene_list, !duplicated(gene_id))\n\n# Ensure that only genes in the STAR Salmon outputs are kept for the gene list\nrna_data &lt;- rna_data[rownames(rna_data) %in% gene_list$gene_id, ]\n\n# Add the ENSEMBL data to the rowData element\nrowData(rna_data) &lt;- merge(gene_list, rowData(rna_data), by = \"gene_id\", all = FALSE)\n\n# Load the RNA metadata\nmetadata_rna &lt;- read_csv(here('input', 'metadata_rna.csv'))\n\n# Sort the metadata rows to match the order of the abundance data\nrownames(metadata_rna) &lt;- metadata_rna$RNA_barcode\nmetadata_rna &lt;- metadata_rna[colnames(rna_data),]\n\n# Create a DGEList from the SummarizedExperiment object\nrna_data_dge &lt;- DGEList(assay(rna_data, 'counts'), \n                        samples = metadata_rna, \n                        group = metadata_rna$group,\n                        genes = rowData(rna_data),\n                        remove.zeros = TRUE)\n\n# Filter the DGEList based on the group information\ndesign &lt;- model.matrix(~ group, data = rna_data_dge$samples)\nkeep_min10 &lt;- filterByExpr(rna_data_dge, design, min.count = 10)\nrna_data_dge_min10 &lt;- rna_data_dge[keep_min10, ]\n\n# Calculate norm factors and perform voom normalisation\nrna_data_dge_min10 &lt;- calcNormFactors(rna_data_dge_min10)\nrna_data_dge_min10 &lt;- voom(rna_data_dge_min10, design, plot = TRUE)\n\n# Add the normalised abundance data from STAR Salmon and filter to match the counts data\nrna_data_dge_min10$abundance &lt;- as.matrix(assay(rna_data, 'abundance'))[keep_min10, ]\n\n# Select protein coding defined genes only\nrna_data_dge_min10 &lt;- rna_data_dge_min10[rna_data_dge_min10$genes$gene_biotype == \"protein_coding\" &amp; rna_data_dge_min10$genes$hgnc_symbol != \"\", ]\n\n# Add symbol as rowname\nrownames(rna_data_dge_min10) &lt;- rna_data_dge_min10$genes$gene_name\n\n# Save the DGEList\nsaveRDS(rna_data_dge_min10, here('input', 'rna_data_dge_min10.rds'))\n</code></pre> Prepare Voom-normalised DGE List<pre><code># Load R packages\npkgs &lt;- c('knitr', 'here', 'SummarizedExperiment', 'biomaRt', 'edgeR', 'limma')\npacman::p_load(char = pkgs)\n\n# Import the bias-corrected counts from STAR Salmon\nrna_data &lt;- readRDS(here('input', 'salmon.merged.gene_counts_length_scaled.rds'))\n\n# Get Ensembl annotations\nensembl &lt;- useMart('ensembl', dataset = 'mmusculus_gene_ensembl')\n\nensemblIDs &lt;- rownames(rna_data)\n\ngene_list &lt;- getBM(attributes = c('ensembl_gene_id', 'mgi_symbol', 'gene_biotype'),\n                filters = 'ensembl_gene_id', values = ensemblIDs, mart = ensembl)\ncolnames(gene_list) &lt;- c(\"gene_id\", \"mgi_symbol\", \"gene_biotype\")\ngene_list &lt;- filter(gene_list, !duplicated(gene_id))\n\n# Ensure that only genes in the STAR Salmon outputs are kept for the gene list\nrna_data &lt;- rna_data[rownames(rna_data) %in% gene_list$gene_id, ]\n\n# Add the ENSEMBL data to the rowData element\nrowData(rna_data) &lt;- merge(gene_list, rowData(rna_data), by = \"gene_id\", all = FALSE)\n\n# Load the RNA metadata\nmetadata_rna &lt;- read_csv(here('input', 'metadata_rna.csv'))\n\n# Sort the metadata rows to match the order of the abundance data\nrownames(metadata_rna) &lt;- metadata_rna$RNA_barcode\nmetadata_rna &lt;- metadata_rna[colnames(rna_data),]\n\n# Create a DGEList from the SummarizedExperiment object\nrna_data_dge &lt;- DGEList(assay(rna_data, 'counts'), \n                        samples = metadata_rna, \n                        group = metadata_rna$group,\n                        genes = rowData(rna_data),\n                        remove.zeros = TRUE)\n\n# Filter the DGEList based on the group information\ndesign &lt;- model.matrix(~ group, data = rna_data_dge$samples)\nkeep_min10 &lt;- filterByExpr(rna_data_dge, design, min.count = 10)\nrna_data_dge_min10 &lt;- rna_data_dge[keep_min10, ]\n\n# Calculate norm factors and perform voom normalisation\nrna_data_dge_min10 &lt;- calcNormFactors(rna_data_dge_min10)\nrna_data_dge_min10 &lt;- voom(rna_data_dge_min10, design, plot = TRUE)\n\n# Add the normalised abundance data from STAR Salmon and filter to match the counts data\nrna_data_dge_min10$abundance &lt;- as.matrix(assay(rna_data, 'abundance'))[keep_min10, ]\n\n# Select protein coding defined genes only\nrna_data_dge_min10 &lt;- rna_data_dge_min10[rna_data_dge_min10$genes$gene_biotype == \"protein_coding\" &amp; rna_data_dge_min10$genes$mgi_symbol != \"\", ]\n\n# Add symbol as rowname\nrownames(rna_data_dge_min10) &lt;- rna_data_dge_min10$genes$gene_name\n\n# Save the DGEList\nsaveRDS(rna_data_dge_min10, here('input', 'rna_data_dge_min10.rds'))\n</code></pre>"},{"location":"RNAseq/rnaseq-nfcore/#rights","title":"Rights","text":"<p>NF-CORE/rnaseq</p> <p>There are many people to thank here for writing and maintaining the NF-CORE/rnaseq pipeline (see here). If you use this pipeline for your analysis, please cite it using the following doi: 10.5281/zenodo.1400710</p> <p>This document</p> <ul> <li>Copyright \u00a9 2024 \u2013 Mucosal Immunology Lab, Melbourne VIC, Australia</li> <li>Licence: These tools are provided under the MIT licence (see LICENSE file for details)</li> <li>Authors: M. Macowan</li> </ul>"},{"location":"Utilities/convert-raw-novaseq-outputs/","title":"Handling NovaSeq sequencing outputs","text":"<p>Here we discuss how to process the raw sequencing reads directly from the Illumina NovaSeq sequencer.</p>"},{"location":"Utilities/convert-raw-novaseq-outputs/#what-you-should-have-out-of-the-box","title":"What you should have \"out of the box\" \ud83d\uddc3\ufe0f","text":"<p>Our runs are stored in Vault storage, and need to be transferred to the M3 MASSIVE cluster for processing. To inspect your files, the simplest way is to use FileZilla by setting up an SFTP connection as below. You need to ensure you have file access to the Vault prior to this.</p> <p></p> <p>The basic file structure on the Vault should look something like below, with a main folder (long name) that contains the relevant files you need, and generally some sort of metadata file. You need to ensure that you have given all permissions to every file so that you can transfer them to the cluster \u2013 you can do this by right clicking the NovaSeq parent folder, selecting <code>File Attributes...</code>, and then adding all of the <code>Read</code>, <code>Write</code>, and <code>Execute</code> permissions, ensuring you select <code>Recurse into subdirectories</code>.</p> <p></p>"},{"location":"Utilities/convert-raw-novaseq-outputs/#transfer-files-to-the-cluster","title":"Transfer files to the cluster","text":""},{"location":"Utilities/convert-raw-novaseq-outputs/#sequencing-data-transfer","title":"Sequencing data transfer \ud83d\ude9b","text":"<p>Navigate to an appropriate project folder on the cluster. An example command is shown below for transferring the data folder into a new folder called <code>raw_data</code> using <code>rsync</code>. If it doesn't exist, the folder you name will be created for you (just make sure you put a <code>/</code> after the new folder name).</p> <pre><code>rsync -aHWv --stats --progress MONASH\\\\mmac0026@vault-v2.erc.monash.edu:Marsland-CCS-RAW-Sequencing-Archive/vault/03_NovaSeq/NovaSeq25_Olaf_Shotgun/231025_A00611_0223_AHGMNNDRX2/ raw_data/\n</code></pre>"},{"location":"Utilities/convert-raw-novaseq-outputs/#bcl-convert-sample-sheet-preparation","title":"BCL Convert sample sheet preparation \ud83d\uddd2\ufe0f","text":"<p>Create a sample sheet document for BCL Convert (the tool that will demultiplex and prepare out FASTQ files from the raw data). The full documentation can be viewed here.</p> <p>The document should be in the following format, where <code>index</code> is the <code>i7 adapter sequence</code> and <code>index2</code> is the <code>i5 adapter sequence</code>. An additional first column called <code>Lane</code> can be provided to specify a particular lane number only for FASTQ file generation. We will call this file <code>samplesheet.txt</code>.</p> <p>For the indexes, both sequences used on the sample sheet should be the reverse complement of the actual sequences.</p> <p>Ensure correct file encoding \ud83e\ude9f\ud83d\udc40</p> <p>If you make this on a Windows system, ensure you save your output encoded by <code>UTF-8</code> and not <code>UTF-8 with BOM</code>.</p> <pre><code>[Header]\nFileFormatVersion,2\n\n[BCLConvert_Settings]\nCreateFastqForIndexReads,0\n\n[BCLConvert_Data]\nSample_ID,i7_adapter,index,i5_adapter,index2\nAbx1_d21,N701,TAAGGCGA,S502,ATAGAGAG\nAbx2_d21,N702,CGTACTAG,S502,ATAGAGAG\nAbx3_d21,N703,AGGCAGAA,S502,ATAGAGAG\nAbx4_d21,N704,TCCTGAGC,S502,ATAGAGAG\nAbx5_d21,N705,GGACTCCT,S502,ATAGAGAG\n#etc.\n</code></pre>"},{"location":"Utilities/convert-raw-novaseq-outputs/#bcl-convert","title":"BCL Convert \ud83d\udd04","text":""},{"location":"Utilities/convert-raw-novaseq-outputs/#install","title":"Install \u2b07\ufe0f","text":"<p>If you feel the need to have the latest version, visit the Illumina support website and copy the link for the latest CentOS version of the BCL Convert tool.</p> <p>Otherwise use the version that is available on the M3 MASSIVE cluster, and skip to the run section.</p> <pre><code># Download from the support website in the main folder\nwget https://webdata.illumina.com/downloads/software/bcl-convert/bcl-convert-4.2.4-2.el7.x86_64.rpm\n\n# Install using rpm2cpio (change file name as required)\nmodule load rpm2cpio\nrpm2cpio bcl-convert-4.2.4-2.el7.x86_64.rpm | cpio -idv\n</code></pre> <p>The most up-to-date bcl-convert will be inside the output <code>usr/bin/</code> folder, and can be called from that location.</p>"},{"location":"Utilities/convert-raw-novaseq-outputs/#run","title":"Run \ud83c\udfc3","text":"<p>With the <code>raw_data</code> folder and <code>samplesheet.txt</code> both in the same directory, we can now run BCL Convert to generate our demultiplexed FASTQ files. Ensure you have at least 64GB of RAM in your interactive smux session.</p> <p>Open file limit error</p> <p>You will need a very high limit for open files \u2013 BCL Convert will attempt to set this limit to 65,535. However, by default, the limit on the M3 MASSIVE cluster is only 1,024 and cannot be increased by users themselves.</p> <p>You can request additional open file limit from the M3 MASSIVE help desk.</p> <p>Can I run this on my local machine?</p> <p>Please note that the node <code>m3k010</code> has been decommissioned due to system upgrades.</p> <p>However, it is more than possible to run this process quickly on a local machine if you have the raw BCL files available. The minimum requirements (as of BCL Convert v4.0) are:</p> <ul> <li>Hardware requirements<ul> <li>Single multiprocessor or multicore computer</li> <li>Minimum 64 GB RAM</li> </ul> </li> <li>Software requirements<ul> <li>Root access to your computer</li> <li>File system access to adjust ulimit</li> </ul> </li> </ul> <p>You can start an interactive bash session and increase the open file limit as follows:</p> <pre><code># Begin a new interactive bash session on the designated node\nsrun --pty --partition=genomics --qos=genomics --nodelist=m3k010 --mem=320GB --ntasks=1 --cpus-per-task=48 bash -i\n\n# Increase the open file limit to 65,535\nulimit -n 65535\n</code></pre> <pre><code># Run bcl-convert\nbcl-convert \\\n    --bcl-input-directory raw_data \\\n    --output-directory fastq_files \\\n    --sample-sheet samplesheet.txt\n</code></pre> <p>This will create a new output folder called <code>fastq_files</code> that contains your demultiplexed samples.</p>"},{"location":"Utilities/convert-raw-novaseq-outputs/#merge-lanes","title":"Merge lanes \u26d9","text":"<p>If you ran your samples without lane splitting, then you can merge the two lanes together using the following code, saved in the main project folder as <code>merge_lanes.sh</code>, and run using the command: <code>bash merge_lanes.sh</code>.</p> merge_lanes.sh<pre><code>#!/bin/bash\n\n# Merge lanes 1 and 2\ncd fastq_files\nfor f in *.fastq.gz\n  do\n  Basename=${f%_L00*}\n  ## merge R1\n  ls ${Basename}_L00*_R1_001.fastq.gz | xargs cat &gt; ${Basename}_R1.fastq.gz\n  ## merge R2\n  ls ${Basename}_L00*_R2_001.fastq.gz | xargs cat &gt; ${Basename}_R2.fastq.gz\n  done\n\n# Remove individual files to make space\nrm -rf *L00*\n</code></pre>"},{"location":"Utilities/data-best-practices/","title":"Data Best Practices","text":"<p>This page provides a guide to data storage best practices within the Mucosal Immunology Lab, focusing on the efficient and organised management of sequencing and LC-MS data. Additionally, we emphasise the importance of maintaining a centralised record by listing datasets in a communal spreadsheet.</p> <p>We will cover the proper use of Monash Vault for long-term storage, including step-by-step instructions for converting, transferring, and retrieving raw sequencing data. To ensure data integrity and quality, this page details processes such as SHA-256 sum checks and quality control using TrimGalore and FastQC. </p>"},{"location":"Utilities/data-best-practices/#data-storage-and-location","title":"Data storage and location","text":"<p>All raw sequencing and LCMS data must meet the following requirements to ensure ongoing access to the data in the future.</p>"},{"location":"Utilities/data-best-practices/#1-store-raw-data-on-the-vault","title":"1) Store raw data on the Vault \ud83c\udfe6","text":"<p>All data must be stored on the Monash Vault with minimal metadata (sample names, groups, library preparation kits, indexes etc.)</p> Marsland Lab sequencing archive Vault location<pre><code>MONASH\\\\&lt;short-monash-id&gt;@vault-v2.erc.monash.edu:Marsland-CCS-RAW-Sequencing-Archive/vault/\n</code></pre> <p>Vault access \ud83d\udd10</p> <p>Users need to make sure they have requested and been granted Vault access first \u2013 Monash credentials alone are not sufficient.</p> <ul> <li>The <code>&lt;short-monash-id&gt;</code> refers to the short version of your username, rather than the full name as per your email address.</li> </ul>"},{"location":"Utilities/data-best-practices/#convert-raw-sequencing-data","title":"Convert raw sequencing data \ud83d\udd04","text":"<p>The majority of the time, your data will have been converted to FASTQ format via BaseSpace immediately following sequencing. However, if your raw sequencing data is still in <code>BCL</code> format, follow the guide for converting raw NovaSeq outputs.</p>"},{"location":"Utilities/data-best-practices/#transfer-your-data-to-the-vault","title":"Transfer your data to the Vault \u2b06\ufe0f\ud83d\udcc2","text":"<p>Transferring data is a simple process that involves using <code>rsync</code>, which is already installed on the M3 MASSIVE cluster \u2013 see the guide on Vault storage. Simply swap out the following values:</p> <ul> <li><code>local-folder-path</code>: the path to the local (or M3 MASSIVE cluster) folder.</li> <li><code>short-monash-id</code>: your Monash ID.</li> <li><code>sharename</code>: the name of the Vault share folder you want to copy to, i.e. <code>Marsland-CCS-RAW-Sequencing-Archive</code>.</li> <li><code>path</code>: the path to the Vault folder.</li> </ul> <pre><code>rsync -aHWv --stats --progress --no-p --no-g --chmod=ugo=rwX /&lt;local-folder-path&gt;/ MONASH\\\\&lt;short-monash-id&gt;@vault-v2.erc.monash.edu:&lt;sharename&gt;/vault/&lt;path&gt;\n</code></pre>"},{"location":"Utilities/data-best-practices/#transfer-your-data-from-vault","title":"Transfer your data from Vault \u2b07\ufe0f\ud83d\udcc2","text":"<p>The same process works in reverse to retrieve data from the Vault.</p> <pre><code>rsync -aHWv --stats --progress --no-p --no-g --chmod=ugo=rwX MONASH\\\\&lt;short-monash-id&gt;@vault-v2.erc.monash.edu:&lt;sharename&gt;/vault/&lt;path&gt; /&lt;local-folder-path&gt;/\n</code></pre>"},{"location":"Utilities/data-best-practices/#2-list-your-dataset-in-the-communal-spreadsheet","title":"2) List your dataset in the communal spreadsheet \ud83d\udccb","text":"<p>The dataset must be listed in the communal Google Drive Sequencing Data spreadsheet.</p>"},{"location":"Utilities/data-best-practices/#3-check-dataset-integrity-and-quality","title":"3) Check dataset integrity and quality \u2705\ud83d\udcbe","text":"<p>We can verify the integrity of files we transfer up to the Vault using SHA-256 checksums. By generating a checksum for each file before transfer and comparing it with the checksum of the received file, we can confirm that no data corruption or tampering occurred during the process. This method ensures the security and reliability of critical file exchanges, providing confidence that the uploaded data remains identical to the original. Implementing checksum verification as a routine step can be particularly valuable for sensitive or large-scale data transfers.</p>"},{"location":"Utilities/data-best-practices/#generating-sha-256-checksum-files","title":"Generating SHA-256 checksum files \ud83d\udee0\ufe0f\ud83d\udd10","text":"<p>The bash script <code>generate_checksums.sh</code> automates that creation of SHA-256 checksums for all files in each directory within a specified base folder. It will generate one <code>SHA256SUMS</code> files per directory which contains the names and checksums of all regular files in that directory. These will then be uploaded to the Vault along with your data, and can be verified when the data is downloaded again.</p> Generate SHA-256 checksum files<pre><code># Run script with 8 parallel processes\nbash generate_checksums.sh &lt;path-to-folder&gt; -j 8\n</code></pre> What does the script do? <p>Workflow:</p> <ol> <li>It removes any existing <code>SHA256SUMS</code> files in each directory to prevents duplication or conflicts.</li> <li>It generates SHA-256 checksums for all regular files in the directory (excluding <code>SHA256SUMS</code>) and sorts the output for consistency.</li> <li>The checksums are saved in a new <code>SHA256SUMS</code> file in each respective folder.</li> </ol> <p>Parallelisation: it utilises GNU Parallel for faster execution, allowing checksum generation across multiple directories at the same time.</p> <p>Usage:</p> <ul> <li>The default behaviour is to process the current directory (and its subdirectories).</li> <li>Accepts a base directory and additional GNU Parallel options (e.g. specifying the number of parallel jobs using <code>-j</code>).</li> </ul> <p>Ensure GNU Parallel is installed</p> <p>Make sure you have installed GNU Parallel on your system. The script will exit and warn you if it's not installed in any case.</p>"},{"location":"Utilities/data-best-practices/#verifying-sha-256-checksum-files","title":"Verifying SHA-256 checksum files \ud83d\udd0d\ud83d\udd11","text":"<p>The bash script <code>verify_checksums.sh</code> ensures the integrity of files by validating SHA-256 checksums against pre-existing <code>SHA256SUMS</code> files within directory and subfolders. This process is particularly useful for making sure that large files have not become corrupted during data transfer.</p> Verify SHA-256 checksum files<pre><code># Verify checksums with 8 parallel processes\nbash verify_checksums.sh &lt;path-to-folder&gt; -j 8\n</code></pre> What does the script do? <p>Workflow:</p> <ol> <li>Identifies all directories that contain a <code>SHA256SUMS</code> file.</li> <li>Recomputes SHA-256 checksums for all regular files in the directory (excluding the <code>SHA256SUMS</code> file itself).</li> <li>Sorts and compares the recomputed checksums with those in the reference <code>SHA256SUMS</code> file.</li> <li>Reports whether the verification succeeded or failed for each directory, highlighting any discrepancies.</li> </ol> <p>Parallelisation: it utilises GNU Parallel for faster execution, allowing checksum generation across multiple directories at the same time.</p> <p>Usage:</p> <ul> <li>The default behaviour is to process the current directory (and its subdirectories).</li> <li>Accepts a base directory and additional GNU Parallel options (e.g. specifying the number of parallel jobs using <code>-j</code>).</li> </ul> <p>Error handling:</p> <ul> <li>Exits with an error if <code>SHA256SUMS</code> is missing or if checksum verification fails in any directory.</li> <li>Provides detailed output for failed comparisons to help with debugging.</li> </ul> <p>Ensure GNU Parallel is installed</p> <p>Make sure you have installed GNU Parallel on your system. The script will exit and warn you if it's not installed in any case.</p>"},{"location":"Utilities/market-storage/","title":"Market Storage","text":""},{"location":"Utilities/market-storage/#overview","title":"Overview","text":"<p>Market storage, also known as Market-file, is a disk-based storage solution designed for research data that requires regular access, typically on a daily basis. It functions similarly to Monash University's <code>S:\\</code> Drive and is ideal for frequently changing data that is too large for local storage. Market storage can be accessed as a desktop share/network drive (SMB) or mounted on a server (NFS). Access is managed via user groups for SMB shares or network addresses for NFS shares. See the Market Storage User Guide for more information.</p> <p>Requesting storage</p> <p>Storage can be requested or amended on the Monash University research cloud data dashboard.</p>"},{"location":"Utilities/market-storage/#external-access","title":"External access \ud83e\udd77","text":"<p>Access to your Market storage when not using Monash Wi-Fi or WLAN connections requires your computer to be connected to the Monash VPN. Instructions for set-up and connection are available here.</p>"},{"location":"Utilities/market-storage/#features-and-backup-system","title":"Features and backup system \u2601\ufe0f","text":"<p>Exclusively using disk capacity, Market storage offers fast and immediate file access. Data remains in Market until it is either archived to Vault storage or deleted. Backups are conducted daily, with a 30-day retention period for recovery. </p> <p>Backup considerations</p> <p>Changes made within a single day cannot be individually recovered, as only the latest version before the nightly backup is retained. Deleted files can be recovered within the 30-day window but are irretrievable beyond that.</p>"},{"location":"Utilities/market-storage/#recommendations-for-large-collections","title":"Recommendations for large collections \ud83d\udce6\ud83d\udce6\ud83d\udce6","text":"<p>While Market storage supports large datasets, collections exceeding 10TB are best managed using a combination of Market and Vault storage. Active or analysed data should be stored in a smaller Market allocation, while older, unused data can be archived in Vault and retrieved as needed. Market storage can be shared externally through Aspera, a web-based tool for transferring large datasets, though it is unsuitable for critical or sensitive information. With less capacity than Vault storage, Market allocations are quota-based and can be requested via the Data Dashboard.</p>"},{"location":"Utilities/market-storage/#map-the-drive-to-your-computer","title":"Map the drive to your computer \ud83d\uddfa\ufe0f","text":"Linux \ud83d\udc27MacOS \ud83c\udf4eWindows \ud83e\ude9f <p>Monash VPN</p> <p>Ensure that you have connected to the Monash VPN if you are off-site and not connected to the Monash network.</p> <p>The mount method for Linux requires a folder to be present somewhere on your machine \u2013 this needs to be created prior to mapping. This process requires root access to your machine, and will need to be run each time you reboot your machine.</p> Create the mount folder<pre><code>sudo mkdir /mnt/Market-Storage\n</code></pre> <p>The command below mounts a remote storage location to a local directory on a Linux system using the <code>cifs</code> protocol (Common Internet File System). It provides access to a given share for the a particular user within Monash University's domain, using specific permissions and configuration settings.</p> Mount the Market storage to a local folder<pre><code>sudo mount -t cifs //storage.erc.monash.edu.au/shares/&lt;name-of-share&gt; /mnt/Market-Storage -o rw,dir_mode=0777,file_mode=0777,user=&lt;short-monash-id&gt;,domain=MONASH,vers=3\n</code></pre> What are the different parts of the command line code? <ol> <li><code>sudo</code>:<ul> <li>Executes the command with superuser privileges, necessary for mounting file systems.</li> </ul> </li> <li><code>mount -t cifs</code>:<ul> <li><code>mount</code>: mounts a file system</li> <li><code>-t cifs</code>: specifies that a CIFS protocol filesystem is being used</li> </ul> </li> <li><code>//storage.erc.monash.edu.au/shares/&lt;name-of-share&gt;</code>:<ul> <li>The remote network location, specifying the server (<code>//storage.erc.monash.edu</code>) and the specific share being accessed.</li> </ul> </li> <li><code>/mnt/Market-Storage</code>:<ul> <li>The local directory where the remote share will be mounted. Files and directories within the remote share will appear under this directory.</li> </ul> </li> <li><code>-o</code>:<ul> <li>Indicates additional options to be passed to the mount command.</li> </ul> </li> <li>Options in the <code>-o</code> flag:<ul> <li><code>rw</code>: mounts the share with both read and write access.</li> <li><code>dir_mode=0777</code>: sets the permissions for directories to full access (read, write, execute) for all users.</li> <li><code>file_mode=0777</code>: sets the permissions for files to full access (read, write, execute) for all users.</li> <li><code>user=&lt;short-monash-id&gt;</code>: specifies the username to authenticate with the network share. This is your short Monash ID code, not the longer version.</li> <li><code>domain=MONASH</code>: indicates the domain for authentication, which is <code>MONASH</code> here.</li> <li><code>vers=3</code>: specifies the version of the SMB (Server Message Block) protocol to use. Version 3 is more secure and efficient than prior versions.</li> </ul> </li> </ol> <p>Password</p> <p>When you run the <code>mount</code> command, you will be prompted to enter your Monash password. This is your Monash login password, and not necessarily the same as your computer login, depending on which machine you are using.</p> <p>Monash VPN</p> <p>Ensure that you have connected to the Monash VPN if you are off-site and not connected to the Monash network.</p> <ol> <li>With your Finder window open, click the <code>Go</code> menu at the top-left of the screen. </li> <li>Navigate to the <code>Connect to Server...</code> menu, and a new pop-up window will appear.</li> <li>Enter the appropriate remote network location into the top, and press <code>Connect</code>.<ul> <li>This should look something like this: <code>smb://storage.erc.monash.edu.au/shares/&lt;name-of-share&gt;</code></li> <li>Double check whether the network location provided to you has a <code>monash.edu.au</code> or <code>monash.edu</code> suffix.</li> </ul> </li> <li>Enter your Monash ID and password details when prompted, and click on <code>Connect</code></li> </ol> <p>Monash vs. personal computer</p> <p>Note that if you are not using a Monash supplied and configured computer, you will need to add <code>MONASH\\</code> to your username.</p> <p></p> <p>The newly-mapped share will then appear as a folder in the MacOS Finder.</p> <p></p> <p>Monash VPN</p> <p>Ensure that you have connected to the Monash VPN if you are off-site and not connected to the Monash network.</p> <p>The command below mounts a remote storage location to a local directory on a Windows system using the PowerShell command line interface on Windows. It provides access to a given share for the a particular user within Monash University's domain.</p> Mount the Market storage to a local folder<pre><code>net use M: \\\\storage.erc.monash.edu.au\\shares\\&lt;name-of-share&gt; /USER:MONASH\\&lt;short-monash-id&gt;\n</code></pre> What are the different parts of the command line code? <ol> <li><code>net use</code>:<ul> <li>A built-in Windows command to manage network connections. It is used to map or disconnect a network drive or check the status of existing connections.</li> </ul> </li> <li><code>M:</code>:<ul> <li>Specifies the local drive letter to assign to the network share. In this example, the share will be accessible through the <code>M:</code> drive on the user's computer, but can be changed to anything you would like.</li> </ul> </li> <li><code>\\\\storage.erc.monash.edu.au\\shares\\&lt;name-of-share&gt;</code>:<ul> <li>The remote network location, specifying the server (<code>storage.erc.monash.edu</code>) and the specific share being accessed.</li> </ul> </li> <li><code>/USER:MONASH\\&lt;short-monash-id&gt;</code>:<ul> <li>Specifies the credentials to authenticate with the network share. This is your short Monash ID code, not the longer version.</li> </ul> </li> </ol> <p>You can also add <code>/PERSISTENT:YES</code> to the end as an option to make the mapping persistent across reboots.</p> <p>Clearing cached credentials</p> <p>Sometimes you need to clear the cached credentials, perhaps because you entered them incorrectly. To do this, delete the credentials that Windows has \"helpfully\" saved.</p> Clear cached credentials<pre><code>net use \\\\storage.erc.monash.edu\\shares\\&lt;name-of-share&gt; /delete\n</code></pre> <p>You can then re-enter the correct (or updated) credentials.</p>"},{"location":"Utilities/sra-data-submission/","title":"SRA sequencing data submission","text":"<p>A guide to submitting sequencing data to the National Center for Biotechnology Information (NCBI) sequencing read archive (SRA) database. Includes information on uploading data to the SRA using the high-speed Aspera Connect tool.</p> <p>Patient-derived sequencing files</p> <p>If your samples are derived from humans, ensure that your file names include no reference to patient identifiers. Once uploaded to the SRA database, it is very difficult to change the names of files, and requires directly contacting the database to arrange for removal of files and for you to reupload the data. It also involves a difficult process of them re-mapping the new uploads to your existing SRA metadata files.</p> <p>Also ensure that you only include the absolute minimum amount of metadata, in a manner that protects patient confidentiality. Absolutely no information should be unique to one single patient in your cohort, even an age (if you have a patient with a unique age, this should be replaced with <code>NA</code> for the purposes of SRA submission). For manuscripts, you can include a phrase indicating that further metadata is available upon reasonable request. The important thing here is to not infringe on patient privacy and confidentiality.</p> <p>Things you could potentially include:</p> <ul> <li>Modified and anonymised patient ID</li> <li>Sampling group</li> <li>Timepoint (not exact days or months)</li> <li>Sex</li> <li>Collection year (no exact dates)</li> <li>Tissue</li> </ul>"},{"location":"Utilities/sra-data-submission/#process-overview","title":"Process overview","text":"<ol> <li>Register a BioProject</li> <li>Register BioSamples for the related BioProject</li> <li>Submit data to SRA</li> </ol>"},{"location":"Utilities/sra-data-submission/#register-a-bioproject","title":"Register a BioProject \ud83d\udcd4","text":"<p>The BioProject is an important element that can link together different types of sequencing data, and represents all the sequencing data for a given experiment.</p> <p>Go to the SRA submission website to register a new BioProject.</p> <ul> <li>Sample scope: Multispecies (if you have microbiome data)</li> <li>Target description: Bacterial 16S metagenomics (change if you have shotgun metagenomics and/or host transcriptomics)</li> <li>Organism name: Human (change if using mouse or rat data)</li> <li>Project type: Metagenome (add transcriptome if you also have host transcriptomics)</li> </ul>"},{"location":"Utilities/sra-data-submission/#register-biosamples","title":"Register BioSamples \ud83e\uddea","text":""},{"location":"Utilities/sra-data-submission/#microbiome-data","title":"Microbiome data \ud83e\udda0","text":"<p>Microbiome samples will be registered as MIMARKS Specimen samples. On the BioSample Attributes tab, download the BioSample metadata Excel template, and complete it accordingly before uploading. Be very careful with the required field formats. You can double check ontology using the EMBL-EBI Ontology Lookup Service.</p> <ul> <li>Use the BioProject accession number previously generated</li> <li>Organism: <code>human metagenome</code> (or as appropriate)</li> <li>Env broad scale: <code>host-associated</code></li> <li>Env local scale: <code>mammalia-associated habitat</code></li> <li>Env medium: (as appropriate \u2013 e.g. <code>bronchial brushing</code>, <code>stool</code>, etc.)</li> <li>Strain, isolate, cultivar, ecotype: <code>NA</code></li> <li>Host: (as appropriate \u2013 e.g. <code>Homo sapiens</code>, <code>Mus musculus</code>, etc.)</li> <li>Latitude/Longitude: (as appropriate \u2013 e.g. <code>37.8457 S 144.9819 E</code> for the Alfred Hospital, Melbourne)</li> <li>Add any other relevant host information in the table, as well as the host tissue samples</li> <li>Any other column which is not relevant can be set to <code>NA</code></li> </ul> <p>The SRA Metadata tab is what will join everything together. Once again, download the provided Excel template, and fill everything in carefully.</p> <ul> <li>Sample name: the base name of your samples</li> <li>Library ID: you may have named your files differently than your sample names \u2013 provide this if so, otherwise you can repeat the sample name</li> <li>Title: a short description of the sample in the form \"<code>{methodology}</code> of <code>{organism}</code>: <code>{sample_info}</code>\" \u2013 e.g. \"Shotgun metagenomics of Homo sapiens: childhood bronchial brushing\".</li> <li>Library strategy: <code>WGS</code></li> <li>Library source: <code>METAGENOMIC</code></li> <li>Library selection: <code>RANDOM</code></li> <li>Library layout: <code>paired</code></li> <li>Platform: <code>ILLUMINA</code></li> <li>Instrument model: <code>Illumina NovaSeq 6000</code></li> <li>Design description: <code>NA</code></li> <li>Filetype: <code>fastq</code></li> <li>Filename: the file name of the forward reads</li> <li>Filename2: the file name of the reverse reads</li> </ul>"},{"location":"Utilities/sra-data-submission/#transcriptomics-data","title":"Transcriptomics data \ud83d\udc68\ud83d\udc2d","text":"<p>Host transcriptomics samples will be registered as either HUMAN or Model organism or animal samples. On the BioSample Attributes tab, download the BioSample metadata Excel template, and complete it accordingly before uploading. Be very careful with the required field formats. You can double check ontology using the EMBL-EBI Ontology Lookup Service.</p> <ul> <li>Use the BioProject accession number previously generated</li> <li>Organism: <code>Homo sapiens</code> (or <code>Mus musculus</code>/<code>Rattus norvegicus</code> as appropriate)</li> <li>Isolate: NA</li> <li>Age: fill this in, but leave <code>NA</code> for human samples if it would result in a unique combination of metadata variables with potential to allow identification of any individual.</li> <li>Biomaterial provider: enter the lab, organisation etc. that provided the samples</li> <li>Collection date: do not enter any exact dates for human samples</li> <li>Geo loc name: country in which samples were collected</li> <li>Sex: provide sex of host</li> <li>Tissue: specify tissue origin of samples</li> <li>Add any other relevant data, such as sampling group</li> </ul> <p>As above, the SRA Metadata tab is where the magic will happen :magic_wand:. Once again, download the provided Excel template, and fill everything in carefully.</p> <ul> <li>Sample name: the base name of your samples</li> <li>Library ID: you may have named your files differently than your sample names \u2013 provide this if so, otherwise you can repeat the sample name</li> <li>Title: a short description of the sample in the form \"<code>{methodology}</code> of <code>{organism}</code>: <code>{sample_info}</code>\" \u2013 e.g. \"RNA-Seq of Homo sapiens: childhood bronchial brushing\".</li> <li>Library strategy: <code>RNA-Seq</code></li> <li>Library source: <code>TRANSCRIPTOMIC</code></li> <li>Library selection: <code>RANDOM</code></li> <li>Library layout: <code>paired</code></li> <li>Platform: <code>ILLUMINA</code></li> <li>Instrument model: <code>Illumina NovaSeq 6000</code></li> <li>Design description: <code>NA</code></li> <li>Filetype: <code>fastq</code></li> <li>Filename: the file name of the forward reads</li> <li>Filename2: the file name of the reverse reads</li> </ul>"},{"location":"Utilities/sra-data-submission/#submit-data-to-sra","title":"Submit data to SRA \ud83d\udce4","text":"<p>Which upload option should I choose?</p> <p>You can choose either of the following upload options, and each has pros and cons.</p> <ul> <li>Aspera Connect (at least with NCBI) only allows sequential uploads, but the upload speed is significantly faster.</li> <li>Filezilla allows parallel uploads according to your settings, but upload speed is typically slower.</li> </ul> Aspera ConnectFileZilla \ud83e\udd96 <p>The IBM Aspera Connect tool allows for much faster uploads than FileZilla, and is a good alternative for large files.</p> <p>Linux process \ud83d\udc27</p> <p>The process described here is for Linux, but is similar for Windows and MacOS operating systems. More information is provided on the IBM website.</p> <ul> <li>Download the Aspera Connect software.</li> <li>Open a new terminal window (<code>Ctrl+Alt+T</code>)</li> <li>Navigate to downloads, extract the <code>tar.gz</code> file.</li> <li>Run the install script.</li> </ul> <pre><code># Extract the file\ntar -zxvf ibm-aspera-connect-version+platform.tar.gz\n# Run the install script\n./ibm-aspera-connect-version+platform.sh\n</code></pre> <ul> <li>Add the Aspera Connect bin folder to your PATH variable (reopen terminal to apply changes).</li> </ul> <pre><code># Add folder to PATH\necho 'export PATH=$PATH:/home/{user}/.aspera/connect/bin/ &gt;&gt; ~/.bashrc'\n</code></pre> <ul> <li>Download the NCBI Aspera Connect key file.</li> <li>Navigate to the parent folder of the folder containing the files you want to upload to the SRA database, and create a new bash script.</li> </ul> <pre><code># Create a new bash script file\ntouch upload_seq_data.sh\n</code></pre> <ul> <li>Add the following code to the bash script file. <ul> <li>The <code>-i</code> argument is the path to the key file, and must be given as a full path (not a relative one).</li> <li>The <code>-d</code> argument specifies that the directory will be created if it doesn't exist.</li> <li>You can adjust the maximum upload speed using the <code>-l500m</code> argument, where <code>500</code> is the speed in Mbps. You could increase or decrease as desired.</li> <li>Add the folder containing the data to upload, which can be relative to the folder containing the bash script.</li> <li>Next provide the upload folder provided by NCBI, which will be user-specific, and ensure you provide a project folder at the end of this. Data will not be available if it is uploaded into the main uploads folder.</li> </ul> </li> </ul> <p>Seriously... remember to specify a new folder \ud83d\udc40</p> <p>If you don't specify a folder (<code>{name-of-project}</code>) after the main directory path, your files will be uploaded to the main user directory. Your files will then not be available for any SRA submission, and you will have to re-upload everything.</p> <p><code>subasp@upload.ncbi.nlm.nih.gov:uploads/{user-specific-ID}/{name-of-project}</code></p> upload_seq_data.sh<pre><code>#!/bin/bash\nascp -i {/full/path/to/key-file/aspera.openssh} -QT -l500m -k1 -d {./name-of-seq-data-folder} subasp@upload.ncbi.nlm.nih.gov:uploads/{user-specific-ID}/{name-of-project}\n</code></pre> <ul> <li>Run the bash script, and upload all files. The default settings will allow you to resume uploads if they are interrupted, and it will not overwrite files that are identical in the destination folder.</li> </ul> <pre><code># Run script\nbash upload_seq_data.sh\n</code></pre> <p>Using FileZilla is more effective when you have large files and/or a large number of files.</p> <p>In FileZilla, open the sites manager and connect to NCBI as follows: - Protocol: <code>FTP</code> - Host: <code>ftp-private.ncbi.nlm.nih.gov</code> - Username: <code>subftp</code> - Password: this is your user-specific NCBI password given when you submit your data</p> <p>In the <code>Advanced</code> tab next to the <code>General</code> tab, set the <code>Default remote directory</code> field to the directory specified by NCBI. This will looks something like: <code>/uploads/{username}_{uniqueID}</code>.</p> <p>Select connect, and gain access to your account folder on the NCBI FTP server.</p> <p>Create a new project folder within the main upload folder, and enter the folder. Add your files to the upload queue, and begin the upload process.</p>"},{"location":"Utilities/vault-storage/","title":"Vault Storage","text":""},{"location":"Utilities/vault-storage/#overview","title":"Overview","text":"<p>Vault storage is a tape-based solution ideal for long-term data retention and redundancy. It is suited for infrequently accessed data and can serve as a backup for locally stored files. Vault is accessible via desktop shares/network drives (SMB), server mounts (NFS), or protocols like SFTP and rsync. Access is managed via user groups for SMB or by network address for NFS. See the Vault Storage User Guide for more information.</p>"},{"location":"Utilities/vault-storage/#tape-storage","title":"Tape storage \ud83d\udcfc","text":"<p>Vault combines disk and tape storage for efficiency. The disk acts as a fast cache for recently written data, while the tape stores older, less accessed files. By default, files not accessed for seven days are migrated to tape, though policies can be adjusted. Taped files remain visible in directories and are automatically recalled when accessed, taking five or more minutes depending on file size and quantity.</p> <p>Requesting storage</p> <p>There is a significant amount of Vault capacity available and allocations are assigned individual quotas. Vault storage can be requested via the Data Dashboard.</p> <p>Recall of tape files</p> <p>Tape recalls can be slow, particularly for many small, scattered files, due to the process of locating tapes, loading them, and accessing specific data. Users are encouraged to organize data into structured folders and bundle small files into archives (e.g., using ZIP, TAR, or SquashFS) to enable quicker retrieval. For urgent access to large datasets, bulk recalls can be requested but are uncommon and assessed on a case-by-case basis.</p>"},{"location":"Utilities/vault-storage/#accessing-the-vault","title":"Accessing the Vault \ud83c\udfe6","text":"Filezilla \ud83e\udd96Rsync \ud83d\udd04 <p>Monash VPN</p> <p>Ensure that you have connected to the Monash VPN if you are off-site and not connected to the Monash network.</p> <p>The easiest way to access your Vault storage is via Filezilla. Once you have downloaded and installed the software, you can set up a new site.</p> <ul> <li>Click the <code>Site Manager</code> icon at the top-left of the screen.</li> </ul> <p></p> <ul> <li>Using the <code>SFTP</code> protocol, connect to <code>vault-v2.erc.monash.edu</code> using <code>Normal</code> logon type and your Monash credentials.</li> </ul> <p>Vault access \ud83d\udd10</p> <p>Users need to make sure they have requested and been granted Vault access first \u2013 Monash credentials alone are not sufficient.</p> <p></p> <ul> <li>Click <code>Connect</code>. You will then see the Vault file explorer show up in the right-hand side of the screen.</li> </ul> <p></p> <ul> <li>You can then navigate to the files you need to download and add them to your FileZilla transfer queue. The process works the same for upload.</li> </ul> <p>Monash VPN</p> <p>Ensure that you have connected to the Monash VPN if you are off-site and not connected to the Monash network.</p> <p>You can also transfer files and folders using <code>rsync</code> \u2013 this is commonly preinstalled on Unix-like operating systems.</p> <ul> <li>You can check for a install by running <code>rsync --help</code>. If you get the help menu showing up, you're good to go.</li> </ul> I don't have it installed... where do I get Rsync from? MacOS \ud83c\udf4eLinux \ud83d\udc27 <ul> <li>Ensure you have Homebrew installed (this requires root access to install).</li> <li>Install using the following Homebrew command:</li> </ul> Install rsync<pre><code># Install using Homebrew\nbrew install rsync\n</code></pre> <ul> <li>It would be quite strange if <code>rsync</code> is not pre-installed, but you can easily install it on Debian-based platforms (like Ubuntu) using the following command (this requires root access):</li> </ul> Install rsync<pre><code># For Debian-based Linux distributions\nsudo apt-get install rsync\n</code></pre> <p>Transferring data \ud83d\ude9b</p> <p>Navigate to an appropriate project folder on the cluster. An example command is shown below for transferring the data folder into a new folder called <code>raw_data</code> using <code>rsync</code>. If it doesn't exist, the folder you name will be created for you (just make sure you put a <code>/</code> after the new folder name).</p> <pre><code>rsync -aHWv --stats --progress MONASH\\\\mmac0026@vault-v2.erc.monash.edu:Marsland-CCS-RAW-Sequencing-Archive/vault/03_NovaSeq/NovaSeq25_Olaf_Shotgun/231025_A00611_0223_AHGMNNDRX2/ raw_data/\n</code></pre> <p>You can upload files back to the Vault by reversing the ordering of the directories so that the local folder comes first.</p>"}]}