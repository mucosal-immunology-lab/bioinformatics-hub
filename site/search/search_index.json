{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Mucosal Immunology Lab Bioinformatics Hub","text":""},{"location":"#overview","title":"Overview","text":"<p>Over the years, we have refined several workflows for processing of the various omic modalities we utilise within our group. As with any workflows in the bioinformatics field, these are constantly evolving as new tools and best practices emerge. As such, this hub is very much a work-in-progress, and will remain so as we continue to add to and update it.</p>"},{"location":"#contributors","title":"Contributors","text":"<p>These sort of tasks are never accomplished alone! Massive thanks to the people who have contributed to this.</p> <ul> <li> <p> Matthew Macowan</p> <p>Bioinformatician \u2013 Mucosal Immunology Lab</p> </li> <li> <p> C\u00e9line Pattaroni</p> <p>Group Leader \u2013 Computational Immunology Group</p> </li> <li> <p> Giulia Iacono</p> <p>Post Doc \u2013 Mucosal Immunology Lab</p> </li> </ul> <ul> <li>Alana Butler: Bioinformatician</li> <li>Bailey Cardwell: PhD student \u2013 Mucosal Immunology Lab</li> </ul>"},{"location":"#group-research-overview","title":"Group Research Overview","text":"<p>The Mucosal Immunology Research Group, led by Professors Marsland, Harris and Westall, is focused on understanding the fundamental principles in health and disease of the gut, lung and nervous system. Projects span the space from discovery using preclinical models of host-microbe interactions and inflammation through to high performance computational approaches to identify clinical biomarkers and development of novel drug candidates.</p>"},{"location":"#group-heads","title":"Group Heads","text":"<ul> <li> <p> Ben Marsland</p> </li> <li> <p> Nicola Harris</p> </li> <li> <p> Glen Westall</p> </li> </ul>"},{"location":"LCMS/lcms-analysis/","title":"LCMS analysis","text":"<p>Here we will provide a guide for the processing of raw LCMS output files from metabolomics and lipidomics runs, how to undertake curation and annotation, and approaches for their analysis downstream.</p>"},{"location":"LCMS/lcms-analysis/#overview","title":"Overview","text":"<p>The MS-DIAL software we recommend provides a pipeline for untargeted metabolomics. Its outputs then require thorough quality control measures and additional annotation steps. Additional annotation with HMDB and/or LIPID MAPS supplements the annotations provided through standards and MS-DIAL to ensure we can retain and utilise the maximum number of features.</p> <p>Analysis workflow</p> <ol> <li>Process raw LCMS data with MS-DIAL</li> <li>Perform automated quality control with pmp</li> <li>Secondary feature annotation with HMDB/LIPID MAPS</li> <li>Manual curation of annotated spectra</li> <li>Downstream analysis</li> </ol>"},{"location":"LCMS/manual-peak-curation/","title":"Manual curation of peaks","text":"<p>Manual peak curation is an essential part of the preprocessing of LMCS datasets, particularly for ensuring that only high quality spectra are retained for downstream analysis and interpretation. Automated tools like <code>pmp</code> are a great start, and remove a lot of the noise (which is a big help in reducing the number of peaks we actually have to check), but they are not foolproof and many poor quality peaks will inevitably remain. Manual curation is therefore critical, and both refines the dataset and strengthens confidence in metabolite and lipid identification, which in turn supports more robust biological conclusions.</p> <p>Cutting through the clutter...</p> <p>You will tend to find that about 30-50% of peaks are poor quality and require removal \u2013 this is of course dataset dependent.</p> <ul> <li>Poor signal-to-noise ratio is a common issue, resulting in \"messy\" and noisy spectra.</li> <li>Peaks only present in a few samples should also be removed to avoid over-fitted data imputation &amp; ndash; it is fine to keep peaks that show up in just a single biological sample group however, because perhaps it is only abundant in this setting.</li> </ul>"},{"location":"LCMS/manual-peak-curation/#prepare-xml-files-for-ms-dial","title":"Prepare XML files for MS-DIAL \ud83d\udcc4\ud83d\udee0\ufe0f","text":"<p>To speed up the process, we can add a <code>1</code> tag to each of the remaining annotated features in our <code>SummarizedExperiment</code> object. In MS-DIAL, a <code>1</code> tag simply refers to a confirmed metabolite, but for our purposes, we simply need a way to filter the tens of thousands of peaks down to just the ones we need to check.</p> <p>MS-DIAL actually creates a file already, and so we can just update the two <code>.xml</code> files for the positive and negative ionisation modes.</p> <p>Where are these files?</p> <p>These files are located in the main MS-DIAL project folder where you processed the raw LCMS data.</p> <p>If you followed the recommendation to choose a custom project name in MS-DIAL, then this will be a lot easier </p> <ul> <li>For example, if your named your project files:<ul> <li><code>metab_stool_pos.mdproject</code></li> <li><code>metab_stool_neg.mdproject</code></li> </ul> </li> <li>Then these files will be:<ul> <li><code>metab_stool_pos_tags.xml</code></li> <li><code>metab_stool_neg_tags.xml</code></li> </ul> </li> </ul>"},{"location":"LCMS/manual-peak-curation/#generate-a-curation-table","title":"Generate a curation table \ud83d\udccb","text":"<p>To begin, we will generate a curation table that will identify which features we will need to manually check. The <code>make_curation_table()</code> function will do this for us.</p> Parameters for <code>make_curation_table()</code> Parameter Description <code>metab_SE</code> A <code>SummarizedExperiment</code> object with secondary annotations, which has been filtered for just annotated features. Generate curation table<pre><code># Make the curation table\ncuration_table &lt;- make_curation_table(metab_SE = metab_stool_glog)\n\n# Save curation table\ncuration_dir &lt;- here('input', '01_LCMS', 'manual_curation')\nsaveRDS(curation_table, here(curation_dir, 'curation_table.rds'))\n</code></pre> <p>Save your curation table</p> <p>Ensure you save the curation table, because you will need to it read back in the manual curation results from the MS-DIAL XML tag files.</p>"},{"location":"LCMS/manual-peak-curation/#prepare-the-relevant-xml-files-for-ms-dial","title":"Prepare the relevant XML files for MS-DIAL \ud83d\uddc2\ufe0f\u2699\ufe0f","text":"<p>MS-DIAL uses the <code>tag</code> files for identifying which features have certain available peak spot characteristics.</p> What peak spot tags does MS-DIAL use? <p>There are 5 different \"Peak Spot Tags\" in MS-DIAL. We are just co-opting the <code>Confirmed</code> tag as a marker for manual curation. The current tags for peaks are:</p> <ol> <li>Confirmed</li> <li>Low quality spectrum</li> <li>Misannotation</li> <li>Coelution (mixed spectra)</li> <li>Overannotation</li> </ol> <p>We will use tag <code>2</code> to mark poor quality peaks as we curate.</p> <p>We will use the <code>create_msdial_xml()</code> function to automate the process of tagging peaks found in the curation table to the relevant ionisation mode tag file.</p> Parameters for <code>create_msdial_xml()</code> Parameter Description <code>curation_table</code> The <code>data.frame</code> curation table output from the <code>make_curation_table()</code> function. <code>path_to_folder</code> The file path to the main MS-DIAL project folder (i.e. where your tag files are). <code>experiment_name</code> The base file name of your tag files. E.g. <code>'metab_stool'</code> for tag files named <code>metab_stool_pos_tags.xml</code> and <code>metab_stool_neg_tags.xml</code>. <code>peak_id_column</code> Default: <code>1</code> \u2013 the number of the column containing the annotation IDs. <code>ionisation</code> Default: <code>c('pos', 'neg')</code> \u2013 the ionisation markers at the end of the tag files. <code>force_proceed</code> Default: <code>FALSE</code> \u2013 an option for whether to force the override of existing files. This is not recommended. Remove the existing update to the tags file, and rename the <code>_BACKUP.xml</code> file to its original name. Then rerun \u2013 this is a safer option. Update the XML tag files<pre><code># Prepare the relevant XML files for MS-DIAL\ncreate_msdial_xml(curation_table = curation_table,\n                  path_to_folder = here('input', '01_LCMS', 'Raw'),\n                  experiment_name = 'metab_stool',\n                  peak_id_column = 1,\n                  ionisation = c('pos', 'neg'))\n</code></pre>"},{"location":"LCMS/manual-peak-curation/#peak-curation-in-ms-dial","title":"Peak curation in MS-DIAL \ud83d\udcc8\ud83d\udd0d","text":"<p>Now that the tag files have been update to indicate which peaks we need to manually curate, we can return to MS-DIAL and check each of the peaks.</p> <p>As you can see below, some of the tags are now highlighted in the peak spot table, which is just what we wanted!</p> <p>Note</p> <p>You should not see anything highlighted in the \"Low quality spectrum\" column at this stage. These images were taken after manual peak checking.</p> <p></p> <p>You can then select the \u2611\ufe0f option next to <code>Tag filter</code> at the top left of the peak spot table to filter for just the peaks requiring curation.</p> <p></p> <p>At this point you can simply work your way through each peak to determine whether it should be retained for downstream analysis.  Mark peaks as having low quality by pressing <code>Ctrl + 2</code> or by clicking the <code>L</code> button.</p>"},{"location":"LCMS/manual-peak-curation/#identifying-good-quality-peaks","title":"Identifying good quality peaks \u2b50\ud83d\udd0e","text":"<p>How do I know what to keep?</p> <p>The simple question you should ask yourself is: \"Would I call this a peak?\".</p> <p>If not, then mark it as low quality, and continue with the next spectrum. This task is not something to labour over \u2013 you will get the hang of things quickly as you see more examples and learn how different spectra look.</p> <p>To help you out, some examples of spectra are shown below.</p> <p></p> <p>Remember too to open the <code>Peak curation (sample table)</code> of the aligned spots. This will give you a much better idea about how your peaks look in individual samples, and can often provide a clearer picture. For example, the left-most \"good\" peak above has one sample with far greater intensity than the other which may skew our impression. The peak curation sample table however would allow us to see that peaks look good in all samples, and importantly that it is also present in the majority of samples (if not all).</p> <p></p>"},{"location":"LCMS/manual-peak-curation/#filter-the-summarizedexperiment-object-for-the-best-features","title":"Filter the <code>SummarizedExperiment</code> object for the best features \u2697\ufe0f","text":"<p>At this point, all of the peaks have been manually checked in both the positive and negative ionisation modes and it is time to read the results back into R. Then we can filter for the best feature for each named metabolite/lipid. You can</p>"},{"location":"LCMS/manual-peak-curation/#read-in-the-manual-curation-results-from-ms-dial","title":"Read in the manual curation results from MS-DIAL \ud83d\udce5\ud83d\udccb","text":"<p>We can now use the <code>read_msdial_xml()</code> function to load in the tag files from MS-DIAL. Recall that we need the curation table we prepared earlier too, so this also needs to be retrieved.</p> Parameters for <code>read_msdial_xml()</code> Parameter Description <code>curation_table</code> The <code>data.frame</code> curation table output from the <code>make_curation_table()</code> function. <code>path_to_folder</code> The file path to the main MS-DIAL project folder (i.e. where your tag files are). <code>experiment_name</code> The base file name of your tag files. E.g. <code>'metab_stool'</code> for tag files named <code>metab_stool_pos_tags.xml</code> and <code>metab_stool_neg_tags.xml</code>. <code>ionisation</code> Default: <code>c('pos', 'neg')</code> \u2013 the ionisation markers at the end of the tag files. <p>Manually check feature names</p> <p>This is entirely up to you, but at this point, you may also wish to double check that your have the optimal <code>shortname</code> values. You can simply add an additional column to a <code>.csv</code> export of your curation table, and adjust anything as required. This can be useful if you identify features that are the same, but have slightly different names or capitalisation etc.</p> Filter out low quality peaks<pre><code># Recover the SummarizedExperiment and curation table\nmetab_stool_glog &lt;- readRDS(here('output', '01_Preprocessing', 'metab_stool_glog_anno.rds'))\ncuration_dir &lt;- here('input', '01_LCMS', 'manual_curation')\ncuration_table &lt;- readRDS(here(curation_dir, 'curation_table.rds'))\n\n# Read in the XML tag files from MS-DIAL\nmanual_curation_xml &lt;- read_msdial_xml(curation_table = curation_table,\n                                       path_to_folder = here('input', '01_LCMS', 'Raw',\n                                       experiment_name = 'metab_stool',\n                                       ionisation = c('pos', 'neg')))\n\n# Filter out the low quality peaks\nmetab_stool_glog &lt;- metab_stool_glog[!manual_curation_xml$Anno2,]\n\n# Save intermediate object\nsaveRDS(metab_stool_glog, here('output', '01_Preprocessing', 'metab_stool_glog_tmp_curated.rds'))\n</code></pre> <p>Why do we filter by <code>Anno2</code>?</p> <p>Each of the tags is assigned to an annotation number. Here, <code>Anno2</code> refers to tag <code>2</code> which, as we saw above, represents low quality peaks. <code>Anno1</code> on the other hand would represent confirmed hits (i.e. the ones we were manually checking).</p>"},{"location":"LCMS/manual-peak-curation/#selecting-the-best-features","title":"Selecting the best features \ud83c\udfaf\u2728","text":"<p>Now we will select for the best features from both the positive and negative ionisation modes so we are left with a single representation of each metabolite/lipid. We find this to be preferable to avoid cases where there are multiple features with the same name and pattern \u2013 this is particularly noticeable on heatmaps when you can see bands of features with identical patterns.</p> <p>To identify the best feature, we will select the highest value from the multiple of <code>Fill %</code> and <code>S/N ratio</code>.</p> Best feature selection<pre><code># Recover metabolomics data\nmetab_stool_glog &lt;- readRDS(here('output', '01_Preprocessing', 'metab_stool_glog_tmp_curated.rds'))\n\n# Create a data.frame with the necessary feature metadata to identify best features\nelement_meta &lt;- data.frame(metab_stool_glog@elementMetadata) %&gt;%\n    mutate(original_order = 1:nrow(metab_stool_glog@elementMetadata)) %&gt;%\n    dplyr::select(original_order, info.Alignment.ID, ionisation,\n                  info.Fill.., info.S.N.average, shortname) %&gt;%\n    arrange(shortname = gsub('_pos.*|_neg.*', '', shortname)) # remove ionisation\n\n# Filter for the highest multiple of 'Fill %' * 'Signal-to-noise ratio'\nelement_meta_top &lt;- element_meta %&gt;%\n    mutate(metab_score = info.Fill.. * info.S.N.average) %&gt;%\n    arrange(desc(metab_score)) %&gt;%\n    group_by(shortname) %&gt;%\n    slice_head(n = 1) %&gt;%\n    ungroup()\n\n# Create a vector of \"best\" features to keep\nfeature_keep &lt;- element_meta %&gt;%\n    left_join(element_meta_top %&gt;% dplyr::select(original_order, metab_score),\n              by = 'original_order') %&gt;%\n    mutate(keep = ifelse(is.na(metab_score), FALSE, TRUE)) %&gt;%\n    arrange(original_order) %&gt;%\n    pull(keep)\n\n# Filter the SummarizedExperiment object\nmetab_stool_glog &lt;- metab_stool_glog[feature_keep,]\n\n# Remove ionisation from the shortname values\nmetab_stool_glog@elementMetadata$shortname &lt;-\n    gsub('_pos.*|_neg.*', '', metab_stool_glog@elementMetadata$shortname)\n\n# Remove the QC samples\nmetab_stool_glog &lt;- metab_stool_glog[, !str_detect(colnames(metab_stool_glog), 'QC')]\n\n# Save to object\nsaveRDS(metab_stool_glog, here('output', '01_Preprocessing', 'metab_stool_glog_curated.rds'))\n</code></pre> <p>Why would we use Fill% and S/N ratio to select the best features?</p> <ul> <li>Fill %: this refers to the percentage of samples in which a given feature (peak) is detected across the entire dataset. It provides an indication of the feature's prevalence or coverage among the samples.<ul> <li>High Fill % indicates that the feature is detected in most or all samples, suggesting it may be a consistently present metabolite or compound.</li> <li>Low Fill % indicates that the feature is detected in only a few samples, which could imply it is rare, a contaminant, or specific to certain conditions or samples.</li> </ul> </li> <li>S/N ratio: signal-to-noise ratio represents the relative strength of a detected signal compared to the background noise level. It is calculated as the ratio of the intensity of the analyte's signal to the baseline noise level.<ul> <li>High S/N ratio indicates a strong, reliable signal that stands out clearly from the background noise. Typically considered robust and accurate.</li> <li>Low S/N ratio suggests a weak signal relative to noise, which may be less reliable and could represent a borderline or questionable detection.</li> </ul> </li> </ul>"},{"location":"LCMS/manual-peak-curation/#next-steps","title":"Next steps \u27a1\ufe0f","text":"<p>Congratulations! The pre-processing steps are complete, and you now have a prepared <code>SummarizedExperiment</code> that has undergone quality control, secondary annotation, and manual curation.</p> <p>From here, you can move onto downstream analysis of the data.</p>"},{"location":"LCMS/manual-peak-curation/#rights","title":"Rights","text":"<ul> <li>Copyright \u00a9\ufe0f 2024 Respiratory Immunology lab, Monash University, Melbourne, Australia.</li> <li>HMDB version 5.0: Wishart DS, Guo A, Oler E, Wang F, Anjum A, Peters H, Dizon R, Sayeeda Z, Tian S, Lee BL, Berjanskii M. HMDB 5.0: the human metabolome database for 2022. Nucleic acids research. 2022 Jan 7;50(D1):D622-31.</li> <li>License: This pipeline is provided under the MIT license.</li> <li>Authors: M. Macowan, B. Cardwell, and C. Pattaroni</li> </ul>"},{"location":"LCMS/msdial-processing/","title":"Processing metabolome and lipidome data with MS-DIAL","text":"<p>MS-DIAL provides a pipeline for untargeted metabolomics. Here we will discuss the process for generating intensity height tables from raw LC-MS data, and subsequent export for downstream analysis and secondary annotation methods.</p>"},{"location":"LCMS/msdial-processing/#requirements","title":"Requirements","text":"<p>Conversion of .RAW files</p> <p>As of MS-DIAL version 4.70, it is no longer a requirement to convert your .RAW LC-MS files into .ABF format, however if you are using an older version of MS-DIAL, you will need to download the ABF file converter.</p> <ul> <li>ABF File Converter (depending on MS-DIAL version).</li> <li>MS-DIAL software for Windows.</li> <li>MassBank MS/MS positive and negative database files.</li> <li>The LC-MS files all present in a single folder.</li> <li>The LC-MS standards file generated during data collection (should contain columns for: metabolite name, m/z, and retention time).</li> </ul>"},{"location":"LCMS/msdial-processing/#setting-up-a-new-project","title":"Setting up a new project \ud83c\udfd7\ufe0f","text":"<p>Note</p> <p>The instructions given here are for MS-DIAL version 5.</p> <p>Begin by opening MS-DIAL and starting a new project using the icon in the centre of the screen.</p> <p>MS-DIAL will prompt you to enter a <code>Project title</code> and a <code>Project file path</code>. Please remember that the project file path must contain all of your raw LCMS files \u2013 you can navigate to this folder using the <code>'Browse'</code> button.</p> <p>Change the default project name</p> <p>It is recommended that you change the default <code>.mdproject</code> file name generated by MS-DIAL to something more easily recognisable in the future, such as <code>date_sampletype_ionisationmode.mtd</code>.</p> <p></p>"},{"location":"LCMS/msdial-processing/#raw-measurement-files","title":"Raw measurement files \ud83d\uddc2\ufe0f","text":"<p>The next screen will ask you to define your <code>'Analysis file paths'</code>. Click the browse button, and select all of your LC-MS files. Note that these must be located in the same folder as your project in order to be valid.</p> <p>Once you have done this, you will need to change the <code>'Type'</code> column to indicate whether that sample is a sample, blank, QC, or standard. It is also recommended, for quick analysis later on, to change the values in the <code>'Class ID'</code> column.</p> <p>For example, in the <code>'Class ID'</code> column below, it is indicated what the sample type is (if you have multiple groups, you can specify these here), and of note, while <code>MS2</code> samples are given a <code>'Type'</code> of <code>QC</code>, you can specify that they are their own class in this column.</p> <p>If you were provided with information about the order in which your samples were actually run, you can input this in the <code>Analytical order</code> column. This will allow retention time correction. However, as the majority of our annotations will be provided by the Human Metabolome Database (HMDB), which does not take retention time into account, this step will be less important downstream. One benefit however will be for the alignment of peaks in the event you have significant retention time drift over the course of data acquisition.</p> <p>Once you have finished, click <code>'Next'</code> to continue.</p> <p></p>"},{"location":"LCMS/msdial-processing/#measurement-parameters","title":"Measurement parameters \ud83d\udcd0","text":"<p>The next screen will ask you to select various parameters regarding how your data was acquired.</p> <p>Firstly, you can define a project name. It is recommended that you change the default <code>.mddata</code> file name generated by MS-DIAL to something more easily recognisable in the future, such as <code>date_sampletype_ionisationmode.mddata</code>.</p> <p>File name suffix</p> <p>No matter how you decide to name your files, ensure that you use the same name for both positive and negative ionisations, and end each with <code>_pos</code> and <code>_neg</code> respectively. If you don't, you won't be able to make easy use of the XML curation function to speed up your manual curation later on.</p> <ul> <li>If you use <code>_positive</code> and <code>_negative</code> like the example below, you'll also be fine.</li> <li>The key is the make sure the file base name is identical.</li> </ul> <p>For our data, most of the default options will be appropriate. </p> <ul> <li>Data type for both <code>MS1</code> and <code>MS2</code> should be set to <code>Profile data</code> for our data, acquired using a Thermo Fisher Scientific LCMS apparatus.</li> <li>Ensure you choose the correct <code>'Ion mode'</code> at the bottom left, either positive or negative ionisation. and that you select the correct <code>'Target omics'</code> from either metabolomics or lipidomics.</li> </ul> <p>Click <code>Next</code> once complete.</p> <p></p>"},{"location":"LCMS/msdial-processing/#analysis-parameter-settings","title":"Analysis parameter settings \ud83d\udd27","text":"<p>Each tab below covers the settings for a tab on MS-DIAL. Input the correct settings, and then proceed to running your samples.</p> Data collection \ud83d\udccaPeak detection \ud83d\udcc8\ud83d\udd0dSpectrum deconvolution \ud83c\udf08\ud83d\udd2cIdentification \ud83c\udd94\ud83d\udd0eAdduct ion \u2795\u269b\ufe0fAlignment parameters \ud83d\udd17\ud83d\udccf <p>Load parameters</p> <p>If you have a parameter configuration file, you can load it in via the <code>Load parameter</code> button in the bottom-left of the window.</p> <p>We begin the setting of analysis parameters by inputting the data collection parameters.</p> <p>Mass Accuracy:</p> Metabolomics Lipidomics MS1 tolerance 0.002 Da 0.002 Da MS2 tolerance 0.002 Da 0.002 Da <p>Data collection parameters:</p> Metabolomics Lipidomics Retention time begin 0 min 0 min Retention time end 40 min 40 min MS1 mass range begin 50 Da 200 Da MS1 mass range end 1000 Da 1300 Da MS/MS mass range begin 50 Da 200 Da MS/MS mass range end 1000 Da 1300 Da Execute retention time correction FALSE <p>Retention time correction option</p> <p>You can choose here to perform retention time correction. This should typically be set to <code>FALSE</code> for at least your initial processing, and always set to <code>FALSE</code> if you do not have information about the analytical order of your samples.</p> <p>Isotope recognition:</p> Metabolomics Lipidomics Maximum charged number 2 2 Consider Cl and Br elements FALSE FALSE <p>Multithreading (will depend on your machine):</p> Metabolomics Lipidomics Number of threads 8 8 Execute retention time corrections FALSE FALSE <p></p> <p>Next we select the minimum peak height threshold. Peaks below this threshold will not be retained. A value of <code>100,000</code> is recommended for data acquired by Thermo Fisher Scientific machines. However, this will vary by apparatus, and may require data-dependant tuning.</p> <p>We will leave the <code>'Mass slice width'</code> value to the default, along with all options in the drop-down <code>'Advanced'</code> menu.</p> Metabolomics Lipidomics Minimum peak height 100000 100000 Mass slice width 0.05 Da 0.05 Da <p></p> <p>The default values are suitable.</p> Metabolomics Lipidomics Sigma window value 0.5 0.5 MS/MS abundance cut off 0 0 Exclude after precursor ion TRUE TRUE Keep the isotopic ions until 5 Da 5 Da Keep the isotopic ions w/o MS2Dec FALSE FALSE <p></p> <p>In this tab, you will add your databases as appropriate. One of these will be the appropriate <code>.msp</code> MassBank database file (either positive or negative depending on the ionisation mode you are currently running).</p> <p>Click the green tick next the <code>Database setting</code> heading. In the <code>DataBase</code> box, select <code>Msp</code> as your database type and then navigate and select the appropriate MassBank file using the <code>Browse</code> button. The <code>Database name</code> and other values will be automatically populated.</p> <p>We will now adjust a number of parameters (the lipidomics columns are left blank below as you will not need to input or change anything).</p> <p>Retention time tolerance</p> <p>Keep in mind that the <code>Retention time tolerance</code> parameter can be adjusted in a project-specific manner. Using a value of about 2 minutes has worked well in the past, but may require fine-tuning. Setting the value lower will result in a greater number of highly similar features, and may result in issues downstream during manual curation steps.</p> <p>MS/MS identification setting:</p> Metabolomics Lipidomics Accurate mass tolerance (MS1) 0.002 Da Accurate mass tolerance (MS2) 0.002 Da Retention time tolerance 2 min <p>We will leave the other drop-down menus with their default settings.</p> <p></p> <p>In addition, you can select a text file containing the standards run during data acquisition. This file should contain three columns, including the metabolite name, m/z, and retention time (in that order). These columns should be named <code>Metabolite</code>, <code>MZ</code>, and <code>RT</code>.</p> <p>Click the green tick again to add another database. This time, choose <code>Text</code> as your database type. Navigate and select your standards file, and the other fields will be populated.</p> <p>Once again, we will change a few parameters:</p> <p>Tolerance:</p> Metabolomics Lipidomics Accurate mass tolerance (MS1) 0.002 Da Retention time tolerance 2 min <p></p> <p>Here we will select the appropriate adduct ion settings for our runs.</p> MetabolomicsLipidomics Metabolomics Positive Metabolomics Negative [M+H]+ [M-H]- [M+NH4]+ [M+Na-2H]- [M+Na]+ [M+Cl]- Lipidomics Positive Lipidomics Negative [M+H]+ [M-H]- [M+NH4]+ <p></p> <p>Here, you should rename the <code>'Result name:'</code> to something more recognisable. For example, <code>alignment_data_sampletype_ionisation</code>.</p> <p>Set the <code>'Reference file:'</code> to your second QC sample. The first QC sample is typically different than the others, and this will affect your results.</p> <p>The <code>'Retention time tolerance:'</code> parameter will be data-dependant, however a good start is to try either <code>2 min</code> or <code>1 min</code>.</p> Metabolomics Lipidomics Retention time tolerance 2 min 2 min MS1 tolerance 0.002 Da 0.002 Da <p>There are also some other parameters we need to check in the <code>'Advanced'</code> drop-down menu. Because we don't remove the features based on blank information, the following three tick boxes are 'greyed out', and not available to change. You do not need to change the default options for these.</p> <p>Gap filling</p> <p>We will use <code>Gap filling by compulsion</code>, as it improves peak alignment (often resulting in fewer highly similar features), but you may choose to skip this step. The choice will largely be data-dependent, and with a higher retention time tolerance, you may not see much of a difference.</p> Metabolomics Lipidomics Retention time factor 0.5 0.5 MS1 factor 0.5 0.5 Peak count filter 0 % 0 % N% detected in at least one group 0 % 0 % Remove features based on blank information FALSE FALSE Sample max / blank average 5 5 Keep 'reference matched' metabolite features FALSE FALSE Keep 'suggested (w/o MS2)' metabolite features FALSE FALSE Keep removable features and assign the tag FALSE FALSE Gap filling by compulsion TRUE TRUE <p></p>"},{"location":"LCMS/msdial-processing/#run-the-pipeline","title":"Run the pipeline \ud83c\udfc3\ud83d\udcbb","text":"<p>Once you have finished setting up the analysis parameters, click <code>'Finish'</code> to begin the run.</p> <p>When the run finishes, the data will appear on the screen. On the left-hand side of the screen, you will see an <code>'Alignment navigator'</code> box. Double click the file that is inside to load all of your data at once.</p>"},{"location":"LCMS/msdial-processing/#viewing-and-exporting-data","title":"Viewing and exporting data \ud83d\udc40\ud83d\udcbe","text":"<p>The <code>'Show ion table'</code> button in the middle of the screen is a good place to start to see how many metabolites/lipids (features) were found during the run. You can click the <code>'Metabolite name'</code> column to group all features that were annotated.</p>"},{"location":"LCMS/msdial-processing/#aligned-spot-information","title":"Aligned spot information \ud83d\udccd\ud83d\udcdd","text":"<p>If you want to view more information about a specific feature, you can single click on the row within the ion table list.</p> <p>At the top of the screen, the default image shown is the <code>'Bar chart of aligned spot (OH)'</code>. This will show you the average intensities of each of your <code>'Class IDs'</code>.</p> <p>To view the actual peak itself, you can click the adjacent <code>'EIC of aligned spot'</code> tab to view and assess the quality of the peak. Good quality peaks, especially for smaller weight features, are tight and well-aligned (see the image below).</p> <p>Next, if you want to see the individual peaks for each of your samples, you can right-click on the peak window and click the <code>Peak curation (Sample table)</code> option. This will show you the peak for that feature for each sample so you can further assess their quality. This will be necessary for manual curation of features later to confirm confidence in the annotation of significant findings.</p> <p>After downstream pre-processing, you will come back to MS-DIAL and manually curate each of the features, removing those with poor-quality spectra. It should be noted that without manual curation, your ordination plots (e.g. PCoA) will be affected by remaining poor quality features.</p> <p></p>"},{"location":"LCMS/msdial-processing/#exporting-for-downstream-processing-and-analysis","title":"Exporting for downstream processing and analysis \ud83d\udce4","text":"<p>There are a few useful things we can routinely export:</p> <ul> <li>The raw data height matrix (contains all of our intensity values and feature information).</li> <li>Analysis parameters (to streamline future analysis set-up).</li> </ul> <p>Export as CSV</p> <p>For consistency and easy import into R, best practice is to save the height matrix output in <code>.csv</code> format.</p>"},{"location":"LCMS/msdial-processing/#height-table-export","title":"Height table export \ud83d\udcca\ud83d\udcc4","text":"<p>To export the raw height matrix table, navigate to the <code>'Alignment result'</code> option within the <code>'Export'</code> tab in the top menu.</p> <p>Select the directory for export and leave all other options to their defaults. Change the <code>Export format</code> to <code>csv</code>, and click <code>'Export'</code>.</p> <p></p>"},{"location":"LCMS/msdial-processing/#parameter-export","title":"Parameter export \ud83d\udee0\ufe0f\ud83d\udce4","text":"<p>To export the analysis parameters, open the green tab at the top-left of the screen with the new project icon, select <code>Save</code>, and then select the <code>Save parameter option</code>.</p> <p></p>"},{"location":"LCMS/msdial-processing/#next-steps","title":"Next steps \u27a1\ufe0f","text":"<p>Now that you have your height matrices (positive and negative ionisation modes), you can proceed to automated quality control steps, using Peak Matrix Processing.</p>"},{"location":"LCMS/msdial-processing/#rights","title":"Rights","text":"<ul> <li>Copyright \u00a9\ufe0f 2024 Mucosal Immunology Lab, Monash University, Melbourne, Australia.</li> <li><code>pmp</code> package: Jankevics A, Lloyd GR, Weber RJM (2021). pmp: Peak Matrix Processing and signal batch correction for metabolomics datasets. R package version 1.4.0.</li> <li>License: This pipeline is provided under the MIT license.</li> <li>Authors: M. Macowan and C. Pattaroni</li> </ul>"},{"location":"LCMS/pmp-quality-control/","title":"Peak Matrix Processing for raw MS-DIAL height matrices","text":"<p>The raw height matrices output by MS-DIAL will typically contain thousands (if not tens of thousands) of so-called \"uninformative\" and \"irreproducible\" features. Features such as these could hinder downstream analyses, including statistical analysis, biomarker discovery, or pathway inference.</p> <p>Therefore the common practice is to apply peak matrix validation and filtering procedures as described in Di Guida et al.  (2016), Broadhurst et al. (2018), and Schiffman et al. (2019).</p> <p>Functions within the <code>pmp</code> (Peak Matrix Processing) package are designed to help users to prepare data for further statistical data analysis in a fast,  easy to use and reproducible manner.</p>"},{"location":"LCMS/pmp-quality-control/#preprocessing-script","title":"Preprocessing script \ud83d\udcdc","text":"<p>An R function to handle the various steps is available as <code>pmp_preprocess()</code>, and will be explained in-depth below.</p> Parameters for <code>pmp_preprocess</code> Parameter Description <code>pos_df</code> The positive ionisation intensity <code>data.frame</code>. <code>neg_df</code> The negative ionisation intensity <code>data.frame</code>. <code>metadata</code> The metadata <code>data.frame</code>. <code>samples_key</code> Default: <code>'Sample'</code> \u2013 a universal string that is unique to the start of all samples (and not blanks or QCs etc.). <code>intens_cols</code> A vector of column numbers corresponding to columns with intensity values. <code>info_cols</code> A vector of column numbers corresponding to the peak information. <code>blankFC</code> Default: <code>5</code> \u2013 the minimum fold change above the average intensity of blanks in order for peaks (and samples) to be retained. <code>max_perc_mv</code> Default: <code>0.8</code> \u2013 the maximum percentage of peaks that can be missing for a sample to be retained. <code>missingPeaksFraction</code> Default: <code>0.8</code> \u2013 the maximum percentage of samples that can contain a missing value for a peak to be retained. <code>max_rsd</code> Default: <code>25</code> \u2013 the maximum relative standard deviation of intensity values for a given peak within specified QC samples for that peak to be retained. <code>mv_imp_rowmax</code> Default: <code>0.7</code> \u2013 the maximum percentage of missing values allowable in a given row for imputation. <code>mv_imp_colmax</code> Default: <code>0.7</code> \u2013 the maximum percentage of missing values allowable in a given column for imputation. <code>mv_imp_method</code> Default: <code>knn</code> \u2013 the method to be used for imputation."},{"location":"LCMS/pmp-quality-control/#r-processing","title":"R processing \ud83e\uddee","text":""},{"location":"LCMS/pmp-quality-control/#environment-setup","title":"Environment setup \ud83d\udee0\ufe0f\ud83c\udf31","text":"<p>Firstly, we need to load the required packages. We will also display their versions for future reference.</p> Load the required packages<pre><code># Get the R version\nversion$version.string\n\n# Define a vector of required packages\npkgs &lt;- c('here', 'data.table', 'tidyverse', 'kableExtra', 'ggplot2', 'pmp', 'SummarizedExperiment', 'S4Vectors')\n\n# For each of the packages, load it and display the version number\nfor (pkg in pkgs) {\n  suppressPackageStartupMessages(library(pkg, character.only = TRUE))\n  print(paste0(pkg, ': ', packageVersion(pkg)))\n}\n\n# Set the seed for generation of pseudo-random numbers\nset.seed(2)\n</code></pre> <p>Example output:</p> <pre><code>[1] \"R version 4.0.5 (2021-03-31)\"\n[1] \"here: 1.0.1\"\n[1] \"data.table: 1.14.0\"\n[1] \"tidyverse: 1.3.1\"\n[1] \"kableExtra: 1.3.4\"\n[1] \"ggplot2: 3.3.5\"\n[1] \"pmp: 1.2.1\"\n[1] \"SummarizedExperiment: 1.20.0\"\n[1] \"S4Vectors: 0.28.1\"\n</code></pre>"},{"location":"LCMS/pmp-quality-control/#data-import","title":"Data import \ud83d\udce5","text":"<p>The data we will be importing is the height matrix data output from the MS-DIAL pipeline. This output will be imported into a <code>SummarizedExperiment</code> object for pre-processing using the <code>pmp</code> package.</p>"},{"location":"LCMS/pmp-quality-control/#metadata-optional-at-this-stage","title":"Metadata (optional at this stage) \ud83d\uddc2\ufe0f","text":"<p>Why should I include metadata now?</p> <p>While this step is optional at this stage, the benefit of supplying metadata to the <code>SummarizedExperiment</code> object is that your metadata will also be filtered by the pre-processing steps to match the remaining samples and features at the end.</p> <p>Firstly, we will import and prepare the metadata component of our <code>SummarizedExperiment</code> object. For simplicity and economy of code, we can specify the column types of our metadata using the <code>col_types</code> parameter in the <code>read_csv()</code> function. These will usually be one of:</p> <ul> <li><code>'c'</code> = character</li> <li><code>'f'</code> = factor</li> <li><code>'n'</code> = numeric</li> <li><code>'l'</code> = logical (i.e. boolean)</li> </ul> <p>In this example, we will be working with LCMS data from a set of stool samples that were collected from different patients.</p> <p>There are two metadata files: one that contains information about the samples themselves  (e.g. sample name and ID, sampling site, and age at collection), and one that contains information about the patient  (e.g. gender, disease type, other clinical metadata etc.). We will combine these two tables to produce a single metadata file for input into the <code>SummarizedExperiment</code> object.</p> Import clinical metadata<pre><code># Import the patient metadata (the 'here' function makes defining file paths much easier)\nmetadata_patient &lt;- read_csv(here::here('data', 'patient_metadata.csv'), col_types = 'ffffnnccfnc')\n\n# Import the metabolomics sample metadata\nmetadata_metab_stool &lt;- read_csv(here::here('data', 'metadata_metabolomics.csv'), col_types = 'ffffn') %&gt;%\n  filter(origin == 'stool') %&gt;% # filter for the stool metabolomics metadata alone (not from other sites)\n  left_join(metadata_patient, by = c('patient' = 'ID')) %&gt;% # match patient metadata onto the sample metadata rows using a matched column\n  column_to_rownames(var = 'sample') # choose the value that matches the MS-DIAL height matrix column names to be the rownames\n</code></pre>"},{"location":"LCMS/pmp-quality-control/#stool-metabolomics-data","title":"Stool metabolomics data \ud83d\udca9","text":"<p>We will load in both the positive and negative ionisation mode height matrices from the MS-DIAL pipeline. We need to remove the MS/MS columns, as these were acquired in a different manner to the other columns.</p> <p>Then, we can <code>rbind</code> the two tables together after denoting the original ionisation mode in the row names. Because the QC data is incorporated within each feature (row), <code>pmp</code> can pre-process all of our data at once, and normalise the entire dataset.</p> Load the peak height tables from MS-DIAL<pre><code># Load the metabolomics height data\nmetab_stool_pos &lt;- read_csv(here::here('data', 'stool_metabolites_height_positive.csv'), skip = 4) # skip the first 4 rows\nmetab_stool_neg &lt;- read_csv(here::here('data', 'stool_metabolites_height_negative.csv'), skip = 4)\n</code></pre>"},{"location":"LCMS/pmp-quality-control/#run-the-preprocessing-script","title":"Run the preprocessing script \ud83c\udfc3","text":"<p>At this point, we can simply run the <code>pmp_preprocess()</code> function. You will just need to identify which of your columns relate to what, and this could change depending on the exact MS-DIAL version you are using.</p> <ol> <li>Information columns: these contain general information including mass, retention time, signal-to-noise ratio, spectrum fill percentage etc. These may be columns <code>1:32</code>.</li> <li>Intensity columns: these contain the actual intensity values associated with each sample-peak pair, and will vary depending on your LCMS run. </li> </ol> <p>Don't include the averaged columns</p> <p>Ensure you do not include the group averages, and only include individual samples, blanks, and QC samples</p> <pre><code># Process stool data\nmetab_stool_pmp &lt;- pmp_preprocess(pos_df = metab_stool_data_pos,\n                                  neg_df = metab_stool_data_neg,\n                                  metadata = metab_metadata,\n                                  samples_key = 'stool',\n                                  intens_cols = c(33:38, 45:60, 67:76),\n                                  info_cols = 1:32,\n                                  mv_imp_method = 'rf')\n\n# Save output data\nsaveRDS(metab_stool_pmp, here::here('output', 'temp', 'metab_stool_pmp_temp.rds'))\n</code></pre> <p>Skip ahead</p> <p>If you run the script here, you can skip down to the principle component analysis step.</p>"},{"location":"LCMS/pmp-quality-control/#detailed-steps","title":"Detailed steps \ud83d\udcdd","text":"<p>If you have a more complex setup, or would rather just do things manually to fine tune each step, we describe here the individual steps that the <code>pmp_process()</code> function is performing.</p>"},{"location":"LCMS/pmp-quality-control/#continue-manual-data-import","title":"Continue manual data import \ud83d\udd04","text":"Combine ionisation modes and prepare intensity and peak tables<pre><code># Remove the MS/MS samples (not acquired in the same way)\nmetab_stool_pos &lt;- metab_stool_pos[, !(names(metab_stool_pos) %in% c('MSMS_pos', 'MSMS_neg'))]\nmetab_stool_neg &lt;- metab_stool_neg[, !(names(metab_stool_neg) %in% c('MSMS_pos', 'MSMS_neg'))]\n\n# Separate into intensity and information data.frames for the SummarizedExperiment object\nmetab_stool_pos_counts &lt;- as.matrix(metab_stool_pos[, c(33:54, 62:165)]) # you need to find the column numbers that contain Blanks, QCs, and samples\nmetab_stool_neg_counts &lt;- as.matrix(metab_stool_neg[, c(33:54, 62:165)])\n\nmetab_stool_pos_info &lt;- metab_stool_pos[, 1:32]\nmetab_stool_neg_info &lt;- metab_stool_neg[, 1:32]\n\n# Rename the data to indicate ionisation mode (using the MS-DIAL alignment ID)\nstool_pos_rownames &lt;- paste0(metab_stool_pos_info$`Alignment ID`, '_pos')\nstool_neg_rownames &lt;- paste0(metab_stool_neg_info$`Alignment ID`, '_neg')\n\nrownames(metab_stool_pos_counts) &lt;- stool_pos_rownames\nrownames(metab_stool_pos_info) &lt;- stool_pos_rownames\nrownames(metab_stool_neg_counts) &lt;- stool_neg_rownames\nrownames(metab_stool_neg_info) &lt;- stool_neg_rownames\n\n# Merge the postive and negative ionisation modes using rbind\nmetab_stool_counts &lt;- rbind(metab_stool_pos_counts, metab_stool_neg_counts)\nmetab_stool_info &lt;- rbind(metab_stool_pos_info, metab_stool_neg_info)\n</code></pre> <p>Now that we have our intensity and feature information datasets, before we can import them into a <code>SummarizedExperiment</code> object, we need to define class and group vectors so that <code>pmp</code> knows whether our variables are samples, QCs, or blanks.</p> <p>The easiest way to achieve this is by getting the first two characters (a substring) of our column names, and using these as indicators of the classes: <code>'Bl'</code> = blank, <code>'QC'</code> = QC etc.</p> Define sample classes and groups<pre><code># Create class and group vectors\nmetab_stool_class &lt;- substr(colnames(metab_stool_counts), start = 1, stop = 2)\nmetab_stool_group &lt;- substr(colnames(metab_stool_counts), start = 1, stop = 2)\n</code></pre> Why didn't this work for me? <p>If you have samples that start with the same two letters as our ideal blanks (<code>'Bl'</code>) and quality control (<code>'QC'</code>) samples, then this process will not work. For example, if you have blood/serum samples that start with <code>'Bl'</code>, then these will need to be renamed. Alternatively, you may wish to manually define your classes and groups.</p>"},{"location":"LCMS/pmp-quality-control/#import-into-a-summarizedexperiment-object","title":"Import into a SummarizedExperiment object \ud83d\udce6","text":"<p>Now that we have our individual components ready, we can check that the metadata matches our sample data, and then import everything into a single, unified <code>SummarizedExperiment</code> object for pre-processing with the <code>pmp</code> package.</p> Assemble a SummarizedExperiment object<pre><code># Reorder the metadata rows so that they match the column order of samples in the counts data.frame (ignoring blanks and QCs)\nmetadata_metab_stool &lt;- metadata_metab_stool[colnames(metab_stool_counts)[23:126], ] # use only column containing sample intensities\n\n# Check that the metadata matches the samples\nidentical(rownames(metadata_metab_stool), colnames(metab_stool_counts)[23:126]) # should return 'TRUE'\n\n# Create the SummarizedExperiment (SE) object\nmetab_stool_SE &lt;- SummarizedExperiment(assays = list(counts = metab_stool_counts),\n                                       metadata = list(metadata_metab_stool),\n                                       rowData = list(info = metab_stool_info),\n                                       colData = DataFrame(class = metab_stool_class))\n</code></pre>"},{"location":"LCMS/pmp-quality-control/#filtering-on-blank-intensity","title":"Filtering on blank intensity \ud83e\uddea\u26d4","text":"<p>The first filtering steps we will carry out are to replace any so-called \"missing\" values (i.e. 0 values) with <code>NA</code> so they will be compatible with downstream filtering steps.</p> <p>After this, we can filter peaks based on the intensity values of our blanks.</p> Filter on blank intensity<pre><code># Check the original number of features\ndim(metab_stool_SE)\n\n# Replace missing values with NA to be compatible with downstream filtering\nassay(metab_stool_SE) &lt;- replace(assay(metab_stool_SE), assay(metab_stool_SE) == 0, NA)\n\n# Filter peaks and optionally samples based on blanks\nmetab_stool_filt &lt;- filter_peaks_by_blanks(df = metab_stool_SE,\n                                           fold_change = 5, # 5-fold change\n                                           classes = metab_stool_SE$class,\n                                           remove_samples = TRUE,\n                                           remove_peaks = TRUE,\n                                           blank_label = 'Bl') # from the class vector\n\n# Check the number of features/samples remaining\ndim(metab_stool_filt)\n</code></pre> <p>Example output (dimension checks):</p> <pre><code>[1] 81334   126 # Original\n[1] 71700   115 # After blank intensity filtering\n</code></pre>"},{"location":"LCMS/pmp-quality-control/#filtering-on-missing-values-and-qc-intensity-variation","title":"Filtering on missing values and QC intensity variation \ud83e\uddfa\ud83d\udd0e","text":"<p>Next, we can perform a few filtering steps based on missing values and degree of variation in the QC samples.</p> <ul> <li><code>filter_samples_by_mv</code>: removal of samples containing a user-defined maximum percentage of missing values see documentation. E.g. setting the <code>max_perc_mv</code> argument to 0.8 will remove all samples with at least 80% missing values.</li> <li><code>filter_peaks_by_fraction</code>: removal of peaks based upon the relative proportion of samples containing non-missing values see documentation.</li> <li><code>filter_peaks_by_rsd</code>: removal of peaks based upon relative standard deviation of intensity values for a given feature within specified QC samples see documentation.</li> </ul> <p>Relative standard deviation</p> <p>As the name suggests, this value is dependent on the standard deviation that exists in your QC samples. As such, if there is substantial variation in your QC samples you have a couple of options:</p> <ul> <li>Increase the <code>max_rsd</code> value \u2013 this is a threshold of the QC RSD% value, so maybe try <code>40</code> if <code>25</code> is too low.</li> <li>Identify problematic QC samples \u2013 see if you can work out what the issue is, and remove that particular QC sample if appropriate.</li> </ul> Continue filtering process<pre><code># Filter samples based on the percentage of missing values\nmetab_stool_filt &lt;- filter_samples_by_mv(df = metab_stool_filt,\n                                         max_perc_mv = 0.8) # remove samples where &gt;80% of values are missing \n\n# Check the number of features/samples\ndim(metab_stool_filt)\n\n# Filter peaks based on missing values across all samples\nmetab_stool_filt &lt;- filter_peaks_by_fraction(df = metab_stool_filt,\n                                             min_frac = 0.8, # features should have values for &gt;80% of samples\n                                             classes = metab_stool_filt$class,\n                                             method = 'across')\n\n# Check the number of features/samples\ndim(metab_stool_filt)\n\n# Filter peaks based on the percentage of variation in the QC samples\nmetab_stool_filt &lt;- filter_peaks_by_rsd(df = metab_stool_filt,\n                                        max_rsd = 25,\n                                        classes = metab_stool_filt$class,\n                                        qc_label = 'QC')\n\n# Check the number of features/samples\ndim(metab_stool_filt)\n</code></pre> <p>Example output (dimension checks):</p> <pre><code>[1] 71700   115 # After filtering for missing values within peaks\n[1] 17315   115 # After filtering for missing values across samples\n[1] 11648   115 # After filtering out high variability in QC samples\n</code></pre>"},{"location":"LCMS/pmp-quality-control/#pqn-normalisation-and-glog-scaling","title":"PQN normalisation and glog scaling \ud83d\udcca\u2696\ufe0f","text":"<p>Finally we can normalise our data using probabilistic quotient normalisation (PQN) (see documentation), followed by missing value imputation (various method choices available; see documentation),  and then transform the data using a variance-stabling generalised logarithmic transformation (see documentation).</p> <p>What is a glog transformation?</p> <p>Generalised logarithmic (glog) transformation is a statistical technique that applies a modified logarithmic function to raw intensity data, thereby stabilising variance across both low and high intensity mass spectral features. The <code>glog_transformation</code> function uses QC samples to optimise the scaling factor lambda, and using the <code>glog_plot_optimised_lambda</code> function it\u2019s possible to visualise if the optimisation of the given parameter has converged at the minima.</p> PQN normalise, impute missing values, and glog transform data<pre><code># PQN data normalisation\nmetab_stool_norm &lt;- pqn_normalisation(df = metab_stool_filt,\n                                      classes = metab_stool_filt$class,\n                                      qc_label = 'QC')\n\n# Missing values imputation\nmetab_stool_imp &lt;- mv_imputation(df = metab_stool_norm,\n                                 rowmax = 0.7, # max % of missing data allowed in any row\n                                 colmax = 0.7, # max % of missing data allowed in any column\n                                 method = 'knn') # or rf, bcpa, sv, mn, md.\n\n# Data scaling\nmetab_stool_glog &lt;- glog_transformation(df = metab_stool_imp,\n                                        classes = metab_stool_imp$class,\n                                        qc_label = 'QC')\n\nopt_lambda_stool &lt;- processing_history(metab_stool_glog)$glog_transformation$lambda_opt\n\nglog_plot_optimised_lambda(df = metab_stool_imp,\n                           optimised_lambda = opt_lambda_stool,\n                           classes = metab_stool_imp$class,\n                           qc_label = 'QC')\n</code></pre> <p>Example plot:</p> <p>We can also check the number of assigned metabolites that remain.</p> Check number of named features<pre><code># Recover RDS object if you used the pmp_preprocess function\nmetab_stool_pmp &lt;- readRDS(here::here('output', 'temp', 'metab_stool_pmp_temp.rds'))\nmetab_stool_glog &lt;- metab_stool_pmp$glog_results\n\n# Number of assigned metabolites\ntable(rowData(metab_stool_glog)@listData[['info.Metabolite name']] != 'Unknown')\n</code></pre> <p>Example output (189 metabolites with assignments remain):</p> <pre><code>FALSE  TRUE \n11459   189 \n</code></pre>"},{"location":"LCMS/pmp-quality-control/#principle-component-analysis","title":"Principle component analysis \ud83d\udcc8","text":"<p>We can now visualise the output data using a principle component analysis plot (PCA) plot.</p> Generate a PCA plot to visualise QC clustering<pre><code># Recover RDS object if you used the pmp_preprocess function\nmetab_stool_pmp &lt;- readRDS(here::here('output', 'temp', 'metab_stool_pmp_temp.rds'))\nmetab_stool_glog &lt;- metab_stool_pmp$glog_results\n\n# Perform the PCA and retrieve the explained variance values\nPCA_stool &lt;- prcomp(t(assay(metab_stool_glog)), center = TRUE)\nvarexp_stool &lt;- c(summary(PCA_stool)$importance[2,1]*100,\n                  summary(PCA_stool)$importance[2,2]*100)\n\n# Create a dataset for plotting\ndata_PCA_stool &lt;- cbind(data.frame(Samples = rownames(PCA_stool$x),\n                                   PC1 = PCA_stool$x[,1],\n                                   PC2 = PCA_stool$x[,2]),\n                        class = metab_stool_glog$class)\n\n# Plot results\n(PCA_stool_plot &lt;- ggplot(data_PCA_stool, aes(x = PC1, y = PC2)) +\n    geom_point(aes(fill = class, color = factor(class))) +\n    stat_ellipse(aes(fill = class), geom = 'polygon', type = 't', level = 0.9, alpha = 0.2) +\n    labs(title = 'Stool Metabolomics',\n         x = paste0('PC1 ', round(varexp_stool[1], 2), '%'),\n         y = paste0('PC2 ', round(varexp_stool[2], 2), '%'))\n)\n</code></pre> <p>Example plot:</p>"},{"location":"LCMS/pmp-quality-control/#next-steps","title":"Next steps \u27a1\ufe0f","text":"<p>Now that you have finished initial quality control, and have an imputed, glog-normalised dataset, you can proceed to secondary MS1 feature identification.</p>"},{"location":"LCMS/pmp-quality-control/#rights","title":"Rights","text":"<ul> <li>Copyright \u00a9\ufe0f 2024 Mucosal Immunology Lab, Monash University, Melbourne, Australia.</li> <li><code>pmp</code> package: Jankevics A, Lloyd GR, Weber RJM (2021). pmp: Peak Matrix Processing and signal batch correction for metabolomics datasets. R package version 1.4.0.</li> <li>License: This pipeline is provided under the MIT license.</li> <li>Authors: M. Macowan and C. Pattaroni</li> </ul>"},{"location":"LCMS/secondary-feature-annotation/","title":"Secondary MS1 feature annotation","text":"<p>After processing LCMS data through the MS-DIAL pipeline, secondary annotations for MS1 features can be obtained by matching mass data to external databases. </p> What do you mean by MS1 data? <ul> <li>What is MS1 data?<ul> <li>MS1 data represents the initial scan in mass spectrometry, where ions are detected based on their mass-to-charge ratio (m/z). It provides an overview of all molecular features in a sample, including their m/z values and intensities, enabling detection and quantification of compounds.</li> </ul> </li> <li>How does that differ from MS/MS (MS2) data?<ul> <li>While MS1 provides a broad profile of ions without structural information, MS/MS involves selecting specific ions from the MS1 scan (precursor ions), fragmenting them, and analysing the resulting product ions. This fragmentation reveals structural details, enabling precise compound identification and differentiation of isomers. </li> </ul> </li> </ul>"},{"location":"LCMS/secondary-feature-annotation/#secondary-annotation-with-hmdb","title":"Secondary annotation with HMDB \ud83d\udcd8","text":"<p>You should currently have a <code>SummarizedExperiment</code> object that has been pre-processed with <code>pmp</code>. The next stage is to match the MS1 mass data for each feature to the Human Metabolome Database (HMDB) database file.</p> <p>Prepared HMDB database file</p> <p>Download the prepared HMDB database file in RDS format here.</p> <ul> <li>Version 5.0 (November 2023) \u2013 pre-formatted and filtered to include only annotated or documented features.</li> </ul>"},{"location":"LCMS/secondary-feature-annotation/#appending-hmdb-annotations-to-summarizedexperiment-objects","title":"Appending HMDB annotations to <code>SummarizedExperiment</code> objects","text":"<p>The first step is to load the formatted HMDB <code>data.frame</code> into your R session.</p> Load HMDB database into your R session<pre><code># Load HMDB dataset\nhmdb_df &lt;- readRDS(here::here('hmdb', 'hmdb_metabolites_detect_quant_v5_20231102.rds'))\n</code></pre> <p>Then, using the <code>add_hmdb()</code> function, we can search the HMDB annotations in the <code>data.frame</code> and add them to our <code>SummarizedExperiment</code> objects, as shown below with an example stool metabolomics object.</p> Parameters for <code>add_hmdb()</code> Parameter Description <code>metab_SE</code> The <code>SummarizedExperiment</code> object you want to annotate with HMDB. <code>hmdb</code> The HMDB database object. <code>mass_tol</code> Default: <code>0.002</code> \u2013 the mass tolerance allowed for annotation. <code>cores</code> The number of parallel processes to use if desired. <p>Run time</p> <p>Running <code>add_hmdb()</code>, especially without parallel processing, can take a very long time.</p> <p>Which <code>SummarizedExperiment</code> should I use?</p> <p>If you are using the output of the <code>pmp_preprocess()</code> function, you should extract and annotate the <code>glog_results</code>. The standard practice is to use the glog-transformed data from here on.</p> Add HMDB secondary annotations<pre><code># Extract the glog data from the pmp_preprocess output\nmetab_stool_glog &lt;- metab_stool_pmp$glog_results\n\n# Search annotations in HMDB and add to the SE objects\nmetab_stool_glog &lt;- add_hmdb(metab_SE = metab_stool_glog,\n                             hmdb = hmdb_df, \n                             mass_tol = 0.002,\n                             cores = 6)\n</code></pre>"},{"location":"LCMS/secondary-feature-annotation/#secondary-annotation-with-lipid-maps","title":"Secondary annotation with LIPID MAPS \ud83e\uddc8","text":"<p>After processing lipidomics data through MS-DIAL, you can enhance the annotations of MS1 features by leveraging the LIPID MAPS Structure Database (LMSD). At this point, you should have a <code>SummarizedExperiment</code> object containing preliminary annotations and those from the HMDB database. The next step involves matching the MS1 mass data of each feature to the entries in the LMSD database.</p> <p>Prepared LMSD database file</p> <p>Download the prepared LMSD database file in RDS format here.</p> <ul> <li>Version 2022-02-16</li> </ul> <p>Should I run this section?</p> <p>You should definitely run this step if you have lipidomics data to process. If you are processing metabolomics data, you can skip this section.</p>"},{"location":"LCMS/secondary-feature-annotation/#appending-lipid-maps-annotations-to-summarizedexperiment-objects","title":"Appending LIPID MAPS annotations to <code>SummarizedExperiment</code> objects","text":"<p>The first step is to load the formatted LMSD <code>data.frame</code> into your R session.</p> Load LMSD database into your R session<pre><code># Load LMSD dataset\nlmsd_df &lt;- readRDS(here::here('lmsd', 'LMSD_231107.rds'))\n</code></pre> <p>Then, using the <code>add_lmsd()</code> function, we can search the LIPID MAPS annotations in the <code>data.frame</code> and add them to our <code>SummarizedExperiment</code> objects, as shown below with an example stool metabolomics object.</p> Parameters for <code>add_lmsd()</code> Parameter Description <code>metab_SE</code> The <code>SummarizedExperiment</code> object you want to annotate with HMDB. <code>lmsd</code> The LMSD database object. <code>mass_tol</code> Default: <code>0.002</code> \u2013 the mass tolerance allowed for annotation. <code>cores</code> The number of parallel processes to use if desired. <p>Run time</p> <p>Running <code>add_lmsd()</code>, especially without parallel processing, can take a very long time.</p> <pre><code># Search annotations in LMSD and add to the SE objects\n# Create list for all distinct Lipid Maps matching mz in tolerance range 0.002, an aggregated df of distinct lipids and a df to replace SummarizedExperiment metadata [rowData(metab_glog)]\nlmsd_ann_list &lt;- add_lmsd(metab_SE = metab_stool_glog, \n                          lmsd = lmsd_df, \n                          mass_tol = 0.002,\n                          cores = 6) \n\n# Use metadata_lmsd_table to replace the existing SE object metadata\nrowData(metab_stool_glog) &lt;- lmsd_ann_list$metadata_lmsd_table\n</code></pre>"},{"location":"LCMS/secondary-feature-annotation/#comparing-annotations-from-different-databases","title":"Comparing annotations from different databases \u2696\ufe0f\ud83d\udcda","text":"<p>To compare the assigned annotations from each of the methods the compare_annotations_SE() function. It will produce a <code>data.frame</code> containing only features with at least one annotation, and allow us see whether the annotations typically agree with each other.</p> Parameters for <code>compare_annotations_SE()</code> Parameter Description <code>metab_SE</code> The <code>SummarizedExperiment</code> object with secondary annotations. These should include HMDB for metabolomics data, and both HMDB and LMSD for lipidomics data. <code>mode</code> Either <code>'metabolomics'</code> or <code>'lipidomics'</code> depending on your dataset. <code>agg_lmsd_ann</code> The aggregated LMSD annotations you generated using the <code>add_lmsd()</code> function, i.e. <code>lmsd_ann_list$agg_lmsd_df</code>. Only required for <code>mode</code> = <code>'lipidomics'</code> MetabolomicsLipidomics Compare metabolite annotations<pre><code># Prepare data.frame with alignment IDs and annotations, and filter for at least one annotation\nanno_df_metab &lt;- compare_annotations_SE(metab_SE = metab_stool_glog,\n                                        mode = 'metabolomics')\n</code></pre> Compare lipid annotations<pre><code># Prepare data.frame with alignment IDs and annotations, and filter for at least one annotation\nanno_df_lipid &lt;- compare_annotations_SE(metab_SE = lipid_stool_glog,\n                                        mode = 'lipidomics',\n                                        agg_lmsd_ann = lmsd_ann_list$agg_lmsd_df)\n</code></pre>"},{"location":"LCMS/secondary-feature-annotation/#keeping-only-annotated-features","title":"Keeping only annotated features \ud83c\udff7\ufe0f\u2705","text":"<p>From here, we can filter our <code>SummarizedExperiment</code> object for features with at least one annotation. While the other features likely represent interesting metabolites and lipids, without an available annotation, they won't be interpretable downstream.</p> <p>We can achieve this providing our <code>SummarizedExperiment</code> object to the <code>keep_annotated_SE()</code> function, which will output a filtered <code>SummarizedExperiment</code> object.</p> Parameters for <code>keep_annotated_SE()</code> Parameter Description <code>metab_SE</code> The <code>SummarizedExperiment</code> object with secondary annotations. These should include HMDB for metabolomics data, and both HMDB and LMSD for lipidomics data. <code>mode</code> Either <code>'metabolomics'</code> or <code>'lipidomics'</code> depending on your dataset. MetabolomicsLipidomics <p>The function will create a new <code>rowData</code> element called <code>shortname</code>, and also assign this value as the preferred row name.</p> <ul> <li>It uses the following naming hierarchy to decide an appropriate name: HMDB &gt; MS-DIAL.</li> </ul> Remove unannotated features<pre><code># Keep only annotated rows and generate shortname column\nmetab_stool_glog &lt;- keep_annotated(metab_SE = metab_stool_glog,\n                                   mode = 'metabolomics')\n\n# Save the object\nsaveRDS(metab_stool_glog, here('output', '01_Preprocessing', 'metab_stool_glog_anno.rds'))\n</code></pre> <p>Are the other annotations still there?</p> <p>Yes! While the shorter names are succinct and useful for plotting, you can view the additional annotations at any time and alter as required.</p> <pre><code># Get HMDB and KEGG annotations\nhmdb_annotations &lt;- rowData(metab_stool_glog)$HMDB\nkegg_annotations &lt;- rowData(metab_stool_glog)$KEGG\n</code></pre> <p>The function will create a new <code>rowData</code> element called <code>shortname</code>, and also assign this value as the preferred row name.</p> <ul> <li>It uses the following naming hierarchy to decide an appropriate name: MS-DIAL &gt; LMSD &gt; HMDB.</li> <li>MS-DIAL has its own lipid database that is effective and, because it can also utilise MS/MS data in its annotations, is preferred here.</li> </ul> Remove unannotated features<pre><code># Keep only annotated rows and generate shortname column\nlipid_stool_glog &lt;- keep_annotated(metab_SE = lipid_stool_glog,\n                                   mode = 'lipidomics')\n\n# Save the object\nsaveRDS(lipid_stool_glog, here('output', '01_Preprocessing', 'lipid_stool_glog_anno.rds'))\n</code></pre> <p>Are the other annotations still there?</p> <p>Yes! While the shorter names are succinct and useful for plotting, you can view the additional annotations at any time and alter as required.</p> <pre><code># Get HMDB and KEGG annotations\nhmdb_annotations &lt;- rowData(lipid_stool_glog)$HMDB\nkegg_annotations &lt;- rowData(lipid_stool_glog)$KEGG\n</code></pre>"},{"location":"LCMS/secondary-feature-annotation/#next-steps","title":"Next steps \u27a1\ufe0f","text":"<p>You now have a normalised, imputed, dataset that has undergone secondary annotation and been filtered for annotated features. It is now time to proceed to manual curation of the annotated spectra.</p>"},{"location":"LCMS/secondary-feature-annotation/#rights","title":"Rights","text":"<ul> <li>Copyright \u00a9\ufe0f 2024 Respiratory Immunology lab, Monash University, Melbourne, Australia.</li> <li>HMDB version 5.0: Wishart DS, Guo A, Oler E, Wang F, Anjum A, Peters H, Dizon R, Sayeeda Z, Tian S, Lee BL, Berjanskii M. HMDB 5.0: the human metabolome database for 2022. Nucleic acids research. 2022 Jan 7;50(D1):D622-31.</li> <li>License: This pipeline is provided under the MIT license.</li> <li>Authors: M. Macowan and C. Pattaroni</li> </ul>"},{"location":"NextFlow/nf-mucimmuno/","title":"Nextflow Workflows","text":"<p>As they are built and published, this repository will contain Nextflow workflows for processing of different data omic modalities.</p> <p>Additionally, we may provide additional tools and code for further downstream processing, with the goal of standardising data analytic approaches within the Mucosal Immunology Lab.</p>"},{"location":"NextFlow/nf-mucimmuno/#single-cell-rnaseq-fastq-pre-processing","title":"Single-cell RNAseq FASTQ pre-processing","text":"<p>nf-mucimmuno/scRNAseq is a bioinformatics pipeline for single-cell RNA sequencing data that can be used to run quality control steps and alignment to a host genome using STARsolo. Currently only configured for use with data resulting from BD Rhapsody library preparation.</p> <p></p>"},{"location":"NextFlow/scRNAseq/","title":"Single-cell RNAseq FASTQ pre-processing","text":""},{"location":"NextFlow/scRNAseq/#introduction","title":"Introduction","text":"<p>nf-mucimmuno/scRNAseq is a bioinformatics pipeline that can be used to run quality control steps and alignment to a host genome using STARsolo. It takes a samplesheet and FASTQ files as input, performs FastQC, trimming and alignment, and produces an output <code>.tar.gz</code> archive containing the collected outputs from STARsolo, ready for further processing downstream in R. MultiQC is run on the FastQC outputs both before and after TrimGalore! for visual inspection of sample quality \u2013 output <code>.html</code> files are collected in the results.</p> <p></p>"},{"location":"NextFlow/scRNAseq/#usage","title":"Usage","text":""},{"location":"NextFlow/scRNAseq/#download-the-repository","title":"Download the repository \ud83d\udcc1","text":"<p>This repository contains the relevant Nextflow workflow components, including a conda environment and submodules, to run the pipeline. To retrieve this repository alone, run the <code>retrieve_me.sh</code> script above.</p> <p>Git version requirements</p> <p>Git <code>sparse-checkout</code> is required to retrieve just the nf-mucimmuno/scRNAseq pipeline. It was only introduced to Git in version 2.27.0, so ensure that the loaded version is high enough (or that there is a version loaded on the cluster at all). As of July 2024, the M3 MASSIVE cluster has version 2.38.1 available.</p> <pre><code># Check git version\ngit --version\n\n# Load git module if not loaded or insufficient version\nmodule load git/2.38.1\n</code></pre> <p>First, create a new bash script file.</p> <pre><code># Create and edit a new file with nano\nnano retrieve_me.sh\n</code></pre> <p>Add the contents to the file, save, and close.</p> retrieve_me.sh<pre><code>#!/bin/bash\n\n# Define variables\nREPO_URL=\"https://github.com/mucosal-immunology-lab/nf-mucimmuno\"\nREPO_DIR=\"nf-mucimmuno\"\nSUBFOLDER=\"scRNAseq\"\n\n# Clone the repository with sparse checkout\ngit clone --no-checkout $REPO_URL\ncd $REPO_DIR\n\n# Initialize sparse-checkout and set the desired subfolder\ngit sparse-checkout init --cone\ngit sparse-checkout set $SUBFOLDER\n\n# Checkout the files in the subfolder\ngit checkout main\n\n# Move the folder into the main folder and delete the parent\nmv $SUBFOLDER ../\ncd ..\nrm -rf $REPO_DIR\n\n# Extract the larger gzipped CLS files\ngunzip -r \"$SUBFOLDER/modules/starsolo/CLS\"\n\necho \"Subfolder '$SUBFOLDER' has been downloaded successfully.\"\n</code></pre> <p>Then run the script to retrieve the repository into a new folder called <code>scRNAseq</code>, which will house your workflow files and results.</p> <pre><code># Run the script\nbash retrieve_me.sh\n</code></pre>"},{"location":"NextFlow/scRNAseq/#create-the-conda-environment","title":"Create the conda environment \ud83d\udc0d","text":"<p>To create the conda environment, use the provided environment <code>.yaml</code> file. Then activate it to access required functions.</p> <pre><code># Create the environment\nmamba env create -f environment.yaml\n\n# Activate the environment\nmamba activate nextflow-scrnaseq\n</code></pre>"},{"location":"NextFlow/scRNAseq/#prepare-the-genome","title":"Prepare the genome \ud83e\uddec","text":"<p>Create a new folder somewhere to store your genome files. Enter the new folder, and run the relevant code depending on your host organism. Run these steps in an interactive session with ~48GB RAM and 16 cores, or submit them as an sbatch job.</p> <p>Do these databases exist already?</p> <p>Please check if these are already available somewhere before regenerating them yourself!</p> <p>STAR should be loaded already via the conda environment for the genome indexing step. We will set <code>--sjdbOverhang</code> to 79 to be suitable for use with the longer <code>R2</code> FASTQ data resulting from BD Rhapsody single cell sequencing. This may require alteration for other platforms. Essentially, you just need to set <code>--sjdbOverhang</code> to the length of your R2 sequences minus 1.</p> Human genome files \ud83d\udc68\ud83d\udc69Mouse genome files \ud83d\udc01 01_retrieve_human_genome.sh<pre><code>#!/bin/bash\nVERSION=111\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/gtf/homo_sapiens/Homo_sapiens.GRCh38.$VERSION.gtf.gz\ngunzip *\n</code></pre> 01_retrieve_mouse_genome.sh<pre><code>#!/bin/bash\nVERSION=111\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/fasta/mus_musculus/dna/Mus_musculus.GRCm39.dna_sm.primary_assembly.fa.gz\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/gtf/mus_musculus/Mus_musculus.GRCm39.$VERSION.gtf.gz\ngunzip *\n</code></pre> <p>Then use STAR to prepare the genome index.</p> Human genome files \ud83d\udc68\ud83d\udc69Mouse genome files \ud83d\udc01 02_index_human_genome.sh<pre><code>#!/bin/bash\nVERSION=111\nSTAR \\\n    --runThreadN 16 \\\n    --genomeDir \"STARgenomeIndex79/\" \\\n    --runMode genomeGenerate \\\n    --genomeFastaFiles \"Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa\" \\\n    --sjdbGTFfile \"Homo_sapiens.GRCh38.$VERSION.gtf\" \\\n    --sjdbOverhang 79\n</code></pre> 02_index_mouse_genome.sh<pre><code>#!/bin/bash\nVERSION=111\nSTAR \\\n    --runThreadN 16 \\\n    --genomeDir \"STARgenomeIndex79/\" \\\n    --runMode genomeGenerate \\\n    --genomeFastaFiles \"Mus_musculus.GRCm39.dna_sm.primary_assembly.fa\" \\\n    --sjdbGTFfile \"Mus_musculus.GRCm39.$VERSION.gtf\" \\\n    --sjdbOverhang 79\n</code></pre>"},{"location":"NextFlow/scRNAseq/#prepare-your-sample-sheet","title":"Prepare your sample sheet \u270f\ufe0f","text":"<p>This pipeline requires a sample sheet to identify where your FASTQ files are located, and which cell label sequences (CLS) are being utilised.</p> <p>More information about the CLS tags used with BD Rhapsody single-cell RNAseq library preparation can be found here:</p> <ul> <li>BD Rhapsody Sequence Analysis Pipeline \u2013 User's Guide</li> <li>BD Rhapsody Cell Label Structure \u2013 Python Script</li> </ul> <p>More information about the CLS tags used with 10X Chromium single-cell RNAseq library preparation can be found here:</p> <ul> <li>10X Chromium Single Cell 3' Solution V2 and V3 guide (Teich Lab)</li> <li>10X Chromium V2 CLS sequences are 26bp long.</li> <li>10X Chromium V3 CLS sequences are 28bp long.</li> </ul> <p>The benefit of providing the name of the CLS bead versions in the sample sheet is that you can combine runs that utilise different beads together in the same workflow. Keep in mind that if you do this though, there may be some bead-related batch effects to address and correct downstream \u2013 it is always important to check for these effects when combining sequencing runs in any case. The current options are:</p> CLS option Description BD_Original The original BD rhapsody beads and linker sequences BD_Enhanced_V1 First version of enhanced beads with polyT and 5prime capture oligo types, shorter linker sequences, longer polyT, and 0-3 diversity insert bases at the beginning of the sequence BD_Enhanced_V2 Same structure as the enhanced (V1) beads, but with increased CLS diversity (384 vs. 96) 10X_Chromium_V2 Feature a 16 bp cell barcode and a 10 bp unique molecular identifier (UMI) 10X_Chromium_V3 Enhanced sequencing accuracy and resolution with a 16 bp cell barcode and a 12 bp UMI <p>Further, we also need to provide the path to the STAR genome index folder for each sample \u2013 while in many cases this value will remain constant, the benefit of providing this information is that you can process runs with different R2 sequence lengths at the same time. Recall from above that the genome index you use should use an <code>--sjdbOverhang</code> length that of your R2 sequences minus 1.</p> <p>Your sample sheet should look as follows, ensuring you use the exact column names as below. </p> <p>File paths on M3</p> <p>Remember that on the M3 MASSIVE cluster, you need to use the full file path \u2013 relative file paths don't usually work.</p> <pre><code>sample,fastq_1,fastq_2,CLS,GenomeIndex\nCONTROL_S1,CONTROL_S1_R1.fastq.gz,CONTROL_S1_R2.fastq.gz,BD_Enhanced_V2,mf33/Databases/ensembl/human/STARgenomeIndex79\nCONTROL_S2,CONTROL_S2_R1.fastq.gz,CONTROL_S1_R2.fastq.gz,BD_Enhanced_V2,mf33/Databases/ensembl/human/STARgenomeIndex79\nTREATMENT_S1,TREATMENT_S1_R1.fastq.gz,TREATMENT_S1_R2.fastq.gz,BD_Enhanced_V2,mf33/Databases/ensembl/human/STARgenomeIndex79\n</code></pre> <p>An example is provided here.</p>"},{"location":"NextFlow/scRNAseq/#running-the-pipeline","title":"Running the pipeline \ud83c\udfc3","text":"<p>Now you can run the pipeline. You will need to set up a parent job to run each of the individual jobs \u2013 this can be either an interactive session, or an sbatch job. For example:</p> <pre><code># Start an interactive session with minimal resources\nsmux n --time=3-00:00:00 --mem=16GB --ntasks=1 --cpuspertask=2 -J nf-STARsolo\n</code></pre> <p>Set the correct sample sheet location</p> <p>Make sure you alter the <code>nextflow.config</code> file to provide the path to your sample sheet, unless it is <code>./data/samplesheet.csv</code> which is the default for the cluster profile. Stay within the top <code>cluster</code> profile section to alter settings for Slurm-submitted jobs.</p> <p>Inside your interactive session, be sure to activate your <code>nextflow-scrnaseq</code> environment from above. Then, inside the scRNAseq folder, begin the pipeline using the following command (ensuring you use the <code>cluster</code> profile to make use of the Slurm workflow manager).</p> <pre><code># Activate conda environment\nmamba activate nextflow-scrnaseq\n\n# Begin running the pipeline\nnextflow run process_raw_reads.nf -resume -profile cluster\n</code></pre>"},{"location":"NextFlow/scRNAseq/#customisation","title":"Customisation \u2699\ufe0f","text":"<p>There are several customisation options that are available within the <code>nextflow.config</code> file. While the defaults should be suitable for those with access to the M3 MASSIVE cluster genomics partition, for those without access, of for those who require different amounts of resources, there are ways to change these.</p> <p>In order to work with different technologies, and accommodate for differences in cell label structure (CLS), the STAR parameters <code>--soloType</code> and <code>--soloCBmatchWLtype</code> are set in a CLS-dependent manner. This is required, because the BD Rhapsody system has a complex barcode structure. The 10X Chromium system on the other hand has a simple barcode structure with a single barcode and single UMI. Additionally, the <code>--soloCBmatchWLtype = EditDist2</code> only works with <code>--soloType = CB_UMI_Complex</code>, and therefore <code>--soloCBmatchWLtype = 1MM multi Nbase pseudocounts</code> is used for 10X Chromium runs.</p> <ul> <li>For BD Rhapsody sequencing: <code>--soloType = CB_UMI_Complex</code> and <code>--soloCBmatchWLtype = EditDist2</code>.</li> <li>For 10X Chromium sequencing: <code>--soloType = CB_UMI_Simple</code> and <code>--soloCBmatchWLtype = 1MM multi Nbase pseudocounts</code>.</li> <li>Additionally, 10X Chromium runs use <code>--clipAdapterType = CellRanger4</code>.</li> </ul> <p>To adjust the <code>cluster</code> profile settings, stay within the appropriate section at the top of the file.</p> <p>Parameters</p> <p>Visit STAR documentation for explanations of all available options for STARsolo.</p> Option Description samples_csv The file path to your sample sheet outdir A new folder name to be created for your results trimgalore.quality The minimum quality before a sequence is truncated (default: <code>20</code>) trimgalore.adapter A custom adapter sequence for the R1 sequences (default: <code>'AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC'</code>) trimgalore.adapter2 A custom adapter sequence for the R2 sequences (default: <code>'AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT'</code>) starsolo.soloUMIdedup The type of UMI deduplication (default: <code>'1MM_CR'</code>) starsolo.soloUMIfiltering The type of UMI filtering for reads uniquely mapping to genes (default: <code>'MultiGeneUMI_CR'</code>) starsolo.soloCellFilter The method type and parameters for cell filtering (default: <code>'EmptyDrops_CR'</code>) starsolo.soloMultiMappers The counting method for reads mapping for multiple genes (default: <code>'EM'</code>) <p>Process</p> <p>These settings relate to resource allocation and cluster settings. FASTQC and TRIMGALORE steps can take longer than 4 hours for typical single-cell RNAseq file, and therefore the default option is to run these steps on the <code>comp</code> partition.</p> Option Description executor The workload manager (default: <code>'slurm'</code>) conda The conda environment to use (default: <code>'./environment.yaml'</code>) queueSize The maximum number of jobs to be submitted at any time (default: <code>12</code>) submitRateLimit The rate allowed for job submission \u2013 either a number of jobs per second (e.g. 20sec) or a number of jobs per time period (e.g. 20/5min) (default: <code>'1/2sec'</code>) memory The maximum global memory allowed for Nextflow to use (default: <code>'320 GB'</code>) FASTQC.memory Memory for FASTQC step to use (default: <code>'80 GB'</code>) FASTQC.cpus Number of CPUs for FASTQC step to use (default: <code>8</code>) FASTQC.clusterOptions Specific cluster options for FASTQC step (default: <code>'--time=8:00:00'</code>) TRIMGALORE.memory Memory for TRIMGALORE step to use (default: <code>'80 GB'</code>) TRIMGALORE.cpus Number of CPUs for TRIMGALORE step to use (default: <code>8</code>) TRIMGALORE.clusterOptions Specific cluster options for TRIMGALORE step (default : <code>'--time=8:00:00'</code>) STARSOLO.memory Memory for STARSOLO step to use (default: <code>'80 GB'</code>) STARSOLO.cpus Number of CPUs for STARSOLO step to use (default: <code>12</code>) STARSOLO.clusterOptions Specific cluster options for STARSOLO step (default : <code>'--time=4:00:00 --partition=genomics --qos=genomics'</code>) COLLECT_EXPORT_FILES.memory Memory for COLLECT_EXPORT_FILES step to use (default: <code>'32 GB'</code>) COLLECT_EXPORT_FILES.cpus Number of CPUs for COLLECT_EXPORT_FILES step to use (default: <code>8</code>) COLLECT_EXPORT_FILES.clusterOptions Specific cluster options for COLLECT_EXPORT_FILES step (default : <code>'--time=4:00:00 --partition=genomics --qos=genomics'</code>)"},{"location":"NextFlow/scRNAseq/#outputs","title":"Outputs","text":"<p>Several outputs will be copied from their respective Nextflow <code>work</code> directories to the output folder of your choice (default: <code>results</code>).</p> <p>Alignment summary utility script</p> <p> There is also a utility script in the main <code>scRNAseq</code> directory called <code>collect_alignment_summaries.sh</code>. This will navigate into each of the sample folders inside <code>results/STARsolo</code>, and retrieve some key information for you to validate that the alignment worked successfully (from the <code>GeneFull_Ex50pAS</code> subfolder). This can otherwise take quite some time to go through each folder if you have a lot of samples.</p> <ul> <li>After running this, a new file called <code>AlignmentSummary.txt</code> will be generated in the <code>scRNAseq</code> directory. Each sample will be listed by name, with the number of reads, percentage of reads with valid barcodes, and estimated number of cells.</li> <li>It will be immediately obvious that something has gone wrong if you see that the percentage of reads with valid barcodes is very low (e.g. <code>0.02</code> = 2% valid barcodes) \u2013 this is usually paired with a very low estimated cell number.</li> <li>This could indicate that you have used the wrong barcode version for your runs, and therefore the associated barcode whitelist used by the pipeline was incorrect.</li> </ul> <p>A successful example is shown below </p> <pre><code>Sample: Healthy1\nNumber of Reads,353152389\nReads With Valid Barcodes,0.950799\nEstimated Number of Cells,6623\n\nSample: Healthy2\nNumber of Reads,344989615\nReads With Valid Barcodes,0.948577\nEstimated Number of Cells,6631\n# etc...\n</code></pre>"},{"location":"NextFlow/scRNAseq/#collected-export-files","title":"Collected export files \ud83d\udce6","text":"<p>The main output will be a single archive file called <code>export_files.tar.gz</code> that you will take for further downstream pre-processing. It contains STARsolo outputs for each sample, with the respective subfolders described below.</p>"},{"location":"NextFlow/scRNAseq/#reports","title":"Reports \ud83d\udcc4","text":"<p>Within the <code>reports</code> folder, you will find the MultiQC outputs from pre- and post-trimming.</p>"},{"location":"NextFlow/scRNAseq/#starsolo","title":"STARsolo \u2b50","text":"<p>Contains the outputs for each sample from STARsolo, including various log files and package version information.</p> <p>The main output of interest here is a folder called <code>{sample}.Solo.out</code>, which houses subfolders called <code>Gene</code>, <code>GeneFull_Ex50pAS</code>, and <code>Velocyto</code>. It is this main folder for each sample that is added to <code>export_files.tar.gz</code>. * As you will use the gene count data from <code>GeneFull_Ex50pAS</code> downstream, it is a good idea to check the <code>Summary.csv</code> within this folder for each sample to ensure mapping was successful (or use the utility script above).   * One of the key values to inspect is <code>Reads With Valid Barcodes</code>, which should be &gt;0.8 (indicating at least 80% of reads had valid barcodes).   * If you note that this value is closer to 0.02 (i.e. ~2% had valid barcodes), you should double-check to make sure you specified the correct BD Rhapsody beads version. For instance, if you specified <code>BD_Enhanced_V1</code> but actually required <code>BD_Enhanced_V2</code>, the majority of your reads will not match the whitelist, and therefore the reads will be considered invalid.</p> <p>Folder structure</p> <p>Below is an example of the output structure for running one sample. The STARsolo folder would contain additional samples as required.</p> <pre><code>scRNAseq\n\u2514\u2500\u2500 results/\n    \u251c\u2500\u2500 export_files.tar.gz\n    \u251c\u2500\u2500 reports/\n    \u2502   \u251c\u2500\u2500 pretrim_multiqc_report.html\n    \u2502   \u2514\u2500\u2500 posttrim_multiqc_report.html\n    \u2514\u2500\u2500 STARsolo/\n        \u2514\u2500\u2500 sample1/\n            \u251c\u2500\u2500 sample1.Solo.out/\n            \u2502   \u251c\u2500\u2500 Gene/\n            \u2502   \u2502   \u251c\u2500\u2500 filtered/\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 barcodes.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 features.tsv.gz\n            \u2502   \u2502   \u2502   \u2514\u2500\u2500 matrix.mtx.gz\n            \u2502   \u2502   \u251c\u2500\u2500 raw/\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 barcodes.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 features.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 matrix.mtx.gz\n            \u2502   \u2502   \u2502   \u2514\u2500\u2500 UniqueAndMult-EM.mtx.gz\n            \u2502   \u2502   \u251c\u2500\u2500 Features.stats\n            \u2502   \u2502   \u251c\u2500\u2500 Summary.csv\n            \u2502   \u2502   \u2514\u2500\u2500 UMIperCellSorted.txt\n            \u2502   \u251c\u2500\u2500 GeneFull_Ex50pAS/\n            \u2502   \u2502   \u251c\u2500\u2500 filtered/\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 barcodes.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 features.tsv.gz\n            \u2502   \u2502   \u2502   \u2514\u2500\u2500 matrix.mtx.gz\n            \u2502   \u2502   \u251c\u2500\u2500 raw/\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 barcodes.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 features.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 matrix.mtx.gz\n            \u2502   \u2502   \u2502   \u2514\u2500\u2500 UniqueAndMult-EM.mtx.gz\n            \u2502   \u2502   \u251c\u2500\u2500 Features.stats\n            \u2502   \u2502   \u251c\u2500\u2500 Summary.csv\n            \u2502   \u2502   \u2514\u2500\u2500 UMIperCellSorted.txt\n            \u2502   \u251c\u2500\u2500 Velocyto/\n            \u2502   \u2502   \u251c\u2500\u2500 filtered/\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 ambiguous.mtx.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 barcodes.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 features.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 spliced.mtx.gz\n            \u2502   \u2502   \u2502   \u2514\u2500\u2500 unspliced.mtx.gz\n            \u2502   \u2502   \u251c\u2500\u2500 raw/\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 ambiguous.mtx.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 barcodes.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 features.tsv.gz\n            \u2502   \u2502   \u2502   \u251c\u2500\u2500 spliced.mtx.gz\n            \u2502   \u2502   \u2502   \u2514\u2500\u2500 unspliced.mtx.gz\n            \u2502   \u2502   \u251c\u2500\u2500 Features.stats\n            \u2502   \u2502   \u2514\u2500\u2500 Summary.csv\n            \u2502   \u2514\u2500\u2500 Barcodes.stats\n            \u251c\u2500\u2500 sample1.Log.final.out\n            \u251c\u2500\u2500 sample1.Log.out\n            \u251c\u2500\u2500 sample1.Log.progress.out\n            \u2514\u2500\u2500 versions.yml\n</code></pre>"},{"location":"PublicDatasets/public-datasets/","title":"Public datasets","text":"<p>Here we provide a list of publicly-available datasets that we have generated and uploaded to repositories. Some of the data is yet to be released, and will be available following publication.</p>"},{"location":"PublicDatasets/public-datasets/#ncbi-sequencing-read-archive","title":"NCBI Sequencing Read Archive","text":"<p>The following datasets have been uploaded to the NCBI Sequencing Read Archive (SRA) database in their original FASTQ data format.</p>"},{"location":"PublicDatasets/public-datasets/#summary","title":"Summary","text":"Sequencing type Sequencing runs (uploaded) Bulk transcriptomics 425 Single-cell transcriptomics 2 Shotgun metagenomics 310 16S amplicon 1,146 ITS amplicon 373"},{"location":"PublicDatasets/public-datasets/#datasets","title":"Datasets","text":"Host organism Context BioProject Availability Bulk transcriptomics Single-cell transcriptomics Shotgun metagenomics 16S amplicon ITS amplicon Mouse SHIP-deficient model of Crohn's-like ileitis and chronic lung inflammation PRJNA1086166 \u2013 2024  Released 24 stool samples Human Paediatric severe wheeze + asthma PRJNA1080233 \u2013 2024  Released 55 bronchial brushes 28 bronchial brushes Human Paediatric healthy + infant wheeze PRJNA1076275 \u2013 2024  Released 188 nasal swabs + 73 blood samples 320 nasal swabs 135 nasal swabs Human Infant cystic fibrosis PRJNA978345 \u2013 2024  Released 96 stool samples 75 BAL samples Rat Early life stress + mild traumatic brain injury PRJNA940177 \u2013 2024  Released 76 stool samples Mouse OTII cells Germinal centre expansion + IL-21 role PRJNA776662 \u2013 2021  Released 8 culture samples Human Early life + airways PRJNA694493 \u2013 2021  Released 85 nasal swabs 118 nasal swabs + 119 oropharyngeal swabs 119 nasal swabs + 119 oropharyngeal swabs Mouse Allergic airway inflammation PRJNA641984 \u2013 2020  Released 20 stool samples 127 stool samples Human Male-associated infertility PRJNA509076 \u2013 2018  Released 94 seminal fluid samples Human Early life + immune development PRJNA475630 \u2013 2018  Released 16 tracheal aspirates 45 tracheal aspirates Mouse High fat diet PRJNA1131116  To be released 24 ileum luminal samples + 24 ileum mucosal samples + 22 colon luminal samples 77 stool samples Mouse Early life antibiotic treatment PRJNA1112091  To be released 2 lung structural cell digests 96 stool samples 41 lung tissue samples + 30 BAL samples"},{"location":"PublicDatasets/public-datasets/#european-nucleotide-archive","title":"European Nucleotide Archive","text":"<p>The following datasets have been uploaded to the European Nucleotide Archive (ENA) database in their original FASTQ data format.</p>"},{"location":"PublicDatasets/public-datasets/#summary_1","title":"Summary","text":"Sequencing type Sequencing runs (uploaded) 16S amplicon 1,179"},{"location":"PublicDatasets/public-datasets/#datasets_1","title":"Datasets","text":"Host organism Context Project ID 16S amplicon Availability Human Early life + atopic dermatitis PRJEB42268 \u2013 2022  Released 1,179 lateral upper arm swabs"},{"location":"RNAseq/rnaseq-nfcore/","title":"Processing RNA sequencing data with nf-core","text":""},{"location":"RNAseq/rnaseq-nfcore/#overview","title":"Overview","text":"<p>Here we will describe the process for processing RNA sequencing data using the nf-core/rnaseq pipeline. This document was written as of version 3.14.0</p> <p>nf-core/rnaseq is a bioinformatics pipeline that can be used to analyse RNA sequencing data obtained from organisms with a reference genome and annotation. It takes a samplesheet and FASTQ files as input, performs quality control (QC), trimming and (pseudo-)alignment, and produces a gene expression matrix and extensive QC report.</p> <p>Full details of the pipeline and the many customisable options can be view on the pipeline website.</p> <p></p>"},{"location":"RNAseq/rnaseq-nfcore/#installation","title":"Installation","text":"<p>In this section, we discuss the installation process on the M3 MASSIVE cluster.</p>"},{"location":"RNAseq/rnaseq-nfcore/#create-nextflow-environment","title":"Create nextflow environment \ud83d\udc0d","text":"<p>To begin with, we need to create a new environment using mamba. Mamba is recommended here over conda due to its massively improved dependency solving speeds and parallel package downloading (among other reasons).</p> <pre><code># Create environment\nmamba create -n nextflow nextflow \\\n    salmon=1.10.0 fq fastqc umi_tools \\\n    trim-galore bbmap sortmerna samtools \\\n    picard stringtie bedtools rseqc \\\n    qualimap preseq multiqc subread \\\n    ucsc-bedgraphtobigwig ucsc-bedclip \\\n    bioconductor-deseq2\n\n# Activate environment\nmamba activate nextflow\n</code></pre>"},{"location":"RNAseq/rnaseq-nfcore/#download-and-compile-rsem","title":"Download and compile RSEM","text":"<p>RSEM is a software package for estimating gene and isoform expression levels from RNA-Seq data.</p> <pre><code># Download RSEM\ngit clone https://github.com/deweylab/RSEM\n\n# Enter the directory (RSEM) and compile\ncd RSEM; make\n</code></pre> <p>Make note of this directory for your run script so you can add this to your PATH variable.</p>"},{"location":"RNAseq/rnaseq-nfcore/#prepare-your-sample-sheet","title":"Prepare your sample sheet \u270f\ufe0f","text":"<p>You will need to have a sample sheet prepared that contains a sample name, the <code>fastq.gz</code> file paths, and the strandedness of the read files.</p> <p>If you are working with a single-ended sequencing run, leave the <code>fastq_2</code> column empty, but the header still needs to be included.</p> <p>For example, <code>samplesheet.csv</code>:</p> <pre><code>sample,fastq_1,fastq_2,strandedness\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L003_R1_001.fastq.gz,AEG588A1_S1_L003_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L004_R1_001.fastq.gz,AEG588A1_S1_L004_R2_001.fastq.gz,auto\n</code></pre> <p>Each row represents a fastq file (single-end) or a pair of fastq files (paired end). Rows with the same sample identifier are considered technical replicates and merged automatically. The strandedness refers to the library preparation and will be automatically inferred if set to auto.</p>"},{"location":"RNAseq/rnaseq-nfcore/#run-the-pipeline","title":"Run the pipeline \ud83c\udf4f","text":""},{"location":"RNAseq/rnaseq-nfcore/#start-a-new-interactive-session","title":"Start a new interactive session","text":"<p>Firstly, we will start a new interactive session on the M3 MASSIVE cluster.</p> <pre><code>smux n --time=2-00:00:00 --mem=64GB --ntasks=1 --cpuspertask=12 -J nf-core/rnaseq\n</code></pre> <p>Once we are inside the interactive session, we need to select an appropriate version of the Java JDK to use. For the Nextflow pipeline we will be running, we need at least version 17+.</p> <pre><code># View available java JDK modules\nmodule avail java\n\n# Load an appropriate one (over version 17)\nmodule load java/openjdk-17.0.2\n\n# Can double-check the correct version is loaded\njava --version\n</code></pre>"},{"location":"RNAseq/rnaseq-nfcore/#test-your-set-up-optional","title":"Test your set-up (optional) \ud83e\uddba","text":"<p>This step is optional, but highly advisable for a first-time setup or when re-installing.</p> <pre><code>nextflow run nf-core/rnaseq -r 3.14.0 \\\n    -profile test \\\n    --outdir test \\\n    -resume \\\n    --skip-dupradar \\\n    --skip_markduplicates\n</code></pre> <p>Um... why are we skipping things?</p> <ul> <li>We skip the <code>dupradar</code> step, because to install <code>bioconductor-dupradar</code>, mamba wants to downgrade <code>salmon</code> to a very early version, which is not ideal </li> <li>We also skip the <code>markduplicates</code> step because it is not recommended to remove duplicates anyway due to normal biological duplicates (i.e. there won't just be 1 copy of a given gene in a complete sample) </li> </ul>"},{"location":"RNAseq/rnaseq-nfcore/#download-genome-files","title":"Download genome files \ud83e\uddec","text":"<p>To avoid issues with genome incompatibility with the version of STAR you are running, it is recommended to simply download the relevant genome fasta and GTF files using the following scripts, and then supply them directly to the function call.</p> Human genome files \ud83d\udc68\ud83d\udc69Mouse genome files \ud83d\udc01 01_retrieve_human_genome.sh<pre><code>#!/bin/bash\nVERSION=111\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/gtf/homo_sapiens/Homo_sapiens.GRCh38.$VERSION.gtf.gz\n</code></pre> 01_retrieve_mouse_genome.sh<pre><code>#!/bin/bash\nVERSION=111\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/fasta/mus_musculus/dna/Mus_musculus.GRCm39.dna_sm.primary_assembly.fa.gz\nwget -L ftp://ftp.ensembl.org/pub/release-$VERSION/gtf/mus_musculus/Mus_musculus.GRCm39.$VERSION.gtf.gz\n</code></pre>"},{"location":"RNAseq/rnaseq-nfcore/#run-your-rna-sequencing-reads","title":"Run your RNA sequencing reads \ud83c\udfc3","text":"<p>To avoid typing the whole command out (and in case the pipeline crashes), create a script that will handle the process. Two examples are given here, with one for human samples, and one for mouse samples.</p> <ul> <li>You will need to replace the RSEM folder location with your own path from above.</li> <li>Using the <code>save_reference</code> option stores the formatted genome files to save time if you need to resume or restart the pipeline.</li> </ul> Human run script \ud83d\udc68\ud83d\udc69Mouse genome files \ud83d\udc01 02_run_rnaseq_human.sh<pre><code>#!/bin/bash\nmodule load java/openjdk-17.0.2\nexport PATH=$PATH:/home/mmacowan/mf33/tools/RSEM/\n\nnextflow run nf-core/rnaseq -r 3.14.0 \\\n    --input samplesheet.csv \\\n    --outdir rnaseq_output \\\n    --fasta /home/mmacowan/mf33/scratch_nobackup/RNA/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz \\\n    --gtf /home/mmacowan/mf33/scratch_nobackup/RNA/Homo_sapiens.GRCh38.111.gtf.gz \\\n    --skip_dupradar \\\n    --skip_markduplicates \\\n    --save_reference \\\n    -resume\n</code></pre> 02_run_rnaseq_mouse.sh<pre><code>#!/bin/bash\nmodule load java/openjdk-17.0.2\nexport PATH=$PATH:/home/mmacowan/mf33/tools/RSEM/\n\nnextflow run nf-core/rnaseq -r 3.14.0 \\\n    --input samplesheet.csv \\\n    --outdir rnaseq_output \\\n    --fasta /home/mmacowan/mf33/scratch_nobackup/RNA/Mus_musculus.GRCm39.dna_sm.primary_assembly.fa.gz \\\n    --gtf /home/mmacowan/mf33/scratch_nobackup/RNA/Mus_musculus.GRCm39.111.gtf.gz \\\n    --skip_dupradar \\\n    --skip_markduplicates \\\n    --save_reference \\\n    -resume\n</code></pre>"},{"location":"RNAseq/rnaseq-nfcore/#import-data-into-r","title":"Import data into R \ud83d\udce5","text":"<p>We have a standardised method for importing data into R. Luckily for us, the NF-CORE/rnaseq pipeline outputs are provided in <code>.rds</code> format as <code>SummarizedExperiment</code> objects, with bias-corrected gene counts without an offset.</p> <ul> <li><code>salmon.merged.gene_counts_length_scaled.rds</code></li> </ul> Tell me more! <ul> <li>There are two matrices provided to us: <code>counts</code> and <code>abundance</code>.<ul> <li>The <code>counts</code> matrix is a re-estimated counts table that aims to provide count-level data to be compatible with downstream tools such as DESeq2.</li> <li>The <code>abundance</code> matrix is the scaled and normalised transcripts per million (TPM) abundance. TPM explicitly erases information about library size. That is, it estimates the relative abundance of each transcript proportional to the total population of transcripts sampled in the experiment. Thus, you can imagine TPM, in a way, as a partition of unity \u2014 we want to assign a fraction of the total expression (whatever that may be) to transcript, regardless of whether our library is 10M fragments or 100M fragments.</li> </ul> </li> <li>The <code>tximport</code> package has a single function for importing transcript-level estimates. The type argument is used to specify what software was used for estimation. A simple list with matrices, <code>\"abundance\"</code>, <code>\"counts\"</code>, and <code>\"length\"</code>, is returned, where the transcript level information is summarized to the gene-level. Typically, abundance is provided by the quantification tools as TPM (transcripts-per-million), while the counts are estimated counts (possibly fractional), and the <code>\"length\"</code> matrix contains the effective gene lengths. The <code>\"length\"</code> matrix can be used to generate an offset matrix for downstream gene-level differential analysis of count matrices.</li> </ul>"},{"location":"RNAseq/rnaseq-nfcore/#r-code-for-import-and-voom-normalisation","title":"R code for import and voom-normalisation","text":"<p>Here we show our standard process for preparing RNAseq data for downstream analysis.</p> Prepare Voom-normalised DGE List<pre><code># Load R packages\npkgs &lt;- c('knitr', 'here', 'SummarizedExperiment', 'biomaRt', 'edgeR', 'limma')\npacman::p_load(char = pkgs)\n\n# Import the bias-corrected counts from STAR Salmon\nrna_data &lt;- readRDS(here('input', 'salmon.merged.gene_counts_length_scaled.rds'))\n\n# Get Ensembl annotations\nensembl &lt;- useMart('ensembl', dataset = 'hsapiens_gene_ensembl')\n\nensemblIDsBronch &lt;- rownames(rna_bronch)\n\ngene_list &lt;- getBM(attributes = c('ensembl_gene_id', 'hgnc_symbol', 'gene_biotype'),\n                   filters = 'ensembl_gene_id', values = ensemblIDsBronch, mart = ensembl)\ncolnames(gene_list) &lt;- c(\"gene_id\", \"hgnc_symbol\", \"gene_biotype\")\ngene_list &lt;- filter(gene_list, !duplicated(gene_id))\n\n# Ensure that only genes in the STAR Salmon outputs are kept for the gene list\nrna_data &lt;- rna_data[rownames(rna_data) %in% gene_list$gene_id, ]\n\n# Add the ENSEMBL data to the rowData element\nrowData(rna_data) &lt;- merge(gene_list, rowData(rna_data), by = \"gene_id\", all = FALSE)\n\n# Load the RNA metadata\nmetadata_rna &lt;- read_csv(here('input', 'metadata_rna.csv'))\n\n# Sort the metadata rows to match the order of the abundance data\nrownames(metadata_rna) &lt;- metadata_rna$RNA_barcode\nmetadata_rna &lt;- metadata_rna[colnames(rna_data),]\n\n# Create a DGEList from the SummarizedExperiment object\nrna_data_dge &lt;- DGEList(assay(rna_data, 'counts'), \n                        samples = metadata_rna, \n                        group = metadata_rna$group,\n                        genes = rowData(rna_data),\n                        remove.zeros = TRUE)\n\n# Filter the DGEList based on the group information\ndesign &lt;- model.matrix(~ group, data = rna_data_dge$samples)\nkeep_min10 &lt;- filterByExpr(rna_data_dge, design, min.count = 10)\nrna_data_dge_min10 &lt;- rna_data_dge[keep_min10, ]\n\n# Calculate norm factors and perform voom normalisation\nrna_data_dge_min10 &lt;- calcNormFactors(rna_data_dge_min10)\nrna_data_dge_min10 &lt;- voom(rna_data_dge_min10, design, plot = TRUE)\n\n# Add the normalised abundance data from STAR Salmon and filter to match the counts data\nrna_data_dge_min10$abundance &lt;- as.matrix(assay(rna_bronch, 'abundance'))[keep_min10, ]\n\n# Select protein coding defined genes only\nrna_data_dge_min10 &lt;- rna_data_dge_min10[rna_data_dge_min10$genes$gene_biotype == \"protein_coding\" &amp; rna_data_dge_min10$genes$hgnc_symbol != \"\", ]\n\n# Add symbol as rowname\nrownames(rna_data_dge_min10) &lt;- rna_data_dge_min10$genes$gene_name\n\n# Save the DGEList\nsaveRDS(rna_data_dge_min10, here('input', 'rna_data_dge_min10.rds'))\n</code></pre>"},{"location":"RNAseq/rnaseq-nfcore/#rights","title":"Rights","text":"<p>NF-CORE/rnaseq</p> <p>There are many people to thank here for writing and maintaining the NF-CORE/rnaseq pipeline (see here). If you use this pipeline for your analysis, please cite it using the following doi: 10.5281/zenodo.1400710</p> <p>This document</p> <ul> <li>Copyright \u00a9 2024 \u2013 Mucosal Immunology Lab, Melbourne VIC, Australia</li> <li>Licence: These tools are provided under the MIT licence (see LICENSE file for details)</li> <li>Authors: M. Macowan</li> </ul>"},{"location":"Utilities/convert-raw-novaseq-outputs/","title":"Handling NovaSeq sequencing outputs","text":"<p>Here we discuss how to process the raw sequencing reads directly from the Illumina NovaSeq sequencer.</p>"},{"location":"Utilities/convert-raw-novaseq-outputs/#what-you-should-have-out-of-the-box","title":"What you should have \"out of the box\" \ud83d\uddc3\ufe0f","text":"<p>Our runs are stored in Vault storage, and need to be transferred to the M3 MASSIVE cluster for processing. To inspect your files, the simplest way is to use FileZilla by setting up an SFTP connection as below. You need to ensure you have file access to the Vault prior to this.</p> <p></p> <p>The basic file structure on the Vault should look something like below, with a main folder (long name) that contains the relevant files you need, and generally some sort of metadata file. You need to ensure that you have given all permissions to every file so that you can transfer them to the cluster \u2013 you can do this by right clicking the NovaSeq parent folder, selecting <code>File Attributes...</code>, and then adding all of the <code>Read</code>, <code>Write</code>, and <code>Execute</code> permissions, ensuring you select <code>Recurse into subdirectories</code>.</p> <p></p>"},{"location":"Utilities/convert-raw-novaseq-outputs/#transfer-files-to-the-cluster","title":"Transfer files to the cluster","text":""},{"location":"Utilities/convert-raw-novaseq-outputs/#sequencing-data-transfer","title":"Sequencing data transfer \ud83d\ude9b","text":"<p>Navigate to an appropriate project folder on the cluster. An example command is shown below for transferring the data folder into a new folder called <code>raw_data</code> using <code>rsync</code>. If it doesn't exist, the folder you name will be created for you (just make sure you put a <code>/</code> after the new folder name).</p> <pre><code>rsync -aHWv --stats --progress MONASH\\\\mmac0026@vault-v2.erc.monash.edu:Marsland-CCS-RAW-Sequencing-Archive/vault/03_NovaSeq/NovaSeq25_Olaf_Shotgun/231025_A00611_0223_AHGMNNDRX2/ raw_data/\n</code></pre>"},{"location":"Utilities/convert-raw-novaseq-outputs/#bcl-convert-sample-sheet-preparation","title":"BCL Convert sample sheet preparation \ud83d\uddd2\ufe0f","text":"<p>Create a sample sheet document for BCL Convert (the tool that will demultiplex and prepare out FASTQ files from the raw data). The full documentation can be viewed here.</p> <p>The document should be in the following format, where <code>index</code> is the <code>i7 adapter sequence</code> and <code>index2</code> is the <code>i5 adapter sequence</code>. An additional first column called <code>Lane</code> can be provided to specify a particular lane number only for FASTQ file generation. We will call this file <code>samplesheet.txt</code>.</p> <p>For the indexes, both sequences used on the sample sheet should be the reverse complement of the actual sequences.</p> <p>Ensure correct file encoding \ud83e\ude9f\ud83d\udc40</p> <p>If you make this on a Windows system, ensure you save your output encoded by <code>UTF-8</code> and not <code>UTF-8 with BOM</code>.</p> <pre><code>[Header]\nFileFormatVersion,2\n\n[BCLConvert_Settings]\nCreateFastqForIndexReads,0\n\n[BCLConvert_Data]\nSample_ID,i7_adapter,index,i5_adapter,index2\nAbx1_d21,N701,TAAGGCGA,S502,ATAGAGAG\nAbx2_d21,N702,CGTACTAG,S502,ATAGAGAG\nAbx3_d21,N703,AGGCAGAA,S502,ATAGAGAG\nAbx4_d21,N704,TCCTGAGC,S502,ATAGAGAG\nAbx5_d21,N705,GGACTCCT,S502,ATAGAGAG\n#etc.\n</code></pre>"},{"location":"Utilities/convert-raw-novaseq-outputs/#bcl-convert","title":"BCL Convert \ud83d\udd04","text":""},{"location":"Utilities/convert-raw-novaseq-outputs/#install","title":"Install \u2b07\ufe0f","text":"<p>If you feel the need to have the latest version, visit the Illumina support website and copy the link for the latest CentOS version of the BCL Convert tool.</p> <p>Otherwise use the version that is available on the M3 MASSIVE cluster, and skip to the run section.</p> <pre><code># Download from the support website in the main folder\nwget https://webdata.illumina.com/downloads/software/bcl-convert/bcl-convert-4.2.4-2.el7.x86_64.rpm\n\n# Install using rpm2cpio (change file name as required)\nmodule load rpm2cpio\nrpm2cpio bcl-convert-4.2.4-2.el7.x86_64.rpm | cpio -idv\n</code></pre> <p>The most up-to-date bcl-convert will be inside the output <code>usr/bin/</code> folder, and can be called from that location.</p>"},{"location":"Utilities/convert-raw-novaseq-outputs/#run","title":"Run \ud83c\udfc3","text":"<p>With the <code>raw_data</code> folder and <code>samplesheet.txt</code> both in the same directory, we can now run BCL Convert to generate our demultiplexed FASTQ files. Ensure you have at least 64GB of RAM in your interactive smux session.</p> <p>Open file limit error</p> <p>You will need a very high limit for open files \u2013 BCL Convert will attempt to set this limit to 65,535. However, by default, the limit on the M3 MASSIVE cluster is only 1,024 and cannot be increased by users themselves.</p> <p>You can request additional open file limit from the M3 MASSIVE help desk.</p> <p>Can I run this on my local machine?</p> <p>Please note that the node <code>m3k010</code> has been decommissioned due to system upgrades.</p> <p>However, it is more than possible to run this process quickly on a local machine if you have the raw BCL files available. The minimum requirements (as of BCL Convert v4.0) are:</p> <ul> <li>Hardware requirements<ul> <li>Single multiprocessor or multicore computer</li> <li>Minimum 64 GB RAM</li> </ul> </li> <li>Software requirements<ul> <li>Root access to your computer</li> <li>File system access to adjust ulimit</li> </ul> </li> </ul> <p>You can start an interactive bash session and increase the open file limit as follows:</p> <pre><code># Begin a new interactive bash session on the designated node\nsrun --pty --partition=genomics --qos=genomics --nodelist=m3k010 --mem=320GB --ntasks=1 --cpus-per-task=48 bash -i\n\n# Increase the open file limit to 65,535\nulimit -n 65535\n</code></pre> <pre><code># Run bcl-convert\nbcl-convert \\\n    --bcl-input-directory raw_data \\\n    --output-directory fastq_files \\\n    --sample-sheet samplesheet.txt\n</code></pre> <p>This will create a new output folder called <code>fastq_files</code> that contains your demultiplexed samples.</p>"},{"location":"Utilities/convert-raw-novaseq-outputs/#merge-lanes","title":"Merge lanes \u26d9","text":"<p>If you ran your samples without lane splitting, then you can merge the two lanes together using the following code, saved in the main project folder as <code>merge_lanes.sh</code>, and run using the command: <code>bash merge_lanes.sh</code>.</p> merge_lanes.sh<pre><code>#!/bin/bash\n\n# Merge lanes 1 and 2\ncd fastq_files\nfor f in *.fastq.gz\n  do\n  Basename=${f%_L00*}\n  ## merge R1\n  ls ${Basename}_L00*_R1_001.fastq.gz | xargs cat &gt; ${Basename}_R1.fastq.gz\n  ## merge R2\n  ls ${Basename}_L00*_R2_001.fastq.gz | xargs cat &gt; ${Basename}_R2.fastq.gz\n  done\n\n# Remove individual files to make space\nrm -rf *L00*\n</code></pre>"},{"location":"Utilities/sra-data-submission/","title":"SRA sequencing data submission","text":"<p>A guide to submitting sequencing data to the National Center for Biotechnology Information (NCBI) sequencing read archive (SRA) database. Includes information on uploading data to the SRA using the high-speed Aspera Connect tool.</p> <p>Patient-derived sequencing files</p> <p>If your samples are derived from humans, ensure that your file names include no reference to patient identifiers. Once uploaded to the SRA database, it is very difficult to change the names of files, and requires directly contacting the database to arrange for removal of files and for you to reupload the data. It also involves a difficult process of them re-mapping the new uploads to your existing SRA metadata files.</p> <p>Also ensure that you only include the absolute minimum amount of metadata, in a manner that protects patient confidentiality. Absolutely no information should be unique to one single patient in your cohort, even an age (if you have a patient with a unique age, this should be replaced with <code>NA</code> for the purposes of SRA submission). For manuscripts, you can include a phrase indicating that further metadata is available upon reasonable request. The important thing here is to not infringe on patient privacy and confidentiality.</p> <p>Things you could potentially include: - Modified and anonymised patient ID - Sampling group - Timepoint (not exact days or months) - Sex - Collection year (no exact dates) - Tissue</p>"},{"location":"Utilities/sra-data-submission/#process-overview","title":"Process overview","text":"<ol> <li>Register a BioProject</li> <li>Register BioSamples for the related BioProject</li> <li>Submit data to SRA</li> </ol>"},{"location":"Utilities/sra-data-submission/#register-a-bioproject","title":"Register a BioProject \ud83d\udcd4","text":"<p>The BioProject is an important element that can link together different types of sequencing data, and represents all the sequencing data for a given experiment.</p> <p>Go to the SRA submission website to register a new BioProject.</p> <ul> <li>Sample scope: Multispecies (if you have microbiome data)</li> <li>Target description: Bacterial 16S metagenomics (change if you have shotgun metagenomics and/or host transcriptomics)</li> <li>Organism name: Human (change if using mouse or rat data)</li> <li>Project type: Metagenome (add transcriptome if you also have host transcriptomics)</li> </ul>"},{"location":"Utilities/sra-data-submission/#register-biosamples-test_tube","title":"Register BioSamples :test_tube:","text":""},{"location":"Utilities/sra-data-submission/#microbiome-data","title":"Microbiome data \ud83e\udda0","text":"<p>Microbiome samples will be registered as MIMARKS Specimen samples. On the BioSample Attributes tab, download the BioSample metadata Excel template, and complete it accordingly before uploading. Be very careful with the required field formats. You can double check ontology using the EMBL-EBI Ontology Lookup Service.</p> <ul> <li>Use the BioProject accession number previously generated</li> <li>Organism: <code>human metagenome</code> (or as appropriate)</li> <li>Env broad scale: <code>host-associated</code></li> <li>Env local scale: <code>mammalia-associated habitat</code></li> <li>Env medium: (as appropriate)</li> <li>Strain, isolate, cultivar, ecotype: <code>NA</code></li> <li>Add any other relevant host information in the table, as well as the host tissue samples</li> <li>Any other column which is not relevant can be set to <code>NA</code></li> </ul> <p>The SRA Metadata tab is what will join everything together. Once again, download the provided Excel template, and fill everything in carefully.</p> <ul> <li>Sample name: the base name of your samples</li> <li>Library ID: you may have named your files differently than your sample names \u2013 provide this if so, otherwise you can repeat the sample name</li> <li>Title: a short description of the sample in the form \"<code>{methodology}</code> of <code>{organism}</code>: <code>{sample_info}</code>\" \u2013 e.g. \"Shotgun metagenomics of Homo sapiens: childhood bronchial brushing\".</li> <li>Library strategy: <code>WGS</code></li> <li>Library source: <code>METAGENOMIC</code></li> <li>Library selection: <code>RANDOM</code></li> <li>Library layout: <code>paired</code></li> <li>Platform: <code>ILLUMINA</code></li> <li>Instrument model: <code>Illumina NovaSeq 6000</code></li> <li>Design description: <code>NA</code></li> <li>Filetype: <code>fastq</code></li> <li>Filename: the file name of the forward reads</li> <li>Filename2: the file name of the reverse reads</li> </ul>"},{"location":"Utilities/sra-data-submission/#transcriptomics-data","title":"Transcriptomics data \ud83d\udc68\ud83d\udc2d","text":"<p>Host transcriptomics samples will be registered as either HUMAN or Model organism or animal samples. On the BioSample Attributes tab, download the BioSample metadata Excel template, and complete it accordingly before uploading. Be very careful with the required field formats. You can double check ontology using the EMBL-EBI Ontology Lookup Service.</p> <ul> <li>Use the BioProject accession number previously generated</li> <li>Organism: <code>Homo sapiens</code> (or <code>Mus musculus</code>/<code>Rattus norvegicus</code> as appropriate)</li> <li>Isolate: NA</li> <li>Age: fill this in, but leave <code>NA</code> for human samples if it would result in a unique combination of metadata variables with potential to allow identification of any individual.</li> <li>Biomaterial provider: enter the lab, organisation etc. that provided the samples</li> <li>Collection date: do not enter any exact dates for human samples</li> <li>Geo loc name: country in which samples were collected</li> <li>Sex: provide sex of host</li> <li>Tissue: specify tissue origin of samples</li> <li>Add any other relevant data, such as sampling group</li> </ul> <p>As above, the SRA Metadata tab is where the magic will happen :magic_wand:. Once again, download the provided Excel template, and fill everything in carefully.</p> <ul> <li>Sample name: the base name of your samples</li> <li>Library ID: you may have named your files differently than your sample names \u2013 provide this if so, otherwise you can repeat the sample name</li> <li>Title: a short description of the sample in the form \"<code>{methodology}</code> of <code>{organism}</code>: <code>{sample_info}</code>\" \u2013 e.g. \"RNA-Seq of Homo sapiens: childhood bronchial brushing\".</li> <li>Library strategy: <code>RNA-Seq</code></li> <li>Library source: <code>TRANSCRIPTOMIC</code></li> <li>Library selection: <code>RANDOM</code></li> <li>Library layout: <code>paired</code></li> <li>Platform: <code>ILLUMINA</code></li> <li>Instrument model: <code>Illumina NovaSeq 6000</code></li> <li>Design description: <code>NA</code></li> <li>Filetype: <code>fastq</code></li> <li>Filename: the file name of the forward reads</li> <li>Filename2: the file name of the reverse reads</li> </ul>"},{"location":"Utilities/sra-data-submission/#submit-data-to-sra","title":"Submit data to SRA \ud83d\udce4","text":"<p>Which upload option should I choose?</p> <p>You can choose either of the following upload options, and each has pros and cons.</p> <ul> <li>Filezilla allows parallel uploads according to your settings, but upload speed is typically slower.</li> <li>Aspera Connect (at least with NCBI) only allows sequential uploads, but the upload speed is significantly faster.</li> </ul>"},{"location":"Utilities/sra-data-submission/#filezilla","title":"FileZilla \ud83e\udd96","text":"<p>Using FileZilla is more effective when you have large files and/or a large number of files.</p> <p>In FileZilla, open the sites manager and connect to NCBI as follows: - Protocol: <code>FTP</code> - Host: <code>ftp-private.ncbi.nlm.nih.gov</code> - Username: <code>subftp</code> - Password: this is your user-specific NCBI password given when you submit your data</p> <p>In the <code>Advanced</code> tab next to the <code>General</code> tab, set the <code>Default remote directory</code> field to the directory specified by NCBI. This will looks something like: <code>/uploads/{username}_{uniqueID}</code>.</p> <p>Select connect, and gain access to your account folder on the NCBI FTP server.</p> <p>Create a new project folder within the main upload folder, and enter the folder. Add your files to the upload queue, and begin the upload process.</p>"},{"location":"Utilities/sra-data-submission/#aspera-connect","title":"Aspera Connect","text":"<p>The IBM Aspera Connect tool allows for much faster uploads than FileZilla, and is a good alternative for large files.</p>"},{"location":"Utilities/sra-data-submission/#linux-process","title":"Linux process \ud83d\udc27","text":"<p>The process described here is for Linux, but is similar for Windows and MacOS operating systems. More information is provided on the IBM website.</p> <ol> <li>Download the Aspera Connect software.</li> <li>Open a new terminal window (<code>Ctrl+Alt+T</code>)</li> <li>Navigate to downloads, extract the <code>tar.gz</code> file.</li> <li>Run the install script.</li> </ol> <pre><code># Extract the file\ntar -zxvf ibm-aspera-connect-version+platform.tar.gz\n# Run the install script\n./ibm-aspera-connect-version+platform.sh\n</code></pre> <ol> <li>Add the Aspera Connect bin folder to your PATH variable (reopen terminal to apply changes).</li> </ol> <pre><code># Add folder to PATH\necho 'export PATH=$PATH:/home/{user}/.aspera/connect/bin/ &gt;&gt; ~/.bashrc'\n</code></pre> <ol> <li>Download the NCBI Aspera Connect key file.</li> <li>Navigate to the parent folder of the folder containing the files you want to upload to the SRA database, and create a new bash script.</li> </ol> <pre><code># Create a new bash script file\ntouch upload_seq_data.sh\n</code></pre> <ol> <li>Add the following code to the bash script file. </li> <li>The <code>-i</code> argument is the path to the key file, and must be given as a full path (not a relative one).</li> <li>The <code>-d</code> argument specifies that the directory will be created if it doesn't exist.</li> <li>You can adjust the maximum upload speed using the <code>-l500m</code> argument, where <code>500</code> is the speed in Mbps. You could increase or decrease as desired.</li> <li>Add the folder containing the data to upload, which can be relative to the folder containing the bash script.</li> <li>Next provide the upload folder provided by NCBI, which will be user-specific, and ensure you provide a project folder at the end of this. Data will not be available if it is uploaded into the main uploads folder.</li> </ol> upload_seq_data.sh<pre><code>#!/bin/bash\nascp -i {/full/path/to/key-file/aspera.openssh} -QT -l500m -k1 -d {./name-of-seq-data-folder} subasp@upload.ncbi.nlm.nih.gov:uploads/{user-specific-ID}/{name-of-project}\n</code></pre> <ol> <li>Run the bash script, and upload all files. The default settings will allow you to resume uploads if they are interrupted, and it will not overwrite files that are identical in the destination folder.</li> </ol> <pre><code># Run script\nbash upload_seq_data.sh\n</code></pre>"}]}